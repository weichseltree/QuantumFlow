globals: 
    - CNN_KineticEnergyFunctional
    - CNN_KineticEnergyDensityFunctional
    - KineticEnergyFunctionalDerivativeModel
    - ResNet_KineticEnergyDensityFunctional
    - WarmupExponentialDecay
    - CustomTensorBoard
    #- RectifiedAdam
    #- Ranger

default: &DEFAULT
    load_checkpoint: null

    N: 1
    seed: 0
    dtype: float32
    dataset_train: recreate/dataset_paper.hdf5
    dataset_validate: recreate/dataset_validate.hdf5
    dataset_info:
        shapes: True
        h: True

    model: CNN_KineticEnergyFunctional
    model_kwargs: &DEFAULT_MODEL_KWARGS
        filters: [32, 32, 32, 32, 32]
        kernel_size: [100, 100, 100, 100, 100]
        padding: valid
        activation: softplus
        l2_regularisation: 0.00025
        bias_mean_initialisation: False
        batch_normalization: False

    features: [density]
    targets: [kinetic_energy]

    metrics: [mean_absolute_error]
    loss: [mean_squared_error]

    optimizer: Adam
    optimizer_kwargs: &DEFAULT_OPTIMIZER_KWARGS
        learning_rate: WarmupExponentialDecay
        beta_1: 0.9
        beta_2: 0.999
        epsilon: 1e-07
        amsgrad: False

        clipnorm: 100.0

        learning_rate_kwargs:
            initial_learning_rate: 0.0001
            final_learning_rate: 0.0000000001
            decay_rate: 0.9
            decay_steps: 1000 # batches

            cold_steps: 21800 # batches
            warmup_steps: 0 # batches
            cold_factor: 1.0

        #learning_rate_kwargs:
        #    initial_learning_rate: 0.001
        #    final_learning_rate: 0.0000001
        #    decay_rate: 0.99 
        #    decay_steps: 100 # batches

        #    cold_steps: 1000 # batches
        #    warmup_steps: 1000 # batches
        #    cold_factor: 0.1

    fit_kwargs:
        batch_size: 100
        epochs: 100000
        verbose: 0
        #validation_split: 0.1
        validation_freq: 10000 # epochs
        shuffle: False
        initial_epoch: 0

    checkpoint: True
    checkpoint_kwargs:
        filename: weights.{epoch:03d}.tf
        save_freq: 20000 # 2000000 samples
        save_weights_only: True

    tensorboard: CustomTensorBoard
    tensorboard_kwargs:
        histogram_freq: 10000 # epochs
        metrics_freq: 100 # epochs
        write_graph: True
        write_images: True
        update_freq: epoch
        profile_batch: 0

    save_model: True
    export: True
    export_model: self # part of the model to export 

cnn: &CNN
    <<: *DEFAULT
    model: KineticEnergyFunctionalDerivativeModel
    model_kwargs: &CNN_MODEL_KWARGS
        base_model: CNN_KineticEnergyFunctional
        filters: [32, 32, 32, 32, 32]
        kernel_size: [100, 100, 100, 100, 100]
        padding: valid
        activation: softplus
        l2_regularisation: 0.00025
        bias_mean_initialisation: False
        batch_normalization: False

    features: [density]
    targets: [kinetic_energy, derivative]

    metrics: 
        kinetic_energy: mean_absolute_error
        derivative: mean_absolute_error

    loss: 
        kinetic_energy: mean_squared_error
        derivative: mean_squared_error

    loss_weights:
        kinetic_energy: 0.2
        derivative: 1.0

    save_model: False
    export_model: model

    # hyperparameter
    int_min: 
        seed: 0
    int_max:
        seed: 9

density: &DENSITY
    <<: *DEFAULT
    model: KineticEnergyFunctionalDerivativeModel
    model_kwargs: &DENSITY_MODEL_KWARGS
        base_model: CNN_KineticEnergyDensityFunctional
        filters: [32, 32, 32, 32, 32]
        kernel_size: [100, 100, 100, 100, 100]
        padding: same
        activation: softplus
        l2_regularisation: 0.00025
        bias_mean_initialisation: False
        batch_normalization: False

    features: [density]
    targets: [kinetic_energy, kinetic_energy_density, derivative]

    metrics: 
        kinetic_energy: mean_absolute_error
        kinetic_energy_density: mean_absolute_error
        derivative: mean_absolute_error

    loss: 
        kinetic_energy: mean_squared_error
        kinetic_energy_density: mean_squared_error
        derivative: mean_squared_error

    loss_weights:
        kinetic_energy: 0.0
        kinetic_energy_density: 1.0
        derivative: 1.0

    optimizer: Adam
    optimizer_kwargs:
        learning_rate: WarmupExponentialDecay
        beta_1: 0.99
        beta_2: 0.999
        epsilon: 1e-03
        amsgrad: True

        clipnorm: 100.0

        learning_rate_kwargs:
            initial_learning_rate: 0.0001
            final_learning_rate: 0.0000000001
            decay_rate: 0.9
            decay_steps: 1000 # batches

            cold_steps: 21800 # batches
            warmup_steps: 0 # batches
            cold_factor: 1.0

    save_model: False
    export_model: model


resnet: &RESNET
    load_checkpoint: null

    N: 1
    seed: 0
    dtype: float32
    dataset_train: recreate/dataset_paper.hdf5
    dataset_validate: recreate/dataset_validate.hdf5
    dataset_info:
        shapes: True
        h: True

    model: KineticEnergyFunctionalDerivativeModel
    model_kwargs: &RESNET_MODEL_KWARGS
        base_model: ResNet_KineticEnergyDensityFunctional
        l2_regularisation: 0.00025
        bias_mean_initialisation: False
        batch_normalization: False

        resnet_block_kwargs: &RESNET_RESNET_BLOCK
            filters: [32, 32]
            kernel_size: [100, 100]
            padding: same
            activation: softplus
            add_input: True

        final_block_kwargs: &RESNET_FINAL_BLOCK
            filters: [1]
            kernel_size: [100]
            padding: same
            activation: null
            add_input: False

        blocks: [<<: *RESNET_RESNET_BLOCK,
                 <<: *RESNET_RESNET_BLOCK,
                 <<: *RESNET_RESNET_BLOCK,
                 <<: *RESNET_FINAL_BLOCK]
                    

    features: [density]
    targets: [kinetic_energy, kinetic_energy_density, derivative]

    metrics: 
        kinetic_energy: mean_absolute_error
        kinetic_energy_density: mean_absolute_error
        derivative: mean_absolute_error

    loss: 
        kinetic_energy: mean_squared_error
        kinetic_energy_density: mean_squared_error
        derivative: mean_squared_error

    loss_weights:
        kinetic_energy: 0
        kinetic_energy_density: 1.0
        derivative: 1.0

    optimizer: Adam
    optimizer_kwargs:
        learning_rate: WarmupExponentialDecay
        beta_1: 0.9
        beta_2: 0.999
        epsilon: 1e-07
        amsgrad: False

        clipnorm: 100.0

        learning_rate_kwargs:
            initial_learning_rate: 0.0001
            final_learning_rate: 0.0000000001
            decay_rate: 0.9
            decay_steps: 1000 # batches

            cold_steps: 21800 # batches
            warmup_steps: 0 # batches
            cold_factor: 1.0

    fit_kwargs:
        batch_size: 100
        epochs: 100000
        verbose: 0
        #validation_split: 0.1
        validation_freq: 10000 # epochs
        shuffle: False
        initial_epoch: 0

    checkpoint: True
    checkpoint_kwargs:
        filename: weights.{epoch:03d}.tf
        save_freq: 2000000 # samples
        save_weights_only: True

    tensorboard: CustomTensorBoard
    tensorboard_kwargs:
        histogram_freq: 10000 # epochs
        metrics_freq: 100 # epochs
        write_graph: True
        write_images: True
        update_freq: epoch
        profile_batch: 0

    save_model: False
    export: True
    export_model: model

    # hyperparameter
    #int_min: 
    #    seed: 0
    #int_max:
    #    seed: 9


resnet_l2:
    <<: *RESNET
    model_kwargs:
        <<: *RESNET_MODEL_KWARGS
        l2_regularisation: 0.001

resnet_l2_more:
    <<: *RESNET
    model_kwargs:
        <<: *RESNET_MODEL_KWARGS
        l2_regularisation: 0.0025

resnet_l2_less:
    <<: *RESNET
    model_kwargs:
        <<: *RESNET_MODEL_KWARGS
        l2_regularisation: 0.000025


resnet_l2_normal:
    <<: *RESNET
    model_kwargs:
        <<: *RESNET_MODEL_KWARGS
        l2_regularisation: 0.00025

resnet_long:
    <<: *RESNET
    #load_checkpoint: 'kd_cnn/resnet_long/weights.180000.tf'

    fit_kwargs:
        batch_size: 100
        epochs: 200000
        verbose: 0
        #validation_split: 0.1
        validation_freq: 10000 # epochs
        shuffle: False
        initial_epoch: 0


resnet_finish:
    load_checkpoint: 'kd_cnn/resnet_long/weights.60000.tf'
    <<: *RESNET
    fit_kwargs:
        batch_size: 100
        epochs: 200000
        verbose: 0
        #validation_split: 0.1
        validation_freq: 10000 # epochs
        shuffle: False
        initial_epoch: 60000

    optimizer: Adam
    optimizer_kwargs:
        learning_rate: WarmupExponentialDecay
        beta_1: 0.9
        beta_2: 0.999
        epsilon: 1e-07
        amsgrad: False

        clipnorm: 100.0

        learning_rate_kwargs:
            initial_learning_rate: 0.001
            final_learning_rate: 0.000001
            decay_rate: 0.9
            decay_steps: 1000 # batches

            cold_steps: 1000 # batches
            warmup_steps: 1000 # batches
            cold_factor: 0.1

cnn_vWall_3:
    <<: *CNN
    N: 'all'
    subtract_von_weizsaecker: True

    fit_kwargs:
        batch_size: 400
        epochs: 100000
        verbose: 0
        #validation_split: 0.1
        validation_freq: 10000 # epochs
        shuffle: False
        initial_epoch: 0

    optimizer: Adam
    optimizer_kwargs:
        learning_rate: WarmupExponentialDecay
        beta_1: 0.9
        beta_2: 0.999
        epsilon: 1e-07
        amsgrad: False

        clipnorm: 100.0

        learning_rate_kwargs:
            initial_learning_rate: 0.0001
            final_learning_rate: 0.0000001
            decay_rate: 0.9
            decay_steps: 1000 # batches

            cold_steps: 21800 # batches
            warmup_steps: 0 # batches
            cold_factor: 1.0


resnet_vW_N2: &RESNET_VW
    <<: *RESNET
    N: 2
    subtract_von_weizsaecker: True

    optimizer: Adam
    optimizer_kwargs:
        learning_rate: WarmupExponentialDecay
        beta_1: 0.9
        beta_2: 0.999
        epsilon: 1e-07
        amsgrad: False

        clipnorm: 100.0

        learning_rate_kwargs:
            initial_learning_rate: 0.0001
            final_learning_rate: 0.0000000001
            decay_rate: 0.9
            decay_steps: 1000 # batches

            cold_steps: 21800 # batches
            warmup_steps: 0 # batches
            cold_factor: 1.0

resnet_vW_N3:
    <<: *RESNET_VW
    N: 3
    subtract_von_weizsaecker: True


resnet_vW_N4:
    <<: *RESNET_VW
    N: 4
    subtract_von_weizsaecker: True


resnet_vW_all:
    <<: *RESNET_VW
    N: 'all'
    subtract_von_weizsaecker: True

    fit_kwargs:
        batch_size: 400
        epochs: 100000
        verbose: 0
        #validation_split: 0.1
        validation_freq: 10000 # epochs
        shuffle: False
        initial_epoch: 0



# Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285: 6790 examples/sec
# name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7