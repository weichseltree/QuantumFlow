{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup notebook if it is run on Google Colab, cwd = notebook file location\n",
    "try:\n",
    "    # change notebook_path if this notebook is in a different subfolder of Google Drive\n",
    "    notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q ruamel.yaml\n",
    "\n",
    "    %load_ext tensorboard\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# imports\n",
    "import tensorflow as tf\n",
    "\n",
    "# setup paths and variables for shared code (../quantumflow) and data (../data)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# import shared code, must run 0_create_shared_project_files.ipynb first!\n",
    "from quantumflow.utils import load_hyperparameters, train, build_model, QFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'kd_resnet'\n",
    "\n",
    "base_dir = os.path.join(data_dir, experiment)\n",
    "if not os.path.exists(base_dir): os.makedirs(base_dir)\n",
    "\n",
    "file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "file_model = os.path.join(base_dir, \"model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_model\n",
    "import tensorflow as tf\n",
    "from quantumflow.keras_utils import CustomTensorBoard, WarmupExponentialDecay\n",
    "\n",
    "class IntegrateLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, h=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.h*tf.reduce_sum((inputs[:, :-1] + inputs[:, 1:])/2., axis=1, name='trapezoidal_integral_approx')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'h': self.h})\n",
    "        return config\n",
    "\n",
    "class SymmetricConv1D(tf.keras.layers.Conv1D):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.kernel.assign(tf.concat([self.kernel[:(self.kernel_size[0]+1)//2], tf.reverse(self.kernel[:self.kernel_size[0]//2], axis=[0])], axis=0))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_rev = tf.reverse(inputs, axis=[1])\n",
    "        output = super().call(inputs)\n",
    "        output_rev = super().call(input_rev)\n",
    "        return 0.5*(output + tf.reverse(output_rev, axis=[1]))\n",
    "\n",
    "def CNN_KineticEnergyFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['model_kwargs']['l2_regularisation']) if params['model_kwargs'].get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params['model_kwargs'].get('bias_mean_initialisation', False) else None\n",
    "    conv1d_layer = SymmetricConv1D if params['model_kwargs'].get('symmetric', False) else tf.keras.layers.Conv1D\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['dataset']['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    for layer in range(len(params['model_kwargs']['filters'])):\n",
    "        value = conv1d_layer(filters=params['model_kwargs']['filters'][layer], \n",
    "                             kernel_size=params['model_kwargs']['kernel_size'][layer], \n",
    "                             activation=params['model_kwargs']['activation'] if not params['model_kwargs'].get('batch_normalization', False) else None, \n",
    "                             padding=params['model_kwargs']['padding'],\n",
    "                             kernel_regularizer=kernel_regularizer)(value)\n",
    "        if params['model_kwargs'].get('batch_normalization', False):\n",
    "            value = tf.keras.layers.BatchNormalization()(value)\n",
    "            value = tf.keras.layers.Activation(params['model_kwargs']['activation'])(value)\n",
    "\n",
    "    value = tf.keras.layers.Flatten()(value)\n",
    "    value = tf.keras.layers.Dense(1, kernel_regularizer=kernel_regularizer, bias_initializer=bias_initializer)(value)\n",
    "    kinetic_energy = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy')(value)\n",
    "\n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy})\n",
    "\n",
    "\n",
    "def CNN_KineticEnergyDensityFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['model_kwargs']['l2_regularisation']) if params['model_kwargs'].get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params['model_kwargs'].get('bias_mean_initialisation', False) else None\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['dataset']['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    for layer in range(len(params['model_kwargs']['filters'])):\n",
    "        value = tf.keras.layers.Conv1D(filters=params['model_kwargs']['filters'][layer], \n",
    "                                       kernel_size=params['model_kwargs']['kernel_size'][layer], \n",
    "                                       activation=params['model_kwargs']['activation'] if not params['model_kwargs'].get('batch_normalization', False) and layer < len(params['model_kwargs']['filters'])-1 else None, \n",
    "                                       padding=params['model_kwargs']['padding'],\n",
    "                                       kernel_regularizer=kernel_regularizer)(value)\n",
    "\n",
    "        if params['model_kwargs'].get('batch_normalization', False):\n",
    "            value = tf.keras.layers.BatchNormalization()(value)\n",
    "\n",
    "            if layer < len(params['model_kwargs']['filters'])-1:\n",
    "                value = tf.keras.layers.Activation(params['model_kwargs']['activation'])(value)\n",
    "\n",
    "    kinetic_energy_density = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy_density')(value)\n",
    "    kinetic_energy = IntegrateLayer(params['dataset']['h'], name='kinetic_energy')(kinetic_energy_density)\n",
    "    \n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy, 'kinetic_energy_density': kinetic_energy_density})\n",
    "\n",
    "\n",
    "def ResNet_KineticEnergyDensityFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['model_kwargs']['l2_regularisation']) if params['model_kwargs'].get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params['model_kwargs'].get('bias_mean_initialisation', False) else None\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['dataset']['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    def resnet_block(input, filters, kernel_size, padding=None, activation=None, add_input=True):\n",
    "        value = input\n",
    "        for layer in range(len(filters)):\n",
    "            value = tf.keras.layers.Conv1D(filters=filters[layer], \n",
    "                                        kernel_size=kernel_size[layer], \n",
    "                                        padding=padding,\n",
    "                                        use_bias=True,\n",
    "                                        activation= activation if layer < len(filters)-1 else None,\n",
    "                                        kernel_regularizer=kernel_regularizer)(value)\n",
    "\n",
    "        if add_input:\n",
    "            value = tf.keras.layers.Add()([value, input])\n",
    "        \n",
    "        if activation is not None:\n",
    "            return tf.keras.layers.Activation(activation=activation)(value)\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    for layer in range(len(params['model_kwargs']['blocks'])):\n",
    "        value = resnet_block(value, **params['model_kwargs']['blocks'][layer])\n",
    "\n",
    "    kinetic_energy_density = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy_density')(value)\n",
    "    kinetic_energy = IntegrateLayer(params['dataset']['h'], name='kinetic_energy')(kinetic_energy_density)\n",
    "    \n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy, 'kinetic_energy_density': kinetic_energy_density})\n",
    "\n",
    "\n",
    "class KineticEnergyFunctionalDerivativeModel(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = params['model_kwargs']['base_model'](params)\n",
    "        self.h = tf.constant(params['dataset']['h'], dtype=params['dtype'])\n",
    "\n",
    "        self.output_names = sorted(['derivative'] + self.model.output_names)\n",
    "        self.input_names = self.model.input_names\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, density):\n",
    "        density = tf.nest.flatten(density)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(density)\n",
    "            predictions = self.model(density)\n",
    "            kinetic_energy = predictions['kinetic_energy']\n",
    "\n",
    "        predictions['derivative'] = tf.identity(1/self.h*tape.gradient(kinetic_energy, density), name='derivative')\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, y=None, validation_data=None, **kwargs):\n",
    "        if isinstance(y, dict):\n",
    "            y = tf.nest.flatten(y)\n",
    "        \n",
    "        if isinstance(validation_data, (tuple, list)) and isinstance(validation_data[1], dict):\n",
    "            validation_data = (validation_data[0], tf.nest.flatten(validation_data[1]))\n",
    "\n",
    "        super().fit(y=y, validation_data=validation_data, **kwargs)\n",
    "\n",
    "    def _set_output_attrs(self, outputs):\n",
    "        super()._set_output_attrs(outputs)\n",
    "        self.output_names = sorted(['derivative'] + self.model.output_names)\n",
    "\n",
    "    def summary(self, *args, **kwargs):\n",
    "        return self.model.summary(*args, **kwargs)\n",
    "\n",
    "    def save(self, *args, **kwargs):\n",
    "        return self.model.save(*args, **kwargs)\n",
    "\n",
    "    def save_weights(self, *args, **kwargs):\n",
    "        self.model.optimizer = self.optimizer\n",
    "        returns = self.model.save_weights(*args, **kwargs)\n",
    "        self.model.optimizer = None\n",
    "        return returns\n",
    "\n",
    "    def load_weights(self, *args, **kwargs):\n",
    "        self.model.optimizer = self.optimizer\n",
    "        returns = self.model.load_weights(*args, **kwargs)\n",
    "        self.optimizer = self.model.optimizer\n",
    "        self.model.optimizer = None\n",
    "        return returns\n",
    "\n",
    "\n",
    "#from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "\n",
    "#def Ranger(*args, **kwargs):\n",
    "#    return Lookahead(RectifiedAdam(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run $file_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_hyperparams\n",
    "globals: \n",
    "    - CNN_KineticEnergyFunctional\n",
    "    - CNN_KineticEnergyDensityFunctional\n",
    "    - KineticEnergyFunctionalDerivativeModel\n",
    "    - ResNet_KineticEnergyDensityFunctional\n",
    "    - WarmupExponentialDecay\n",
    "    - CustomTensorBoard\n",
    "    #- RectifiedAdam\n",
    "    #- Ranger\n",
    "\n",
    "default: &DEFAULT\n",
    "    load_checkpoint: null\n",
    "\n",
    "    N: 1\n",
    "    seed: 0\n",
    "    dtype: float32\n",
    "    dataset_train: recreate/dataset_paper.hdf5\n",
    "    dataset_validate: recreate/dataset_validate.hdf5\n",
    "    dataset_info:\n",
    "        shapes: True\n",
    "        h: True\n",
    "\n",
    "    model: CNN_KineticEnergyFunctional\n",
    "    model_kwargs: &DEFAULT_MODEL_KWARGS\n",
    "        filters: [32, 32, 32, 32, 32]\n",
    "        kernel_size: [100, 100, 100, 100, 100]\n",
    "        padding: valid\n",
    "        activation: softplus\n",
    "        l2_regularisation: 0.00025\n",
    "        bias_mean_initialisation: False\n",
    "        batch_normalization: False\n",
    "\n",
    "    features: [density]\n",
    "    targets: [kinetic_energy]\n",
    "\n",
    "    metrics: [mean_absolute_error]\n",
    "    loss: [mean_squared_error]\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs: &DEFAULT_OPTIMIZER_KWARGS\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.001\n",
    "            final_learning_rate: 0.0000001\n",
    "            decay_rate: 0.99 \n",
    "            decay_steps: 100 # batches\n",
    "\n",
    "            cold_steps: 1000 # batches\n",
    "            warmup_steps: 1000 # batches\n",
    "            cold_factor: 0.1\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 100000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 10000 # epochs\n",
    "        shuffle: False\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 2000000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "    tensorboard: CustomTensorBoard\n",
    "    tensorboard_kwargs:\n",
    "        histogram_freq: 10000 # epochs\n",
    "        metrics_freq: 100 # epochs\n",
    "        write_graph: True\n",
    "        write_images: True\n",
    "        update_freq: epoch\n",
    "        profile_batch: 0\n",
    "\n",
    "    save_model: True\n",
    "    export: True\n",
    "    export_model: self # part of the model to export \n",
    "\n",
    "cnn: &CNN\n",
    "    <<: *DEFAULT\n",
    "    model: KineticEnergyFunctionalDerivativeModel\n",
    "    model_kwargs: &CNN_MODEL_KWARGS\n",
    "        base_model: CNN_KineticEnergyFunctional\n",
    "        filters: [32, 32, 32, 32, 32]\n",
    "        kernel_size: [100, 100, 100, 100, 100]\n",
    "        padding: valid\n",
    "        activation: softplus\n",
    "        l2_regularisation: 0.00025\n",
    "        bias_mean_initialisation: False\n",
    "        batch_normalization: False\n",
    "\n",
    "    features: [density]\n",
    "    targets: [kinetic_energy, derivative]\n",
    "\n",
    "    metrics: \n",
    "        kinetic_energy: mean_absolute_error\n",
    "        derivative: mean_absolute_error\n",
    "\n",
    "    loss: \n",
    "        kinetic_energy: mean_squared_error\n",
    "        derivative: mean_squared_error\n",
    "\n",
    "    loss_weights:\n",
    "        kinetic_energy: 0.2\n",
    "        derivative: 1.0\n",
    "\n",
    "    save_model: False\n",
    "    export_model: model\n",
    "\n",
    "    # hyperparameter\n",
    "    int_min: \n",
    "        seed: 0\n",
    "    int_max:\n",
    "        seed: 9\n",
    "\n",
    "density: &DENSITY\n",
    "    <<: *DEFAULT\n",
    "    model: KineticEnergyFunctionalDerivativeModel\n",
    "    model_kwargs: &DENSITY_MODEL_KWARGS\n",
    "        base_model: CNN_KineticEnergyDensityFunctional\n",
    "        filters: [32, 32, 32, 32, 32]\n",
    "        kernel_size: [100, 100, 100, 100, 100]\n",
    "        padding: same\n",
    "        activation: softplus\n",
    "        l2_regularisation: 0.00025\n",
    "        bias_mean_initialisation: False\n",
    "        batch_normalization: True\n",
    "\n",
    "    features: [density]\n",
    "    targets: [kinetic_energy, kinetic_energy_density, derivative]\n",
    "\n",
    "    metrics: \n",
    "        kinetic_energy: mean_absolute_error\n",
    "        kinetic_energy_density: mean_absolute_error\n",
    "        derivative: mean_absolute_error\n",
    "\n",
    "    loss: \n",
    "        kinetic_energy: mean_squared_error\n",
    "        kinetic_energy_density: mean_squared_error\n",
    "        derivative: mean_squared_error\n",
    "\n",
    "    loss_weights:\n",
    "        kinetic_energy: 0.0\n",
    "        kinetic_energy_density: 1.0\n",
    "        derivative: 1.0\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.99\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-03\n",
    "        amsgrad: True\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.001\n",
    "            final_learning_rate: 0.0000001\n",
    "            decay_rate: 0.99 \n",
    "            decay_steps: 100 # batches\n",
    "\n",
    "            cold_steps: 1000 # batches\n",
    "            warmup_steps: 1000 # batches\n",
    "            cold_factor: 0.1\n",
    "\n",
    "    save_model: False\n",
    "    export_model: model\n",
    "\n",
    "\n",
    "resnet: &RESNET\n",
    "    load_checkpoint: null\n",
    "\n",
    "    N: 1\n",
    "    seed: 0\n",
    "    dtype: float32\n",
    "    dataset_train: recreate/dataset_paper.hdf5\n",
    "    dataset_validate: recreate/dataset_validate.hdf5\n",
    "    dataset_info:\n",
    "        shapes: True\n",
    "        h: True\n",
    "\n",
    "    model: KineticEnergyFunctionalDerivativeModel\n",
    "    model_kwargs: &RESNET_MODEL_KWARGS\n",
    "        base_model: ResNet_KineticEnergyDensityFunctional\n",
    "        l2_regularisation: 0.00025\n",
    "        bias_mean_initialisation: False\n",
    "        batch_normalization: False\n",
    "\n",
    "        resnet_block_kwargs: &RESNET_RESNET_BLOCK\n",
    "            filters: [32, 32]\n",
    "            kernel_size: [100, 100]\n",
    "            padding: same\n",
    "            activation: softplus\n",
    "            add_input: True\n",
    "\n",
    "        final_block_kwargs: &RESNET_FINAL_BLOCK\n",
    "            filters: [1]\n",
    "            kernel_size: [100]\n",
    "            padding: same\n",
    "            activation: null\n",
    "            add_input: False\n",
    "\n",
    "        blocks: [<<: *RESNET_RESNET_BLOCK,\n",
    "                 <<: *RESNET_RESNET_BLOCK,\n",
    "                 <<: *RESNET_RESNET_BLOCK,\n",
    "                 <<: *RESNET_FINAL_BLOCK]\n",
    "                    \n",
    "\n",
    "    features: [density]\n",
    "    targets: [kinetic_energy, kinetic_energy_density, derivative]\n",
    "\n",
    "    metrics: \n",
    "        kinetic_energy: mean_absolute_error\n",
    "        kinetic_energy_density: mean_absolute_error\n",
    "        derivative: mean_absolute_error\n",
    "\n",
    "    loss: \n",
    "        kinetic_energy: mean_squared_error\n",
    "        kinetic_energy_density: mean_squared_error\n",
    "        derivative: mean_squared_error\n",
    "\n",
    "    loss_weights:\n",
    "        kinetic_energy: 0\n",
    "        kinetic_energy_density: 1.0\n",
    "        derivative: 1.0\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.001\n",
    "            final_learning_rate: 0.0000001\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 1000 # batches\n",
    "\n",
    "            cold_steps: 1000 # batches\n",
    "            warmup_steps: 1000 # batches\n",
    "            cold_factor: 0.1\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 100000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 10000 # epochs\n",
    "        shuffle: False\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 2000000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "    tensorboard: CustomTensorBoard\n",
    "    tensorboard_kwargs:\n",
    "        histogram_freq: 10000 # epochs\n",
    "        metrics_freq: 100 # epochs\n",
    "        write_graph: True\n",
    "        write_images: True\n",
    "        update_freq: epoch\n",
    "        profile_batch: 0\n",
    "\n",
    "    save_model: False\n",
    "    export: True\n",
    "    export_model: model\n",
    "\n",
    "    # hyperparameter\n",
    "    #int_min: \n",
    "    #    seed: 0\n",
    "    #int_max:\n",
    "    #    seed: 9\n",
    "\n",
    "resnet_l2:\n",
    "    <<: *RESNET\n",
    "    model_kwargs:\n",
    "        <<: *RESNET_MODEL_KWARGS\n",
    "        l2_regularisation: 0.001\n",
    "\n",
    "resnet_l2_more:\n",
    "    <<: *RESNET\n",
    "    model_kwargs:\n",
    "        <<: *RESNET_MODEL_KWARGS\n",
    "        l2_regularisation: 0.0025\n",
    "\n",
    "resnet_l2_less:\n",
    "    <<: *RESNET\n",
    "    model_kwargs:\n",
    "        <<: *RESNET_MODEL_KWARGS\n",
    "        l2_regularisation: 0.000025\n",
    "\n",
    "\n",
    "resnet_l2_normal:\n",
    "    <<: *RESNET\n",
    "    model_kwargs:\n",
    "        <<: *RESNET_MODEL_KWARGS\n",
    "        l2_regularisation: 0.00025\n",
    "\n",
    "resnet_long:\n",
    "    <<: *RESNET\n",
    "    #load_checkpoint: 'kd_cnn/resnet_long/weights.180000.tf'\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 200000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 10000 # epochs\n",
    "        shuffle: False\n",
    "        initial_epoch: 0\n",
    "\n",
    "\n",
    "resnet_finish:\n",
    "    load_checkpoint: 'kd_cnn/resnet_long/weights.60000.tf'\n",
    "    <<: *RESNET\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 200000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 10000 # epochs\n",
    "        shuffle: False\n",
    "        initial_epoch: 60000\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.001\n",
    "            final_learning_rate: 0.000001\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 1000 # batches\n",
    "\n",
    "            cold_steps: 1000 # batches\n",
    "            warmup_steps: 1000 # batches\n",
    "            cold_factor: 0.1\n",
    "\n",
    "\n",
    "# Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285: 6790 examples/sec\n",
    "# name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r $base_dir/resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python --bg --out out --err err\n",
    "import sys; sys.path.append('../')\n",
    "from quantumflow.utils import run_experiment\n",
    "run_experiment(experiment='kd_resnet', run_name='resnet_l2_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%python --bg --out out --err err\n",
    "import sys; sys.path.append('../')\n",
    "from quantumflow.utils import run_multiple\n",
    "run_multiple(experiment='kd_cnn', run_name='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "background_jobs = [thread for thread in threading.enumerate() if thread.__class__.__name__ == 'BackgroundJobFunc']\n",
    "display(background_jobs)\n",
    "if len(background_jobs) == 0 and 'out' in globals():\n",
    "    print(out.read().decode('utf-8'))\n",
    "    print(err.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SampleCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, sample_freq=1, merge_layers=None):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.predictions = []\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.epochs = []\n",
    "        self.metrics = []\n",
    "        self.additional = []\n",
    "        \n",
    "        self.merge_layers = merge_layers or {}\n",
    "\n",
    "        self.sample_freq = sample_freq\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % self.sample_freq == 0:\n",
    "            self.epochs.append(epoch)\n",
    "            self.predictions.append(self.model(self.dataset.features))\n",
    "            self.weights.append({weight.name: weight.numpy() for weight in self.model.trainable_variables})\n",
    "    \n",
    "            layers_dict = OrderedDict()\n",
    "\n",
    "            def add_layer(layer, value, name):\n",
    "                if layer.name in self.merge_layers:\n",
    "                    value = layer([value, layers_dict[self.merge_layers[layer.name]]])\n",
    "                else:\n",
    "                    value = layer(value)\n",
    "                layers_dict[name] = value.numpy()\n",
    "                return value\n",
    "\n",
    "            value = self.dataset.density #tf.nest.flatten(self.dataset.features)\n",
    "            for layer in self.model.layers:\n",
    "                if hasattr(layer, 'layers'): # sub-model\n",
    "                    for sub_layer in layer.layers:\n",
    "                        value = add_layer(sub_layer, value, layer.name + '/' + sub_layer.name)\n",
    "                else:\n",
    "                    value = add_layer(layer, value, layer.name)\n",
    "\n",
    "            self.layers.append(layers_dict)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.sample_freq == 0:\n",
    "            self.metrics.append(logs)\n",
    "            self.additional.append({'learning_rate': self.model.optimizer._decayed_lr(tf.float32).numpy(),\n",
    "                                    'adam_iterations': self.model.optimizer.iterations.numpy(),\n",
    "                                    'adam_m_' + self.model.trainable_variables[0].name: self.model.optimizer.get_slot(self.model.trainable_variables[0], 'm').numpy(),\n",
    "                                    'adam_v_' + self.model.trainable_variables[0].name: self.model.optimizer.get_slot(self.model.trainable_variables[0], 'v').numpy(),\n",
    "                                    'adam_beta_1': self.model.optimizer._get_hyper('beta_1', tf.float32).numpy()})\n",
    "    def get_metric(self, key):\n",
    "        return tf.stack([metric[key] for metric in self.metrics])\n",
    "\n",
    "    def get_prediction(self, key):\n",
    "        return tf.stack([prediction[key] for prediction in self.predictions])\n",
    "\n",
    "    def get_weight(self, key):\n",
    "        return tf.stack([weight[key] for weight in self.weights])\n",
    "\n",
    "    def get_layer(self, key):\n",
    "        return tf.stack([layer[key] for layer in self.layers])\n",
    "    \n",
    "    def get_additional(self, key):\n",
    "        return tf.stack([add[key] for add in self.additional])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantumflow.utils import anim_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'full_symmetric'\n",
    "load_checkpoint = 0\n",
    "run_epochs = 100000\n",
    "sample_freq = 500\n",
    "model_dir = os.path.join(data_dir, experiment, run_name)\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "model = build_model(params)\n",
    "display(model.summary())\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "params['fit_kwargs']['initial_epoch'] = load_checkpoint\n",
    "params['load_checkpoint'] = ('kd_cnn/{}/' + params['checkpoint_kwargs']['filename']).format(run_name, epoch=load_checkpoint) if load_checkpoint else None\n",
    "params['fit_kwargs']['epochs'] = params['fit_kwargs']['initial_epoch'] + run_epochs\n",
    "\n",
    "merge_layers = {'add': 'model/lambda',\n",
    "                'add_1': 'model/activation',\n",
    "                'add_2': 'model/activation_1',\n",
    "                'add_3': 'model/activation_2',\n",
    "                'add_4': 'model/activation_3'}\n",
    "\n",
    "#model_dir = '../data/pop_test/' + run_name\n",
    "\n",
    "#params['dataset'] = {'h': 1/499}\n",
    "#params['loss'] = {'kinetic_energy': params['loss']['kinetic_energy'], 'derivative': params['loss']['derivative'](params)}\n",
    "\n",
    "sample_callback = SampleCallback(QFDataset(os.path.join(data_dir, 'recreate/dataset_sample.hdf5'), params), sample_freq=sample_freq, merge_layers=merge_layers)\n",
    "model, params = train(params, callbacks=[sample_callback], model_dir=globals().get('model_dir', None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_callback.weights[0].keys())\n",
    "print(sample_callback.metrics[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(sample_callback.epochs, sample_callback.get_prediction('kinetic_energy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kedensity = sample_callback.get_prediction('kinetic_energy_density')\n",
    "kedensity = np.stack([kedensity, np.repeat(np.expand_dims(sample_callback.dataset.kinetic_energy_density, axis=0), len(kedensity), axis=0)], axis=2)\n",
    "print(kedensity.shape)\n",
    "anim_plot(np.moveaxis(kedensity[:, 0], 2, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative = sample_callback.get_prediction('derivative')\n",
    "derivative = np.stack([derivative[:, 0], np.repeat(np.expand_dims(sample_callback.dataset.derivative, axis=0), len(derivative), axis=0)], axis=2)\n",
    "print(derivative.shape)\n",
    "anim_plot(np.moveaxis(derivative[:, 0], 2, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = sample_callback.layers[0].keys()\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    print(layer_name)\n",
    "    value = sample_callback.get_layer(layer_name)\n",
    "    #frame = 0\n",
    "    #plt.figure(figsize=(10, 1))\n",
    "    #plt.plot(value[frame][0])\n",
    "    #plt.title(layer_name + ' ' + str(value[frame].shape))\n",
    "    #plt.show()\n",
    "\n",
    "    if len(value.shape) < 3:\n",
    "        continue\n",
    "    if len(value.shape) == 3:\n",
    "        value = tf.expand_dims(value, axis=-1)\n",
    "\n",
    "    anim_plot(value[:, 0], figsize=(10, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(sample_callback.epochs, sample_callback.get_metric('loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "for key in sample_callback.metrics[0].keys():\n",
    "    if 'loss' not in key:\n",
    "        continue\n",
    "    metric = sample_callback.get_metric(key)\n",
    "    while len(metric.shape) > 1:\n",
    "        metric = tf.reduce_mean(metric, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, metric, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "for key in sample_callback.metrics[0].keys():\n",
    "    if 'mean' not in key:\n",
    "        continue\n",
    "    metric = sample_callback.get_metric(key)\n",
    "    while len(metric.shape) > 1:\n",
    "        metric = tf.reduce_mean(metric, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, metric, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for key in sample_callback.weights[0].keys():\n",
    "    if not 'conv' in key:\n",
    "        continue\n",
    "    weight = sample_callback.get_weight(key)\n",
    "    while len(weight.shape) > 1:\n",
    "        weight = tf.reduce_mean(weight, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, weight, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sample_callback.get_weight('symmetric_conv1d_3/kernel:0')\n",
    "anim_plot(kernel[:, :, 0], bar='Rendering')\n",
    "gradient = kernel[1:] - kernel[:-1]\n",
    "anim_plot(gradient[:, 0], bar='Rendering')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(np.log(tf.reduce_sum(tf.square(tf.reshape(gradient, [run_epochs//sample_freq-1, -1])), axis=-1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "\n",
    "def my_summary_iterator(path):\n",
    "    for r in tf_record.tf_record_iterator(path):\n",
    "        yield event_pb2.Event.FromString(r)\n",
    "\n",
    "def load_summaries(event_dir, progress=None):\n",
    "    \n",
    "    values = {}\n",
    "\n",
    "    if not os.path.exists(event_dir):\n",
    "        return values\n",
    "\n",
    "    values['wall_time'] = ([], [])\n",
    "\n",
    "    event_files = [os.path.join(event_dir, file) for file in os.listdir(event_dir) if '.tfevents' in file]\n",
    "\n",
    "    if progress is not None and isinstance(progress, int) and progress > 0:\n",
    "        progress = widgets.IntProgress(value=0, max=progress, description='Reading...', bar_style='info', layout=widgets.Layout(width='92%'))\n",
    "        display(progress)\n",
    "\n",
    "    def create_or_append(step, tag, value):\n",
    "        try:\n",
    "            if step not in values[tag][0]:\n",
    "                values[tag][0].append(step)\n",
    "                values[tag][1].append(value)\n",
    "        except KeyError:\n",
    "            values[tag] = ([step], [value])\n",
    "            \n",
    "    for event_file in event_files:\n",
    "        for summary in my_summary_iterator(event_file):\n",
    "            if summary.summary.value.__len__() == 0: continue      \n",
    "\n",
    "            if summary.step not in values['wall_time'][0]:\n",
    "                values['wall_time'][0].append(summary.step)\n",
    "                values['wall_time'][1].append(summary.wall_time)\n",
    "\n",
    "                if progress is not None and summary.step - progress.value >= progress.max//100:\n",
    "                    progress.value = summary.step\n",
    "\n",
    "            for entry in summary.summary.value:\n",
    "                if entry.tag == 'keras':\n",
    "                    continue # model config\n",
    "                elif 'bias' in entry.tag or 'kernel' in entry.tag: \n",
    "                    if 'image' in entry.tag:\n",
    "                        create_or_append(summary.step, 'image/' + entry.tag.replace('image/', ''), entry.image.encoded_image_string)\n",
    "                    else:\n",
    "                        continue #histograms\n",
    "                else: # metrics\n",
    "                    create_or_append(summary.step, entry.tag, entry.simple_value)\n",
    "                \n",
    "    for key in values.keys():        \n",
    "        values[key] = pd.DataFrame(values[key][1], index=values[key][0], columns=[key])\n",
    "    return values\n",
    "\n",
    "\n",
    "def load_summaries2(event_dir, progress=None):\n",
    "    summaries = {}\n",
    "\n",
    "    if not os.path.exists(event_dir):\n",
    "        return summaries\n",
    "\n",
    "    event_files = [os.path.join(event_dir, file) for file in os.listdir(event_dir) if '.tfevents' in file]\n",
    "\n",
    "    if progress is not None and isinstance(progress, int) and progress > 0:\n",
    "        progress = widgets.IntProgress(value=0, max=progress, description='Reading...', bar_style='info', layout=widgets.Layout(width='92%'))\n",
    "        display(progress)\n",
    "\n",
    "    def create_or_append(tag, step, wall_time, keys, values):\n",
    "        try:\n",
    "            if step not in summaries[tag]['step']:\n",
    "                summaries[tag]['step'].append(step)\n",
    "                #summaries[tag]['wall_time'].append(wall_time)\n",
    "                if isinstance(keys, list):\n",
    "                    for key, value in zip(keys, values):\n",
    "                        summaries[tag][key].append(value)\n",
    "                else:\n",
    "                    summaries[tag][keys] = values\n",
    "        except KeyError:\n",
    "            summaries[tag] = {'step': [step]}#, 'wall_time': [wall_time]}\n",
    "            if isinstance(keys, list):\n",
    "                for key, value in zip(keys, values):\n",
    "                    summaries[tag][key] = [value]\n",
    "            else:\n",
    "                summaries[tag][keys] = values\n",
    "            \n",
    "    for event_file in event_files:\n",
    "        for summary in my_summary_iterator(event_file):\n",
    "            if summary.summary.value.__len__() == 0: continue   \n",
    "\n",
    "            if progress is not None and summary.step - progress.value >= progress.max//100:\n",
    "                progress.value = summary.step\n",
    "\n",
    "            for entry in summary.summary.value:\n",
    "                if entry.tag == 'keras':\n",
    "                    continue # model config\n",
    "                elif 'bias' in entry.tag or 'kernel' in entry.tag: \n",
    "                    if 'image' in entry.tag:\n",
    "                        create_or_append('image/' + entry.tag.replace('image/', ''), summary.step, summary.wall_time, entry.tag.replace('image/', ''), entry.image.encoded_image_string)\n",
    "                    else:\n",
    "                        continue #histograms\n",
    "                else: # metrics\n",
    "                    create_or_append(entry.tag, summary.step, summary.wall_time, 'simple_value', entry.simple_value)\n",
    "                \n",
    "    for key in summaries.keys():\n",
    "        summaries[key] = pd.DataFrame(data=summaries[key]).set_index('step')\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'direct'\n",
    "\n",
    "preview = 5\n",
    "figsize = (20, 3)\n",
    "dpi = None\n",
    "kcalmol_per_hartree = 627.51\n",
    "\n",
    "summary_train = load_summaries2(os.path.join(data_dir, experiment, run_name, 'train'), progress=100000)\n",
    "summary_validation = load_summaries2(os.path.join(data_dir, experiment, run_name, 'validation'), progress=100000)\n",
    "\n",
    "display(summary_train.keys())\n",
    "display(summary_validation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "images = reduce(lambda x, y: pd.merge(x, y, on = 'Date'), [value for key, value in summary_train.items() if 'image' in key])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNvjYEGMcEagd1JaHZr6X6t",
   "name": "3a_cnn_train_kedensity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
