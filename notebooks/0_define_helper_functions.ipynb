{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create shared functions\n",
    "\n",
    "Please copy all the notebooks of this project to your Google Drive into a folder named \"notebooks\" inside a folder of your choice. Please also set the \"notebook_path\" variable in every notebook to this folder. This script will generate the folder structure for the data generated by the project.\n",
    "\n",
    "- base folder\n",
    "    - notebooks\n",
    "        - 0_define_helper_functions.ipynb = THIS NOTEBOOK\n",
    "        - 1a_generate_datasets.ipynb\n",
    "        - ...\n",
    "    - data (will be created)\n",
    "    - quantumflow (will be created / overwritten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "if not os.path.exists('../quantumflow'):\n",
    "    os.makedirs('../quantumflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/generate_datasets.py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from quantumflow.colab_utils import load_hyperparameters, integrate, laplace\n",
    "from quantumflow.numerov_solver import solve_schroedinger\n",
    "\n",
    "@tf.function\n",
    "def generate_potentials(return_x=False,\n",
    "                        return_h=False,\n",
    "                        dataset_size=100, \n",
    "                        discretisation_points=500, \n",
    "                        n_gauss=3, \n",
    "                        interval_length=1.0,\n",
    "                        a_minmax=(0.0, 3*10.0), \n",
    "                        b_minmax=(0.4, 0.6), \n",
    "                        c_minmax=(0.03, 0.1), \n",
    "                        n_method='sum',\n",
    "                        dtype='float64',\n",
    "                        **kwargs):\n",
    "    \n",
    "    if dtype == 'double' or dtype == 'float64':\n",
    "        dtype = tf.float64\n",
    "    elif dtype == 'float':\n",
    "        dtype = tf.float32\n",
    "    else:\n",
    "        raise ValueError('unknown dtype {}'.format(dtype))\n",
    "    \n",
    "\n",
    "    x = tf.linspace(tf.constant(0.0, dtype=dtype), interval_length, discretisation_points, name=\"x\")\n",
    "\n",
    "    a = tf.random.uniform((dataset_size, 1, n_gauss), minval=a_minmax[0], maxval=a_minmax[1], dtype=dtype, name=\"a\")\n",
    "    b = tf.random.uniform((dataset_size, 1, n_gauss), minval=b_minmax[0]*interval_length, maxval=b_minmax[1]*interval_length, dtype=dtype, name=\"b\")\n",
    "    c = tf.random.uniform((dataset_size, 1, n_gauss), minval=c_minmax[0]*interval_length, maxval=c_minmax[1]*interval_length, dtype=dtype, name=\"c\")\n",
    "\n",
    "    curves = -tf.square(tf.expand_dims(tf.expand_dims(x, 0), 2) - b)/(2*tf.square(c))\n",
    "    curves = -a*tf.exp(curves)\n",
    "\n",
    "    if n_method == 'sum':\n",
    "        potentials = tf.reduce_sum(curves, -1, name=\"potentials\")\n",
    "    elif n_method == 'mean':\n",
    "        potentials = tf.reduce_mean(curves, -1, name=\"potentials\")\n",
    "    else:\n",
    "        raise NotImplementedError('Method {} is not implemented.'.format(n_method))\n",
    "\n",
    "    returns = [potentials]\n",
    "\n",
    "    if return_x:\n",
    "        returns += [x]\n",
    "    \n",
    "    if return_h:\n",
    "        h = tf.cast(interval_length/(discretisation_points-1), dtype=dtype) # discretisation interval\n",
    "        returns += [h]\n",
    "   \n",
    "    return returns\n",
    "\n",
    "def generate_datasets(data_dir, experiment, generate_names):\n",
    "    if not isinstance(generate_names, list):\n",
    "        generate_names = [generate_names]\n",
    "\n",
    "    base_dir = os.path.join(data_dir, experiment)\n",
    "    file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "\n",
    "    for run_name in generate_names:\n",
    "        params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "\n",
    "        tf.set_random_seed(params['seed'])\n",
    "        potential, x, h = generate_potentials(return_x=True, return_h=True, **params)\n",
    "\n",
    "        params['h'] = h\n",
    "        energies, wavefunctions = solve_schroedinger(potential, params)\n",
    "        \n",
    "        with open(os.path.join(base_dir, params['filename'] + '.pkl'), 'wb') as f:\n",
    "            pickle.dump({'x': x.numpy(), 'h': h.numpy(), 'potential': potential.numpy(), 'wavefunctions': wavefunctions.numpy(), 'energies': energies.numpy()}, f)\n",
    "\n",
    "        print(\"dataset\", params['filename'] + '.pkl', \"saved to\", base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/numerov_solver.py\n",
    "import tensorflow as tf\n",
    "from quantumflow.colab_utils import integrate\n",
    "\n",
    "# recurrent tensorflow cell for solving the numerov equation recursively\n",
    "class ShootingNumerovCell(tf.keras.layers.AbstractRNNCell):\n",
    "    def __init__(self, shape, h, **kwargs):\n",
    "        super(ShootingNumerovCell, self).__init__(**kwargs)\n",
    "        self._h2_scaled = 1 / 12 * h ** 2\n",
    "        self.shape = shape\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.shape + (4,)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.shape + (1,)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        k_m2, k_m1, y_m2, y_m1 = tf.unstack(states[0], axis=-1)\n",
    "        \n",
    "        y = (2 * (1 - 5 * self._h2_scaled * k_m1) * y_m1 - (1 + self._h2_scaled * k_m2) * y_m2) / (1 + self._h2_scaled * inputs)\n",
    "\n",
    "        new_state = tf.stack([k_m1, inputs, y_m1, y], axis=-1)\n",
    "        return y, new_state\n",
    "\n",
    "\n",
    "# tf function for using the shooting numerov method\n",
    "#\n",
    "# the numerov_init_slope is the slope of the solution at x=0\n",
    "# it can be constant>0 because it's actual value will be determined when the wavefunction is normalized\n",
    "#\n",
    "def shooting_numerov(k_squared, params):\n",
    "    h = params['h']\n",
    "    numerov_init_slope = params['numerov_init_slope']\n",
    "    init_values = tf.zeros_like(k_squared[:, 0])\n",
    "    one_step_values = numerov_init_slope * h * tf.ones_like(k_squared[:, 0])\n",
    "    init_state = tf.stack([k_squared[:, 0], k_squared[:, 1], init_values, one_step_values], axis=-1)\n",
    "    outputs = tf.keras.layers.RNN(ShootingNumerovCell(k_squared.shape[2:], h), return_sequences=True)(k_squared[:, 2:], initial_state=init_state)\n",
    "    output = tf.concat([tf.expand_dims(init_values, axis=1), tf.expand_dims(one_step_values, axis=1), outputs], axis=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "# returns the rearranged schroedinger equation term in the numerov equation\n",
    "# k_squared = 2*m_e/h_bar**2*(E - V(x))\n",
    "def numerov_k_squared(potentials, energies):\n",
    "    return 2 * (tf.expand_dims(energies, axis=1) - tf.repeat(tf.expand_dims(potentials, axis=2), energies.shape[1], axis=2))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def find_split_energies(potentials, params):\n",
    "    M = potentials.shape[0]\n",
    "    N = params['n_orbitals']\n",
    "\n",
    "    # Knotensatz: number of roots = quantum state\n",
    "    # so target root = target excited state quantum number\n",
    "    target_roots = tf.repeat(tf.expand_dims(tf.range(N + 1), axis=0), M, axis=0)\n",
    "\n",
    "    # lowest value of potential as lower bound\n",
    "    E_split = tf.repeat(tf.expand_dims(tf.reduce_min(potentials, axis=1), axis=1), N + 1, axis=1)\n",
    "\n",
    "    solutions_split = tf.zeros((potentials.shape[0], potentials.shape[1], N + 1), dtype=potentials.dtype)\n",
    "    not_converged = tf.ones(potentials.shape[0], dtype=tf.bool)\n",
    "    search_boost = tf.ones_like(E_split, dtype=tf.bool)\n",
    "    E_delta = tf.ones_like(E_split)\n",
    "\n",
    "    while tf.math.reduce_any(not_converged):\n",
    "        V_split = numerov_k_squared(tf.boolean_mask(potentials, not_converged), tf.boolean_mask(E_split, not_converged))\n",
    "\n",
    "        solutions_split_new = shooting_numerov(V_split, params)\n",
    "\n",
    "        partitioned_data = tf.dynamic_partition(solutions_split, tf.cast(not_converged, tf.int32) , 2)\n",
    "        condition_indices = tf.dynamic_partition(tf.range(tf.shape(solutions_split)[0]), tf.cast(not_converged, tf.int32) , 2)\n",
    "\n",
    "        solutions_split = tf.dynamic_stitch(condition_indices, [partitioned_data[0], solutions_split_new])\n",
    "        solutions_split.set_shape((potentials.shape[0], potentials.shape[1], N + 1))\n",
    "\n",
    "        roots_split = tf.reduce_sum(tf.cast(detect_roots(solutions_split), tf.int32), axis=1)\n",
    "\n",
    "        not_converged = tf.logical_and(tf.logical_not(tf.reduce_all(tf.equal(roots_split, target_roots), axis=1)), not_converged)\n",
    "\n",
    "        search_direction = tf.cast(roots_split < target_roots, potentials.dtype) - tf.cast(roots_split > target_roots, potentials.dtype)\n",
    "        boost = tf.logical_and(tf.equal(search_direction, tf.sign(E_delta)), search_boost)\n",
    "\n",
    "        E_delta += tf.cast(boost, potentials.dtype)*E_delta\n",
    "        stop_boost = search_direction * tf.sign(E_delta) < 0\n",
    "        search_boost &= tf.logical_not(stop_boost)\n",
    "        E_delta += -1.5*E_delta*tf.cast(stop_boost, potentials.dtype)\n",
    "\n",
    "        E_split += E_delta*tf.expand_dims(tf.cast(not_converged, potentials.dtype), axis=-1)\n",
    "\n",
    "    return E_split\n",
    "\n",
    "\n",
    "def detect_roots(array):\n",
    "    return tf.logical_or(tf.equal(array[:, 1:], 0), array[:, 1:] * array[:, :-1] < 0)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def solve_numerov(potentials, target_roots, split_energies, params):\n",
    "    E_low = split_energies[:, :-1]\n",
    "    E_high = split_energies[:, 1:]\n",
    "\n",
    "    # because the search interval is halved at every step\n",
    "    # 32 iterations will always converge to the best numerically possible accuracy of E\n",
    "    # (empirically ~25 steps)\n",
    "\n",
    "    E = 0.5 * (E_low + E_high)\n",
    "    E_last = E * 2\n",
    "    \n",
    "    while tf.reduce_any(tf.logical_not(tf.equal(E_last, E))):\n",
    "        V = numerov_k_squared(potentials, E)\n",
    "\n",
    "        solutions = shooting_numerov(V, params)\n",
    "        roots = tf.reduce_sum(tf.cast(detect_roots(solutions), tf.int32), axis=1)\n",
    "\n",
    "        update_low = roots <= target_roots\n",
    "        update_high = tf.logical_not(update_low)\n",
    "\n",
    "        E_low = tf.where(update_low, E, E_low)\n",
    "        E_high = tf.where(update_high, E, E_high)\n",
    "\n",
    "        E_last = E\n",
    "        E = 0.5 * (E_low + E_high)\n",
    "\n",
    "    solutions_low = shooting_numerov(numerov_k_squared(potentials, E_low), params)\n",
    "    roots_low = tf.cast(detect_roots(solutions_low), tf.double)\n",
    "\n",
    "    solutions_high = shooting_numerov(numerov_k_squared(potentials, E_high), params)\n",
    "    roots_high = tf.cast(detect_roots(solutions_high), tf.double)\n",
    "\n",
    "    roots_diff = tf.abs(roots_high - roots_low)  \n",
    "\n",
    "    roots_cumsum = tf.cumsum(tf.pad(roots_diff, ((0, 0), (1, 0), (0, 0)), 'constant'), axis=1)\n",
    "\n",
    "    invalid = tf.equal(roots_cumsum, tf.expand_dims(roots_cumsum[:, -1], axis=1))\n",
    "\n",
    "    return solutions_low, E, invalid\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def solve_schroedinger(potentials, params):\n",
    "    M = potentials.shape[0]\n",
    "    G = potentials.shape[1]\n",
    "    N = params['n_orbitals']\n",
    "    \n",
    "    E_split = find_split_energies(potentials, params)\n",
    "\n",
    "    target_roots = tf.repeat(tf.expand_dims(tf.range(N), axis=0), M, axis=0)\n",
    "    solutions_forward, E_forward, invalid_forward = solve_numerov(potentials, target_roots, E_split, params)\n",
    "    #solutions_forward /= tf.expand_dims(tf.reduce_max(tf.abs(solutions_forward)*tf.cast(tf.logical_not(invalid_forward), tf.double), axis=1), axis=1)\n",
    "\n",
    "    solutions_backward, E_backward, invalid_backward = solve_numerov(tf.reverse(potentials, axis=[1]), target_roots, E_split, params)\n",
    "    solutions_backward = tf.reverse(solutions_backward, axis=[1])\n",
    "    invalid_backward = tf.reverse(invalid_backward, axis=[1])\n",
    "    #solutions_backward /= tf.expand_dims(tf.reduce_max(tf.abs(solutions_backward)*tf.cast(tf.logical_not(invalid_backward), tf.double), axis=1), axis=1)\n",
    "\n",
    "    n_invalid_forward = tf.reduce_sum(tf.cast(invalid_forward, tf.int32), axis=1)\n",
    "    n_invalid_backward = tf.reduce_sum(tf.cast(invalid_backward, tf.int32), axis=1)\n",
    "    merge_index = (G - n_invalid_forward - n_invalid_backward)//2 + n_invalid_forward\n",
    "\n",
    "    merge_value_forward = tf.reduce_sum(tf.gather(tf.transpose(solutions_forward, perm=[0, 2, 1]), tf.expand_dims(merge_index, axis=2), batch_dims=2), axis=2)\n",
    "    merge_value_backward = tf.reduce_sum(tf.gather(tf.transpose(solutions_backward, perm=[0, 2, 1]), tf.expand_dims(merge_index, axis=2), batch_dims=2), axis=2)\n",
    "\n",
    "    factor = merge_value_forward/merge_value_backward\n",
    "    solutions_backward *= tf.expand_dims(factor, axis=1)\n",
    "\n",
    "    join_mask = tf.expand_dims(tf.expand_dims(tf.range(G), axis=0), axis=2) < tf.expand_dims(merge_index, axis=1)\n",
    "\n",
    "    solutions = tf.where(join_mask, solutions_forward, solutions_backward)\n",
    "\n",
    "    #normalization\n",
    "    density = solutions ** 2\n",
    "    norm = integrate(density, params['h'])\n",
    "    solutions *= 1 / tf.sqrt(tf.expand_dims(norm, axis=1))\n",
    "\n",
    "    E = 0.5*(E_forward + E_backward)\n",
    "    \n",
    "    return E, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/colab_utils.py\n",
    "import numpy as np\n",
    "\n",
    "def integrate(y, h):\n",
    "    return h*tf.reduce_sum((y[:, :-1] + y[:, 1:])/2., axis=1, name='trapezoidal_integral_approx')\n",
    "\n",
    "def laplace(data, h):  # time_axis=1\n",
    "    temp_laplace = 1 / h ** 2 * (data[:, :-2, :] + data[:, 2:, :] - 2 * data[:, 1:-1, :])\n",
    "    return tf.pad(temp_laplace, ((0, 0), (1, 1), (0, 0)), 'constant')\n",
    "\n",
    "\n",
    "def test_colab_devices():\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    has_gpu = False\n",
    "    has_tpu = False\n",
    "\n",
    "    has_gpu = (tf.test.gpu_device_name() == '/device:GPU:0')\n",
    "\n",
    "    try:\n",
    "        device_name = os.environ['COLAB_TPU_ADDR']\n",
    "        has_tpu = True\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    return has_gpu, has_tpu\n",
    "\n",
    "def get_resolver():\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    try:\n",
    "        device_name = os.environ['COLAB_TPU_ADDR']\n",
    "        TPU_WORKER = 'grpc://' + device_name\n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
    "        tf.config.experimental_connect_to_host(resolver.master())\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "    except KeyError:\n",
    "        resolver = None\n",
    "\n",
    "    return resolver\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)/ float(N))\n",
    "    return cumsum[N:] - cumsum[:-N]\n",
    "\n",
    "def load_hyperparameters(file_hyperparams, run_name='default', globals=None):\n",
    "    from ruamel.yaml import YAML\n",
    "\n",
    "    if globals is not None:\n",
    "        with open(file_hyperparams) as f:\n",
    "            globals_list = YAML().load(f)['globals']\n",
    "\n",
    "    with open(file_hyperparams) as f:\n",
    "        hparams = YAML().load(f)[run_name]\n",
    "\n",
    "    if globals is None:\n",
    "        return hparams\n",
    "\n",
    "    dicts = [hparams]\n",
    "    while len(dicts) > 0:\n",
    "        data = dicts[0]\n",
    "        for idx, obj in enumerate(data):\n",
    "            if isinstance(data[obj], dict):\n",
    "                dicts.append(data[obj])\n",
    "                continue\n",
    "\n",
    "            if data[obj] in globals_list:\n",
    "                data[obj] = globals[data[obj]]\n",
    "        del dicts[0]\n",
    "    return hparams\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, HTML, display\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "def anim_plot(array, x=None, interval=100, bar=\"\", figsize=(15, 3), **kwargs):\n",
    "    frames = len(array)\n",
    "    \n",
    "    if not bar == \"\":\n",
    "        import ipywidgets as widgets\n",
    "        widget = widgets.IntProgress(min=0, max=frames, description=bar, bar_style='success',\n",
    "                                     layout=widgets.Layout(width='92%'))\n",
    "        display(widget)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if x is None:\n",
    "        plt_h = ax.plot(array[0], **kwargs)\n",
    "    else:\n",
    "        plt_h = ax.plot(x, array[0], **kwargs) \n",
    "        \n",
    "    min_last = np.min(array[-1])\n",
    "    max_last = np.max(array[-1])\n",
    "    span_last = max_last - min_last\n",
    "        \n",
    "    ax.set_ylim([min_last - span_last*0.2, max_last + span_last*0.2])\n",
    "\n",
    "    def init():\n",
    "        return plt_h\n",
    "\n",
    "    def animate(f):\n",
    "        if not bar == \"\":\n",
    "            widget.value = f\n",
    "\n",
    "        for i, h in enumerate(plt_h):\n",
    "            if x is None:\n",
    "                h.set_data(np.arange(len(array[f][:, i])), array[f][:, i], **kwargs)\n",
    "            else:\n",
    "                h.set_data(x, array[f][:, i], **kwargs)\n",
    "        return plt_h\n",
    "\n",
    "    # call the animator. blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=frames, interval=interval,\n",
    "                                   blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "    rc('animation', html='html5')\n",
    "    display(HTML(anim.to_html5_video(embed_limit=1024)))\n",
    "\n",
    "    if not bar == \"\":\n",
    "        widget.close()\n",
    "\n",
    "class QFDataset():\n",
    "    def __init__(self, dataset_file, params, set_h=False, set_shapes=False, set_mean=False):\n",
    "        import pickle\n",
    "        import numpy as np\n",
    "\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            x, h, potential, wavefunctions, energies = pickle.load(f).values()\n",
    "            self.dataset_size, self.discretisation_points, self.max_N = wavefunctions.shape\n",
    "            assert(params['N'] <= self.max_N)\n",
    "            density = np.sum(np.square(wavefunctions)[:, :, :params['N']], axis=-1)\n",
    "\n",
    "            potential_energy_densities = np.expand_dims(potential, axis=2)*wavefunctions**2\n",
    "            potential_energies = h * (np.sum(potential_energy_densities, axis=1) - 0.5 * (np.take(potential_energy_densities, 0, axis=1) + np.take(potential_energy_densities, -1, axis=1)))\n",
    "            kinetic_energies = energies - potential_energies\n",
    "\n",
    "            energy = np.sum(energies[:, :params['N']], axis=-1)\n",
    "            kinetic_energy = np.sum(kinetic_energies[:, :params['N']], axis=-1)\n",
    "\n",
    "\n",
    "        if params['dtype'] == 'double' or params['dtype'] == 'float64':\n",
    "            if potential.dtype == np.float32:\n",
    "                raise ImportError(\"requested dtype={}, but dataset is saved with dtype={}, which is less precise.\".format(params['dtype'], potential.dtype))\n",
    "            self.dtype = np.float64\n",
    "        elif params['dtype'] == 'float' or params['dtype'] == 'float32':\n",
    "            self.dtype = np.float32\n",
    "        else:\n",
    "            raise ValueError('unknown dtype {}'.format(params['dtype']))\n",
    "\n",
    "        self.x = x.astype(self.dtype)\n",
    "        self.h = h.astype(self.dtype)\n",
    "        self.potential = potential.astype(self.dtype)\n",
    "        self.density = density.astype(self.dtype)\n",
    "        self.energy = energy.astype(self.dtype)\n",
    "        self.kinetic_energy = kinetic_energy.astype(self.dtype)\n",
    "        self.derivative = -self.potential\n",
    "\n",
    "        if not 'features' in params or not 'targets' in params: \n",
    "            return\n",
    "\n",
    "        self.features = {}\n",
    "        self.targets = {}\n",
    "\n",
    "        def add_by_name(dictionary, name):\n",
    "            if name == 'density':\n",
    "                dictionary['density'] = self.density\n",
    "            elif name == 'derivative':\n",
    "                dictionary['derivative'] = self.derivative\n",
    "            elif name == 'potential':\n",
    "                dictionary['potential'] = self.potential\n",
    "            elif name == 'kinetic_energy':\n",
    "                dictionary['kinetic_energy'] = self.kinetic_energy\n",
    "            else:\n",
    "                raise KeyError('feature/target {} does not exist or is not implemented.'.format(name))\n",
    "\n",
    "        for feature in params['features']:\n",
    "            add_by_name(self.features, feature)\n",
    "\n",
    "        for target in params['targets']:\n",
    "            add_by_name(self.targets, target)\n",
    "\n",
    "        if set_h:\n",
    "            params['h'] = self.h\n",
    "\n",
    "        if set_shapes:\n",
    "            params['features_shape'] = {name:feature.shape[1:] for name, feature in self.features.items()}\n",
    "            params['targets_shape'] = {name:target.shape[1:] for name, target in self.targets.items()}\n",
    "\n",
    "        if set_mean:\n",
    "            params['features_mean'] = {name:np.mean(feature, axis=0) for name, feature in self.features.items()}\n",
    "            params['targets_mean'] = {name:np.mean(target, axis=0) for name, target in self.targets.items()}\n",
    "\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        import tensorflow as tf\n",
    "        return tf.data.Dataset.zip((tf.data.Dataset.zip({name:tf.data.Dataset.from_tensor_slices(feature) for name, feature in self.features.items()}), \n",
    "                                    tf.data.Dataset.zip({name:tf.data.Dataset.from_tensor_slices(target) for name, target in self.targets.items()})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "0_define_helper_functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
