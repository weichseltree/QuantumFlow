{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create shared functions\n",
    "\n",
    "Please copy all the notebooks of this project to your Google Drive into a folder named \"notebooks\" inside a folder of your choice. Please also set the \"notebook_path\" variable in every notebook to this folder. This script will generate the folder structure for the data generated by the project.\n",
    "\n",
    "- base folder\n",
    "    - notebooks\n",
    "        - 0_define_helper_functions.ipynb = THIS NOTEBOOK\n",
    "        - 1a_generate_datasets.ipynb\n",
    "        - ...\n",
    "    - data (will be created)\n",
    "    - quantumflow (will be created / overwritten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "if not os.path.exists('../quantumflow'):\n",
    "    os.makedirs('../quantumflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/generate_potentials.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def tf_generate_potentials(dataset_size=2000, points=500, n_gauss=3, length=1.0,\n",
    "                           a_minmax=(0.0, 3*10.0), b_minmax=(0.4, 0.6), c_minmax=(0.03, 0.1), return_x=False):\n",
    "    x = tf.linspace(0.0, length, points, name=\"x\")\n",
    "\n",
    "    a = tf.random_uniform((dataset_size, 1, n_gauss), minval=a_minmax[0], maxval=a_minmax[1], name=\"a\")\n",
    "    b = tf.random_uniform((dataset_size, 1, n_gauss), minval=b_minmax[0]*length, maxval=b_minmax[1]*length, name=\"b\")\n",
    "    c = tf.random_uniform((dataset_size, 1, n_gauss), minval=c_minmax[0]*length, maxval=c_minmax[1]*length, name=\"c\")\n",
    "\n",
    "    curves = -tf.square(tf.expand_dims(tf.expand_dims(x, 0), 2) - b)/(2*tf.square(c))\n",
    "    curves = -a*tf.exp(curves)\n",
    "\n",
    "    potentials = tf.reduce_sum(curves, -1, name=\"potentials\")\n",
    "\n",
    "    if return_x:\n",
    "        return potentials, x\n",
    "    else:\n",
    "        return potentials\n",
    "\n",
    "def generate_potentials(dataset_size=2000, points=500, n_gauss=3, length=1.0,\n",
    "                        a_minmax=(0.0, 3*10.0), b_minmax=(0.4, 0.6), c_minmax=(0.03, 0.1), return_x=False, seed=0):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "        potentials, x = tf_generate_potentials(dataset_size, points, n_gauss, length,\n",
    "                                               a_minmax, b_minmax, c_minmax, return_x=True)\n",
    "        sess = tf.Session(graph=g)\n",
    "        np_potentials, np_x = sess.run([potentials, x])\n",
    "\n",
    "    if return_x:\n",
    "        return np_potentials, np_x\n",
    "    else:\n",
    "        return np_potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/calculus_utils.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def integrate(data, h, axis=-1):\n",
    "    if data.shape[axis] < 2:\n",
    "        raise ValueError(\n",
    "            \"Integration failed: time-axis {} has {} elements, required: >=2\".format(axis, data.shape[axis]))\n",
    "    return h * (np.sum(data, axis=axis) - 0.5 * (np.take(data, 0, axis=axis) + np.take(data, -1, axis=axis)))\n",
    "\n",
    "\n",
    "def integrate_simpson(data, h, axis=-1):\n",
    "    if data.shape[axis] < 2:\n",
    "        raise ValueError(\n",
    "            \"Integration failed: time-axis {} has {} elements, required: >=2\".format(axis, data.shape[axis]))\n",
    "    integral = 0\n",
    "    if not (data.shape[axis] > 2 and data.shape[axis] % 2 == 1):\n",
    "        integral = integrate(np.take(data, [-2, -1], axis=axis), h, axis)\n",
    "        if data.shape[axis] == 2:\n",
    "            return integral\n",
    "        data = np.take(data, range(0, data.shape[axis] - 1), axis=axis)\n",
    "\n",
    "    even = np.take(data, range(0, data.shape[axis], 2), axis=axis)\n",
    "    odd = np.take(data, range(1, data.shape[axis], 2), axis=axis)\n",
    "\n",
    "    return integral + h / 3 * (2 * np.sum(even, axis=axis) + 4 * np.sum(odd, axis=axis) - np.take(data, 0, axis=axis)\n",
    "                                                                                        - np.take(data, -1, axis=axis))\n",
    "\n",
    "def laplace(data, h):  # time_axis=1\n",
    "    temp_laplace = 1 / h ** 2 * (data[:, :-2, :] + data[:, 2:, :] - 2 * data[:, 1:-1, :])\n",
    "    return np.pad(temp_laplace, ((0, 0), (1, 1), (0, 0)), 'constant')\n",
    "\n",
    "def normalize(function, h, axis=-1):   \n",
    "    norm = integrate_simpson(function, h, axis=axis)\n",
    "    return function * 1 / np.expand_dims(norm, axis=axis)\n",
    "\n",
    "def rbf_kernel(X, X_train, gamma):\n",
    "    return np.exp(-gamma*np.sum(np.square(X[:, :, np.newaxis] - np.transpose(X_train)[np.newaxis, :, :]), 1))\n",
    "\n",
    "def predict(X, X_train, weights, gamma):\n",
    "    return np.sum(weights[np.newaxis, :]*rbf_kernel(X, X_train, gamma), 1)\n",
    "\n",
    "def functional_derivative(X, X_train, weights, gamma, h):\n",
    "    return -1/h*np.sum(weights[np.newaxis, :]*2*gamma*(X[:, :, np.newaxis] - np.transpose(X_train)[np.newaxis, :, :])*rbf_kernel(X, X_train, gamma)[:, np.newaxis, :], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/numerov_solver.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from quantumflow.calculus_utils import integrate, integrate_simpson, laplace\n",
    "\n",
    "# recurrent tensorflow cell for solving the numerov equation recursively\n",
    "class ShootingNumerovCell(tf.nn.rnn_cell.RNNCell):\n",
    "    def __init__(self, h=1.0):\n",
    "        super().__init__()\n",
    "        self._h2_scaled = 1 / 12 * h ** 2\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        k_m2, k_m1, y_m2, y_m1 = tf.unstack(state, axis=-1)\n",
    "\n",
    "        y = (2 * (1 - 5 * self._h2_scaled * k_m1) * y_m1 - (1 + self._h2_scaled * k_m2) * y_m2) / (\n",
    "                    1 + self._h2_scaled * inputs)\n",
    "\n",
    "        new_state = tf.stack([k_m1, inputs, y_m1, y], axis=-1)\n",
    "        return y, new_state\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return 1\n",
    "\n",
    "# tf function for using the shooting numerov method\n",
    "#\n",
    "# the init_factor is the slope of the solution at x=0\n",
    "# it can be constant>0 because it's actual value will be determined when the wavefunction is normalized\n",
    "#\n",
    "def shooting_numerov(k_squared, h=1, init_factor=1e-128):\n",
    "    shooting_cell = ShootingNumerovCell(h=h)\n",
    "    init_state = tf.stack([k_squared[:, 0], k_squared[:, 1], tf.zeros_like(k_squared[:, 2]),\n",
    "                           init_factor * h * tf.ones_like(k_squared[:, 3])], axis=-1)\n",
    "    outputs, _ = tf.nn.static_rnn(shooting_cell, tf.unstack(k_squared, axis=1)[2:], initial_state=init_state)\n",
    "    output = tf.stack([init_state[:, 2], init_state[:, 3]] + outputs, axis=-1)\n",
    "    return output\n",
    "\n",
    "# returns the rearranged schroedinger equation term in the numerov equation\n",
    "# k_squared = 2*m_e/h_bar**2*(E - V(x))\n",
    "def numerov_k_squared(potentials, energies):\n",
    "    return 2 * (np.expand_dims(energies, axis=1) - np.repeat(np.expand_dims(potentials, axis=2), energies.shape[1], axis=2))\n",
    "\n",
    "\n",
    "def detect_roots(array1):\n",
    "    return np.logical_or(array1[:, 1:] == 0, array1[:, 1:] * array1[:, :-1] < 0)\n",
    "\n",
    "\n",
    "class NumerovSolver():\n",
    "    def __init__(self, G, h):\n",
    "        self.K_SQUARED = tf.placeholder(tf.float64, shape=(None, G))\n",
    "        self.solution = shooting_numerov(self.K_SQUARED, h=h)\n",
    "        self.sess = tf.Session()\n",
    "        self.h = h\n",
    "        self.G = G\n",
    "        \n",
    "    # functtion to solve the shooting numerov equation for a given tensor of k_squared functions\n",
    "    # the tensor has to have one dimension for the time along wich to solve the equation\n",
    "    # all other dimensions will be flattened internally but the return value will be reshaped back\n",
    "    def run_numerov(self, k_squared, time_axis=-1):\n",
    "        shape = k_squared.shape[:time_axis] + k_squared.shape[time_axis + 1:]\n",
    "        flattened = np.reshape(np.moveaxis(k_squared, time_axis, -1), (-1, k_squared.shape[time_axis]))\n",
    "        flattened_solutions = self.sess.run(self.solution, feed_dict={self.K_SQUARED: flattened})\n",
    "        solutions = np.reshape(flattened_solutions, shape + (k_squared.shape[time_axis],))\n",
    "        return np.moveaxis(solutions, -1, time_axis)\n",
    "\n",
    "    \n",
    "    def solve_numerov(self, np_potentials, target_roots, split_energies, cut_after_last_root=True, progress=None):\n",
    "\n",
    "        np_E_low = split_energies[:, :-1].copy()\n",
    "        np_E_high = split_energies[:, 1:].copy()\n",
    "\n",
    "        # because the search interval is halved at every step\n",
    "        # 32 iterations will always converge to the best numerically possible accuracy of E\n",
    "        # (empirically ~25 steps)\n",
    "\n",
    "        np_E = 0.5 * (np_E_low + np_E_high)\n",
    "        np_E_last = np.copy(np_E) * 2\n",
    "\n",
    "        \n",
    "        if progress is not None:\n",
    "            progress.value = 0\n",
    "            progress.max = np.prod(np_E.shape)\n",
    "            progress.description = 'Numerov Pass: '\n",
    "        \n",
    "        step = 0\n",
    "        while np.any(np_E_last - np_E):\n",
    "            np_V = numerov_k_squared(np_potentials, np_E)\n",
    "            np_solutions = self.run_numerov(np_V, time_axis=1)\n",
    "            np_roots = np.sum(detect_roots(np_solutions), axis=1)\n",
    "\n",
    "            np_E_low[np_roots <= target_roots] = np_E[np_roots <= target_roots]\n",
    "            np_E_high[np_roots > target_roots] = np_E[np_roots > target_roots]\n",
    "\n",
    "            np_E_last = np_E\n",
    "            np_E = 0.5 * (np_E_low + np_E_high)\n",
    "\n",
    "            if progress is not None:\n",
    "                progress.value = progress.max - np.sum(np_E_last - np_E != 0)\n",
    "                progress.description = 'Numerov Pass: ' + str(progress.value) + '/' + str(progress.max)\n",
    "            step += 1\n",
    "\n",
    "        np_solutions_low = self.run_numerov(numerov_k_squared(np_potentials, np_E_low), time_axis=1)\n",
    "        np_roots_low = 1 * detect_roots(np_solutions_low)\n",
    "\n",
    "        np_solutions_high = self.run_numerov(numerov_k_squared(np_potentials, np_E_high), time_axis=1)\n",
    "        np_roots_high = 1 * detect_roots(np_solutions_high)\n",
    "\n",
    "        np_roots_diff = np.abs(np_roots_high - np_roots_low)  # useless but keep it\n",
    "        # assert(np.all(np.sum(np_roots_diff, axis=1) == 1)) # sometimes roots are at different places!\n",
    "\n",
    "        if cut_after_last_root:\n",
    "            np_nan_cumsum = np.cumsum(np.pad(np_roots_diff, ((0, 0), (1, 0), (0, 0)), 'constant'), axis=1)\n",
    "            np_nan_index = np_nan_cumsum == np.expand_dims(np_nan_cumsum[:, -1], axis=1)\n",
    "\n",
    "            np_solutions_low[np_nan_index] = np.nan\n",
    "\n",
    "        return np_solutions_low, np_E, step\n",
    "\n",
    "    \n",
    "    def find_split_energies(self, np_potentials, N, progress=None):\n",
    "        M = np_potentials.shape[0]\n",
    "        \n",
    "        # Knotensatz: number of roots = quantum state\n",
    "        # so target root = target excited state quantum number\n",
    "        target_roots = np.repeat(np.expand_dims(np.arange(N + 1), axis=0), M, axis=0)\n",
    "\n",
    "        # lowest value of potential as lower bound\n",
    "        np_E_split = np.repeat(np.expand_dims(np.min(np_potentials, axis=1), axis=1), N + 1, axis=1)\n",
    "\n",
    "        np_solutions_split = np.zeros((np_potentials.shape[0], np_potentials.shape[1], N + 1), dtype=np.float64)\n",
    "        not_converged = np.ones(np_potentials.shape[0], dtype=np.bool)\n",
    "        search_boost = np.ones_like(np_E_split)\n",
    "        np_E_delta = np.ones_like(np_E_split)\n",
    "\n",
    "        if progress is not None:\n",
    "            progress.value = 0\n",
    "            progress.max = M\n",
    "            progress.description = 'Searching Roots:'\n",
    "\n",
    "        step = 0\n",
    "        while np.any(not_converged):\n",
    "            np_V_split = numerov_k_squared(np_potentials[not_converged], np_E_split[not_converged])\n",
    "            np_solutions_split[not_converged] = self.run_numerov(np_V_split, time_axis=1)\n",
    "            np_roots_split = np.sum(detect_roots(np_solutions_split), axis=1)\n",
    "\n",
    "            not_converged[np.all(np_roots_split == target_roots, axis=1)] = False\n",
    "\n",
    "            search_direction = 1 * (np_roots_split < target_roots) - 1 * (np_roots_split > target_roots)\n",
    "            np_E_delta[np.logical_and(search_direction == np.sign(np_E_delta), search_boost)] *= 2\n",
    "            search_boost[search_direction * np.sign(np_E_delta) < 0] = 0\n",
    "            np_E_delta[search_direction * np.sign(np_E_delta) < 0] *= -0.5\n",
    "\n",
    "            np_E_split[not_converged] += np_E_delta[not_converged]\n",
    "\n",
    "            if progress is not None:\n",
    "                progress.value = progress.max - np.sum(not_converged)\n",
    "                progress.description = 'Searching Roots: ' + str(progress.value) + '/' + str(progress.max)\n",
    "            step += 1\n",
    "\n",
    "        return np_E_split, step\n",
    "\n",
    "    \n",
    "    def solve_schroedinger(self, np_potentials, N, progress=None):\n",
    "        M = np_potentials.shape[0]\n",
    "        G = np_potentials.shape[1]\n",
    "        \n",
    "        assert (G == self.G)\n",
    "        np_E_split, _ = self.find_split_energies(np_potentials, N, progress=progress)\n",
    "\n",
    "        target_roots = np.repeat(np.expand_dims(np.arange(N), axis=0), M, axis=0)\n",
    "        np_solutions_forward, np_E_forward, _ = self.solve_numerov(np_potentials, target_roots, np_E_split, progress=progress)\n",
    "        np_solutions_forward /= np.expand_dims(np.nanmax(np.abs(np_solutions_forward), axis=1), axis=1)\n",
    "\n",
    "        assert not np.any(np.all(np.isnan(np_solutions_forward), axis=1))\n",
    "\n",
    "        np_solutions_backward, np_E_backward, _ = self.solve_numerov(np.flip(np_potentials, axis=1), target_roots, np_E_split, progress=progress)\n",
    "        np_solutions_backward = np.flip(np_solutions_backward, axis=1)\n",
    "        np_solutions_backward /= np.expand_dims(np.nanmax(np.abs(np_solutions_backward), axis=1), axis=1)\n",
    "\n",
    "        assert not np.any(np.all(np.isnan(np_solutions_backward), axis=1))\n",
    "\n",
    "        np_factor = np_solutions_forward / np_solutions_backward\n",
    "\n",
    "        assert not np.any(np.all(np.isnan(np_factor), axis=1))\n",
    "\n",
    "        np_solutions_backward *= np.expand_dims(np.nanmedian(np_factor, axis=1), axis=1)\n",
    "\n",
    "        join_error = np.nanmin(np.abs(np_solutions_backward - np_solutions_forward), axis=1)\n",
    "\n",
    "        join_error = np.nanmax(np_solutions_backward / np_solutions_forward, axis=1)\n",
    "\n",
    "        join_index = np.nanargmin(np.abs(np_solutions_backward - np_solutions_forward), axis=1)\n",
    "\n",
    "        join_mask = np.expand_dims(np.expand_dims(np.arange(np_solutions_backward.shape[1]), axis=0), axis=2) >= np.expand_dims(join_index, axis=1)\n",
    "\n",
    "        np_solutions = np_solutions_forward\n",
    "        np_solutions[join_mask] = np_solutions_backward[join_mask]\n",
    "\n",
    "        # normalization\n",
    "        np_norm = np_solutions ** 2\n",
    "        np_norm = integrate_simpson(np_norm, self.h, axis=1)\n",
    "        np_solutions *= 1 / np.sqrt(np.expand_dims(np_norm, axis=1))\n",
    "\n",
    "        assert not np.any(np.all(np.isnan(np_solutions), axis=1))\n",
    "        \n",
    "        np_E = 0.5*(np_E_forward + np_E_backward)\n",
    "        \n",
    "        return np_E, np_solutions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/colab_train_utils.py\n",
    "import numpy as np\n",
    "from quantumflow.calculus_utils import integrate, integrate_simpson, laplace\n",
    "\n",
    "def test_colab_devices():\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    has_gpu = False\n",
    "    has_tpu = False\n",
    "\n",
    "    has_gpu = (tf.test.gpu_device_name() == '/device:GPU:0')\n",
    "\n",
    "    try:\n",
    "        device_name = os.environ['COLAB_TPU_ADDR']\n",
    "        has_tpu = True\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    return has_gpu, has_tpu\n",
    "\n",
    "\n",
    "def unpack_dataset(N, dataset):\n",
    "    x, potentials, solutions, E = dataset.values()\n",
    "    density = np.sum(np.square(solutions)[:, :, :N], axis=-1)\n",
    "    \n",
    "    dataset_size, discretization_points, _ = solutions.shape\n",
    "    h = (max(x) - min(x))/(discretization_points-1)\n",
    "    \n",
    "    potential = np.expand_dims(potentials, axis=2)*solutions**2\n",
    "    P = integrate_simpson(potential, h, axis=1)\n",
    "    K = E - P\n",
    "\n",
    "    kinetic_energy = np.sum(K[:, :N], axis=-1)\n",
    "    \n",
    "    return x, potentials, solutions, E, density, kinetic_energy, dataset_size, discretization_points, h    \n",
    "\n",
    "\n",
    "class InputPipeline(object):\n",
    "    def __init__(self, N, dataset_file, is_training=False):\n",
    "        import pickle\n",
    "        self.is_training = is_training\n",
    "\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            self.x, self.potentials, _, self.energies, self.densities, self.kenergies, self.M, self.G, self.h = unpack_dataset(N, pickle.load(f))\n",
    "        self.derivatives = -self.potentials\n",
    "\n",
    "    def input_fn(self, params):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        dataset_densities = tf.data.Dataset.from_tensor_slices(self.densities.astype(np.float32))\n",
    "        dataset_kenergies = tf.data.Dataset.from_tensor_slices(self.kenergies.astype(np.float32))\n",
    "        dataset_derivatives = tf.data.Dataset.from_tensor_slices(self.derivatives.astype(np.float32))\n",
    "\n",
    "        dataset = tf.data.Dataset.zip((dataset_densities, tf.data.Dataset.zip((dataset_kenergies, dataset_derivatives))))\n",
    "\n",
    "        if self.is_training:\n",
    "            dataset = dataset.repeat()\n",
    "\n",
    "        if params['shuffle']:\n",
    "            dataset = dataset.shuffle(buffer_size=params['shuffle_buffer_size'], seed=params.get('seed', None))\n",
    "\n",
    "        dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
    "        return dataset\n",
    "\n",
    "    def features_shape(self):\n",
    "        return self.densities.shape\n",
    "\n",
    "    def targets_shape(self):\n",
    "        return (self.kenergies.shape, self.derivatives.shape)\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        if self.is_training:\n",
    "            string += 'Train Dataset: '\n",
    "        else:\n",
    "            string += 'Dataset: '\n",
    "            \n",
    "        string += str(self.densities.shape) + ' ' + str(self.kenergies.shape) + ' ' + str(self.derivatives.shape) + ' ' + str(self.densities.dtype)\n",
    "        return string\n",
    "\n",
    "\n",
    "def get_resolver():\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    try:\n",
    "        device_name = os.environ['COLAB_TPU_ADDR']\n",
    "        TPU_WORKER = 'grpc://' + device_name\n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
    "        tf.config.experimental_connect_to_host(resolver.master())\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "    except KeyError:\n",
    "        resolver = None\n",
    "\n",
    "    return resolver\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)/ float(N))\n",
    "    return cumsum[N:] - cumsum[:-N]\n",
    "\n",
    "\n",
    "def load_hyperparameters(file_hyperparams, run_name='default', globals=None):\n",
    "    from ruamel.yaml import YAML\n",
    "\n",
    "    if globals is not None:\n",
    "        with open(file_hyperparams) as f:\n",
    "            globals_list = YAML().load(f)['globals']\n",
    "\n",
    "    with open(file_hyperparams) as f:\n",
    "        hparams = YAML().load(f)[run_name]\n",
    "\n",
    "    if globals is None:\n",
    "        return hparams\n",
    "\n",
    "    dicts = [hparams]\n",
    "    while len(dicts) > 0:\n",
    "        data = dicts[0]\n",
    "        for idx, obj in enumerate(data):\n",
    "            if isinstance(data[obj], dict):\n",
    "                dicts.append(data[obj])\n",
    "                continue\n",
    "\n",
    "            if data[obj] in globals_list:\n",
    "                data[obj] = globals[data[obj]]\n",
    "        del dicts[0]\n",
    "    return hparams\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, HTML, display\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "def anim_plot(array, x=None, interval=100, bar=\"\", figsize=(15, 3), **kwargs):\n",
    "    frames = len(array)\n",
    "    \n",
    "    if not bar == \"\":\n",
    "        import ipywidgets as widgets\n",
    "        widget = widgets.IntProgress(min=0, max=frames, description=bar, bar_style='success',\n",
    "                                     layout=widgets.Layout(width='92%'))\n",
    "        display(widget)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if x is None:\n",
    "        plt_h = ax.plot(array[0], **kwargs)\n",
    "    else:\n",
    "        plt_h = ax.plot(x, array[0], **kwargs) \n",
    "        \n",
    "    min_last = np.min(array[-1])\n",
    "    max_last = np.max(array[-1])\n",
    "    span_last = max_last - min_last\n",
    "        \n",
    "    ax.set_ylim([min_last - span_last*0.2, max_last + span_last*0.2])\n",
    "\n",
    "    def init():\n",
    "        return plt_h\n",
    "\n",
    "    def animate(f):\n",
    "        if not bar == \"\":\n",
    "            widget.value = f\n",
    "\n",
    "        for i, h in enumerate(plt_h):\n",
    "            if x is None:\n",
    "                h.set_data(np.arange(len(array[f][:, i])), array[f][:, i], **kwargs)\n",
    "            else:\n",
    "                h.set_data(x, array[f][:, i], **kwargs)\n",
    "        return plt_h\n",
    "\n",
    "    # call the animator. blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=frames, interval=interval,\n",
    "                                   blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "    rc('animation', html='html5')\n",
    "    display(HTML(anim.to_html5_video(embed_limit=1024)))\n",
    "\n",
    "    if not bar == \"\":\n",
    "        widget.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/cnn_tpu_training.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib import summary\n",
    "from tensorflow.contrib.training.python.training import evaluation\n",
    "\n",
    "def conv_nn(input, return_layers=False, filters=(16, 16, 16), kernel_size=(121, 121, 121), strides=(1, 1, 1), padding='valid', activation=tf.nn.softplus, **kwargs):\n",
    "    layers_list = []\n",
    "    value = tf.expand_dims(input, axis=-1)\n",
    "    value = tf.expand_dims(value, axis=2)\n",
    "    layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "    \n",
    "    assert len(filters) == len(kernel_size)\n",
    "    layers = len(filters)\n",
    "    \n",
    "    for l in range(layers -1):\n",
    "        value = tf.layers.conv2d(value, filters=filters[l], kernel_size=(kernel_size[l], 1), strides=strides[l], activation=activation, padding=padding, **kwargs)\n",
    "        layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "        \n",
    "    value = tf.layers.conv2d(value, filters=filters[-1], kernel_size=(kernel_size[-1], 1), padding=padding, **kwargs)\n",
    "    value = tf.reduce_sum(value, axis=2)\n",
    "    layers_list.append(value)\n",
    "    \n",
    "    value = tf.reduce_sum(value, axis=2)\n",
    "    value = tf.reduce_sum(value, axis=1)\n",
    "    layers_list.append(value)\n",
    "    \n",
    "    if return_layers:\n",
    "        return value, layers_list\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "class SineWaveInitializer(tf.initializers.variance_scaling):\n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        G = shape[0]\n",
    "        shape[0] = 1\n",
    "\n",
    "        weights = super().__call__(shape=shape, dtype=dtype, partition_info=partition_info)\n",
    "\n",
    "        lin = tf.reshape(tf.linspace(0.0, np.pi, G), (G, 1, 1, 1))\n",
    "        freq = tf.reshape(tf.range(shape[-1], dtype=tf.float32)+2, (1, 1, 1, shape[-1]))\n",
    "        sine = tf.sin(lin*freq)/G\n",
    "\n",
    "        return sine*weights\n",
    "\n",
    "\n",
    "def learning_rate_schedule(params, global_step):\n",
    "    batches_per_epoch = params['train_total_size'] / params['train_batch_size']\n",
    "    current_epoch = tf.cast((tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n",
    "\n",
    "    initial_learning_rate = params['learning_rate']\n",
    "\n",
    "    if params['use_learning_rate_warmup']:\n",
    "        warmup_decay = params['learning_rate_decay']**(\n",
    "        (params['warmup_epochs'] + params['cold_epochs']) /\n",
    "        params['learning_rate_decay_epochs'])\n",
    "        adj_initial_learning_rate = initial_learning_rate * warmup_decay\n",
    "\n",
    "    final_learning_rate = params['final_learning_rate_factor'] * initial_learning_rate\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=initial_learning_rate,\n",
    "        global_step=global_step,\n",
    "        decay_steps=int(params['learning_rate_decay_epochs'] * batches_per_epoch),\n",
    "        decay_rate=params['learning_rate_decay'],\n",
    "        staircase=True)\n",
    "\n",
    "    if params['use_learning_rate_warmup']:\n",
    "        wlr = 0.1 * adj_initial_learning_rate\n",
    "        wlr_height = tf.cast(0.9 * adj_initial_learning_rate / \n",
    "                                (params['warmup_epochs'] + params['learning_rate_decay_epochs'] - 1), tf.float32)\n",
    "        \n",
    "        epoch_offset = tf.cast(params['cold_epochs'] - 1, tf.int32)\n",
    "        exp_decay_start = (params['warmup_epochs'] + params['cold_epochs'] + params['learning_rate_decay_epochs'])\n",
    "\n",
    "        lin_inc_lr = tf.add(wlr, tf.multiply(tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32), wlr_height))\n",
    "\n",
    "        learning_rate = tf.where(\n",
    "            tf.greater_equal(current_epoch, params['cold_epochs']),\n",
    "            (tf.where(tf.greater_equal(current_epoch, exp_decay_start), learning_rate, lin_inc_lr)), \n",
    "            tf.ones_like(learning_rate)*wlr)\n",
    "\n",
    "    # Set a minimum boundary for the learning rate.\n",
    "    learning_rate = tf.maximum(learning_rate, final_learning_rate, name='learning_rate')\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def deriv_conv_nn_model_fn(features, labels, mode, params):\n",
    "\n",
    "    if isinstance(features, dict):\n",
    "        features = features['feature']\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    is_eval = (mode == tf.estimator.ModeKeys.EVAL)   \n",
    "\n",
    "    target_prediction = conv_nn(features, **params['kwargs'])\n",
    "    derivative_prediction = 1/params['h']*tf.gradients(target_prediction, features)[0]\n",
    "\n",
    "    predictions = {\n",
    "        'value': target_prediction,\n",
    "        'derivative': derivative_prediction\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs={\n",
    "            'regression': tf.estimator.export.PredictOutput(predictions)\n",
    "            })\n",
    "    \n",
    "    target, derivative = labels\n",
    "    \n",
    "    loss_y = tf.losses.mean_squared_error(target_prediction, target)\n",
    "    loss_gradient = tf.losses.mean_squared_error(derivative_prediction, derivative)\n",
    "\n",
    "    loss_l2 = []\n",
    "    for v in tf.trainable_variables():\n",
    "        if 'kernel' in v.name:\n",
    "            loss_l2.append(tf.nn.l2_loss(v))\n",
    "    loss_l2 = tf.add_n(loss_l2)\n",
    "    \n",
    "    loss = loss_y + params['balance']*loss_gradient\n",
    "    \n",
    "    if params['l2_loss'] > 0.0:\n",
    "        loss += params['l2_loss']*loss_l2\n",
    "\n",
    "    host_call = None\n",
    "    train_op = None\n",
    "\n",
    "    if is_training:\n",
    "        batches_per_epoch = params['train_total_size'] / params['train_batch_size']\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        current_epoch = tf.cast((tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n",
    "        learning_rate = learning_rate_schedule(params, global_step)\n",
    "        #tf.summary.scalar('lr', learning_rate) # doesn't work on TPU\n",
    "\n",
    "        if params['optimizer'] == 'Adam':\n",
    "            print('Using Adam optimizer')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            print('Using SGD optimizer')\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        elif params['optimizer'] == 'momentum':\n",
    "            print('Using Momentum optimizer')\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "        elif params['optimizer'] == 'RMS':\n",
    "            print('Using RMS optimizer')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        else:\n",
    "            tf.logging.fatal('Unknown optimizer:', params['optimizer'])\n",
    "\n",
    "        if params['gradient_clipping']:\n",
    "            optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, params['gradient_clip_norm'])\n",
    "\n",
    "        if params['use_tpu']:\n",
    "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "        # To log the loss, current learning rate, and epoch for Tensorboard, the\n",
    "        # summary op needs to be run on the host CPU via host_call. host_call\n",
    "        # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n",
    "        # dimension. These Tensors are implicitly concatenated to\n",
    "        # [params['batch_size']].\n",
    "        gs_t = tf.reshape(global_step, [1])\n",
    "        #loss_t = tf.reshape(loss, [1])\n",
    "        #loss_y_t = tf.reshape(loss_y, [1])\n",
    "        #loss_gradient_t = tf.reshape(loss_gradient, [1])\n",
    "        #loss_l2_t = tf.reshape(loss_l2, [1])\n",
    "        lr_t = tf.reshape(learning_rate, [1])\n",
    "        ce_t = tf.reshape(current_epoch, [1])\n",
    "\n",
    "        if not params['skip_host_call']:\n",
    "            def host_call_fn(gs, lr, ce):\n",
    "                gs = gs[0]\n",
    "                with summary.create_file_writer(params['model_dir']).as_default():\n",
    "                    with summary.always_record_summaries():\n",
    "                        #summary.scalar('loss', tf.reduce_mean(loss), step=gs)\n",
    "                        #summary.scalar('loss_y', tf.reduce_mean(loss_y), step=gs)\n",
    "                        #summary.scalar('loss_gradient', tf.reduce_mean(loss_gradient), step=gs)\n",
    "                        #summary.scalar('loss_l2', tf.reduce_mean(loss_l2), step=gs)\n",
    "\n",
    "                        summary.scalar('learning_rate', tf.reduce_mean(lr), step=gs)\n",
    "                        summary.scalar('current_epoch', tf.reduce_mean(ce), step=gs)\n",
    "\n",
    "                        return summary.all_summary_ops()\n",
    "\n",
    "            host_call = (host_call_fn, [gs_t, lr_t, ce_t])\n",
    "\n",
    "    eval_metrics = None\n",
    "    if is_eval:\n",
    "        def metric_fn(target_prediction, target, derivative_prediction, derivative):\n",
    "            return {\n",
    "                'value_mae': tf.metrics.mean_absolute_error(target_prediction, target),\n",
    "                'derivative_mae': tf.metrics.mean_absolute_error(derivative_prediction, derivative),\n",
    "            }\n",
    "\n",
    "        eval_metrics = (metric_fn, [target_prediction, target, derivative_prediction, derivative])\n",
    "\n",
    "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        host_call=host_call,\n",
    "        eval_metrics=eval_metrics)\n",
    "\n",
    "\n",
    "def train(params, resolver, dataset_train, dataset_eval):\n",
    "    tpu_config = tf.contrib.tpu.TPUConfig(iterations_per_loop=params['iterations'], num_shards=params['num_shards'])\n",
    "\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        cluster=resolver,\n",
    "        model_dir=params['model_dir'],\n",
    "        tf_random_seed=params['seed'],\n",
    "        save_checkpoints_secs=params['save_checkpoints_secs'],\n",
    "        keep_checkpoint_max=params['keep_checkpoint_max'],\n",
    "        save_summary_steps=params['save_summary_steps'],\n",
    "        session_config=tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=params['log_device_placement']),\n",
    "        tpu_config=tpu_config)\n",
    "\n",
    "    model = tf.contrib.tpu.TPUEstimator(\n",
    "        model_fn=params['model_fn'],\n",
    "        use_tpu=params['use_tpu'],\n",
    "        config=run_config,\n",
    "        params=params,\n",
    "        train_batch_size=params['train_batch_size'],\n",
    "        eval_batch_size=params['eval_batch_size'])\n",
    "\n",
    "    print('Training for {} steps with batch size {}, returning to CPU every {} steps\\n'\n",
    "        'summary every {} steps, saving every {} seconds.'.format(params['train_steps'], params['train_batch_size'], params['iterations'], \n",
    "                                                                    params['save_summary_steps'], params['save_checkpoints_secs']))\n",
    "\n",
    "    latest_checkpoint = model.latest_checkpoint()\n",
    "    current_step = int(latest_checkpoint.split('-')[-1]) if latest_checkpoint is not None else 0\n",
    "    while current_step < params['train_steps']:\n",
    "        train_steps = params['train_steps_per_eval'] if current_step % params['train_steps_per_eval'] == 0 else \\\n",
    "                                                        params['train_steps_per_eval'] - current_step % params['train_steps_per_eval']\n",
    "        cycle = current_step // params['train_steps_per_eval']\n",
    "        print('Starting training cycle {} - training for {} steps.'.format(cycle, train_steps))\n",
    "        model.train(input_fn=dataset_train.input_fn, steps=train_steps)\n",
    "        current_step += train_steps\n",
    "\n",
    "        print('Starting evaluation cycle {}.'.format(cycle))\n",
    "        eval_results = model.evaluate(input_fn=dataset_eval.input_fn, steps=params['eval_total_size'] // params['eval_batch_size'])\n",
    "        print('Evaluation results: {}'.format(eval_results))\n",
    "\n",
    "    def serving_input_receiver_fn():\n",
    "        features = tf.placeholder(dtype=tf.float32, shape=[None] + list(dataset_eval.features_shape()[1:]))\n",
    "        receiver_tensors = {'features': features}\n",
    "        return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "    export_path = os.path.join(params['model_dir'], 'saved_model')\n",
    "    print(\"Exporting model to {} with input placeholder {}\".format(export_path, [None] + list(dataset_eval.features_shape()[1:])))\n",
    "    model.export_saved_model(export_path, serving_input_receiver_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "0_define_helper_functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
