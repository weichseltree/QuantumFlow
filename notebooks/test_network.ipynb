{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.DEBUG)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install -q ruamel.yaml\n",
    "    !pip install -q tensorboard-plugin-profile\n",
    "    project_path = '/content/drive/MyDrive/Colab Projects/QuantumFlow'\n",
    "except:\n",
    "    project_path = os.path.expanduser('~/QuantumFlow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tree\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import quantumflow\n",
    "\n",
    "experiment = 'xdiff_perciever'\n",
    "run_name = 'debug_x'\n",
    "epoch = 1000\n",
    "\n",
    "preview = 5\n",
    "\n",
    "base_dir = os.path.join(project_path, \"experiments\", experiment)\n",
    "params = quantumflow.utils.load_yaml(os.path.join(base_dir, 'hyperparams.yaml'))[run_name]\n",
    "run_dir = os.path.join(base_dir, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validate = quantumflow.instantiate(params['dataset_validate'], run_dir=run_dir)\n",
    "dataset_validate.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantumflow.instantiate(params['model'], run_dir=run_dir, dataset=dataset_validate) # TODO: fix missing imports\n",
    "#model = tf.keras.models.load_model(os.path.join(run_dir, 'saved_model'))\n",
    "if epoch is not None: _ = model.load_weights(os.path.join(run_dir, params['checkpoint']['filename'].format(epoch=epoch)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, features, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        return tree.map_structure(lambda out: out.numpy(), model(features))\n",
    "    else:\n",
    "        outputs = []\n",
    "        steps = -(-tree.flatten(features)[0].shape[0]//batch_size)\n",
    "        print_steps = steps//100\n",
    "        print('/', steps)\n",
    "        for i in range(steps):\n",
    "            if i % print_steps == 0: print(i, end=' ')\n",
    "            features_batch = tree.map_structure(lambda inp: inp[i*batch_size:(i+1)*batch_size], features)\n",
    "            outputs.append(model(features_batch))\n",
    "        print()\n",
    "        return tree.map_structure(lambda *outs: np.concatenate(outs, axis=0), *outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_pred = predict(model, dataset_validate.features, params['dataset_validate'].get('max_batch_size', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_pred['kinetic_energy'][:preview]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validate.targets['kinetic_energy'][:preview]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetic_energy_err = targets_pred['kinetic_energy'] - dataset_validate.targets['kinetic_energy'][:len(targets_pred['kinetic_energy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcalmol_per_hartree = 627.5094738898777\n",
    "np.mean(np.abs(kinetic_energy_err))*kcalmol_per_hartree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(dataset_validate.x, dataset_validate.targets['kinetic_energy_density'][:preview, :].transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(dataset_validate.x, targets_pred['kinetic_energy_density'][:preview, :].transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(dataset_validate.x, dataset_validate.targets['kinetic_energy_density'][:preview, :].transpose() - targets_pred['kinetic_energy_density'][:preview, :].transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')['peak']/1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantumflow.utils import anim_plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tree\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features = tree.map_structure(lambda feature: feature[10:11], dataset_validate.features)\n",
    "sample_targets = tree.map_structure(lambda target: target[10:11], dataset_validate.targets)\n",
    "sample_targets_pred = model(sample_features)\n",
    "sample = tree.map_structure(lambda target, target_pred: (target[0], target_pred.numpy()[0]), sample_targets, sample_targets_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_path, (target, target_pred) in tree.flatten_with_path_up_to(sample_targets, sample):\n",
    "    target_name = '/'.join(target_path)\n",
    "    \n",
    "    if np.squeeze(target).shape == dataset_validate.x.shape:\n",
    "\n",
    "        plt.figure(figsize=(20, 3))\n",
    "        plt.plot(dataset_validate.x, target, 'k')\n",
    "        plt.plot(dataset_validate.x, np.squeeze(target_pred))\n",
    "        plt.title(target_name)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{target_name}: {target_pred} ({target})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = model.layers[0](sample_features)\n",
    "x, x_inputs, inputs = model.layers[1](latents['density'])\n",
    "x = model.layers[2](x)\n",
    "\n",
    "self = model.layers[3]\n",
    "\n",
    "x_token = self.x_token # (d_model)\n",
    "for shape in tf.unstack(tf.shape(x))[:-2]:\n",
    "    x_token = tf.repeat(tf.expand_dims(x_token, axis=-3), shape, axis=-3) # (..., latent_size, d_model)\n",
    "x_token = tf.repeat(x_token, tf.shape(x)[-2], axis=0)\n",
    "\n",
    "x_outputs = tf.repeat(x, params['model']['latents_per_x'], axis=-2)\n",
    "xdiff = quantumflow.xdiff.get_xdiff(x, x)/params['model']['scale'] # (..., latent_size, latent_size)\n",
    "xdiff_cross = quantumflow.xdiff.get_xdiff(x, x_inputs)/params['model']['scale'] # (..., latent_size, input_size)\n",
    "\n",
    "latents = x_token\n",
    "\n",
    "layers = []\n",
    "for r in range(params['model']['num_repeats']):\n",
    "    for i in range(params['model']['num_layers']):\n",
    "        layers.append(self.enc_layers[r][i])\n",
    "    layers.append(self.cross_enc_layers[r])\n",
    "\n",
    "for i in range(params['model']['num_layers']):\n",
    "    layers.append(self.enc_layers[params['model']['num_repeats']][i])\n",
    "\n",
    "layers.append(self.layernorm)\n",
    "\n",
    "for layer in self.pre_final_layers:\n",
    "    layers.append(layer)\n",
    "\n",
    "layers.append(self.final_layer)\n",
    "\n",
    "    \n",
    "for i in range(latents.shape[2]):\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.plot(latents[0, :, i, :])\n",
    "    plt.title(f\"Latents {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}\")\n",
    "    plt.show()\n",
    "                \n",
    "for self in layers:\n",
    "    print(self.name)\n",
    "    if 'encoder' in self.name:\n",
    "        if 'cross' in self.name:\n",
    "\n",
    "            inp = inputs\n",
    "            for layer in self.input_layers:\n",
    "                inp = layer(inp)\n",
    "\n",
    "            lat = self.layernorm1a(latents)\n",
    "            inp = self.layernorm1b(inp)\n",
    "                \n",
    "            for i in range(lat.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(lat[0, :, i, :])\n",
    "                plt.title(f\"Normalized Latents\")\n",
    "                plt.show()\n",
    "                \n",
    "            plt.figure(figsize=(20, 3))\n",
    "            plt.plot(inp[0, 0, :, :])\n",
    "            plt.title('Normalized Inputs')\n",
    "            plt.show()\n",
    "                \n",
    "            attn_output, attention = self.mha(lat, inp, inp, xdiff_cross, mask=None)  # (..., latent_size, d_model)\n",
    "\n",
    "            for i in range(attention.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.imshow(attention[0, :, i, 0, :], norm=matplotlib.colors.Normalize(vmin=0, vmax=0.01, clip=False), aspect=1.0)\n",
    "                plt.show()\n",
    "                print(f'Attention Map {np.std(attention[0, :, i, 0, :]):.3f}')\n",
    "                \n",
    "            for i in range(attn_output.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(attn_output[0, :, i, :])\n",
    "                plt.title(f'Attention Output {np.mean(attn_output[0, :, i, :]):.3f} {np.std(attn_output[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            attn_output = self.dropout1(attn_output, training=True)\n",
    "            \n",
    "            latents = latents + attn_output\n",
    "            \n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip Attn Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            lat = self.layernorm2(latents)  # (..., latent_size, d_model)\n",
    "            ffn_output = self.ffn[1](self.ffn[0](lat))  # (..., input_size, d_model)\n",
    "            ffn_output = self.dropout2(ffn_output, training=True)\n",
    "\n",
    "            latents = latents + ffn_output\n",
    "            \n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip FFN Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "        else:\n",
    "            lat = self.layernorm1(latents)  # (..., input_size, d_model)\n",
    "                \n",
    "            for i in range(lat.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(lat[0, :, i, :])\n",
    "                plt.title('Normalized Latents')\n",
    "                plt.show()\n",
    "\n",
    "            attn_output, attention = self.mha(lat, lat, lat, xdiff, mask=None)  # (..., input_size, d_model)\n",
    "\n",
    "            if attention.shape[-1] > 1:\n",
    "                for i in range(attention.shape[2]):\n",
    "\n",
    "                    plt.figure(figsize=(20, 3))\n",
    "                    plt.imshow(attention[0, :, i, 0, :], norm=matplotlib.colors.Normalize(vmin=0, vmax=0.01, clip=False), aspect=1.0)\n",
    "                    plt.show()\n",
    "                    print(f'Attention Map {np.std(attention[0, :, i, 0, :]):.3f}')\n",
    "            else:\n",
    "                print(attention[0, :, :, 0, 0].numpy())\n",
    "            \n",
    "            for i in range(attn_output.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(attn_output[0, :, i, :])\n",
    "                plt.title(f'Attention Output {np.mean(attn_output[0, :, i, :]):.3f} {np.std(attn_output[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            attn_output = self.dropout1(attn_output, training=True)\n",
    "            \n",
    "            latents = latents + attn_output\n",
    "\n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip Attn Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            lat = self.layernorm2(latents)  # (..., input_size, d_model)\n",
    "            ffn_output = self.ffn[1](self.ffn[0](lat))  # (..., input_size, d_model)\n",
    "            ffn_output = self.dropout2(ffn_output, training=True)\n",
    "            \n",
    "            latents = latents + ffn_output\n",
    "            \n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip FFN Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "    else:\n",
    "        latents = self(latents)\n",
    "\n",
    "kinetic_energy_density = tf.reduce_sum(latents[..., 0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = None\n",
    "tensors = {}\n",
    "\n",
    "def plot_layer(layer):\n",
    "    global value, tensors\n",
    "    \n",
    "    if isinstance(layer, tf.keras.Model):\n",
    "        tree.traverse(plot_layer, layer.layers)\n",
    "\n",
    "    elif isinstance(layer, tf.keras.layers.InputLayer):\n",
    "        value = sample_features[layer.name]\n",
    "        tensors[layer.output.name] = value\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.Layer):\n",
    "        if isinstance(layer.input, list):\n",
    "            value = layer([tensors[inp.name] for inp in layer.input])\n",
    "        else:\n",
    "            value = layer(tensors[layer.input.name])\n",
    "        tensors[layer.output.name] = value\n",
    "\n",
    "_ = tree.traverse(plot_layer, model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name, tensor in tensors.items():\n",
    "    if len(tensor.shape) == 3:\n",
    "        plt.figure(figsize=(20, 3))\n",
    "        plt.plot(tensor[0])\n",
    "        plt.title(layer_name)\n",
    "        plt.show()\n",
    "    elif np.prod(tensor.shape) < 100:\n",
    "        print(layer_name, tensor)\n",
    "    else:\n",
    "        print(layer_name, tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnjoWzd/gc4HbCp8FvSubN",
   "collapsed_sections": [],
   "name": "train_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
