{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate M random potentials on [0, L] as sum of n gaussian kernels on G discretization points\n",
    "\n",
    "\n",
    "there are 3 uniformly distributed random parameters a, b, c\n",
    "the potential is calculated by:\n",
    "\n",
    "$$\n",
    "V(x) = -\\sum_{n=1}^{3} a_n \\exp \\left(-\\frac{(x - b_n)^2}{2c_n^2} \\right)\n",
    "$$\n",
    "\n",
    "### Then solve the schroedinger equation in atomic units for 4 spinless non-interacting fermions.\n",
    "\n",
    "The shooting numerov algorithm calculates a forward pass and a backward pass solution.\n",
    "\n",
    "The final solution is then found by aligning the two passes and finding a good split point to concatenate them.\n",
    "\n",
    "All integrals are approximated by the trapezoidal method.\n",
    "The numerical differentiation for the laplacian uses the standard approximation.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "Setting the dataset variable to different values will allow generating all the variations of the datasets:\n",
    "standard, recreate, recreate/fine_0.125, ..., recreate/fine_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup notebook if it is run on Google Colab, cwd = notebook file location\n",
    "try:\n",
    "    # change notebook_path if this notebook is in a different subfolder of Google Drive\n",
    "    notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q ruamel.yaml\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# imports\n",
    "import tensorflow as tf\n",
    "\n",
    "# setup paths and variables for shared code (../quantumflow) and data (../data)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# import shared code, must run 0_create_shared_project_files.ipynb first!\n",
    "from quantumflow.generate_datasets import generate_potentials, generate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"datasets\"\n",
    "base_dir = os.path.join(data_dir, experiment)\n",
    "if not os.path.exists(base_dir): os.makedirs(base_dir)\n",
    "file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_hyperparams\n",
    "globals: []\n",
    "\n",
    "test: &DEFAULT\n",
    "    filename: dataset_test\n",
    "    format: hdf5 # pickle, hdf5\n",
    "    seed: 0\n",
    "    n_orbitals: 4 # N\n",
    "    dataset_size: 1000 # M\n",
    "    batch_size: 10000\n",
    "\n",
    "    discretisation_points: 500 # G\n",
    "    interval_length: 1.0 # [Bohr]\n",
    "    n_gauss: 3 # number of gaussians \n",
    "    n_method: sum # sum or mean the gaussians\n",
    "    a_minmax: [1.0, 10.0] # [hartree] \n",
    "    b_minmax: [0.4, 0.6] # nomalized x\n",
    "    c_minmax: [0.03, 0.1] # nomalized x\n",
    "\n",
    "    dtype: float64\n",
    "    numerov_init_slope: 5.0\n",
    "\n",
    "test_small:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_test_small\n",
    "    seed: 0\n",
    "    dataset_size: 100\n",
    "\n",
    "validate:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_validate\n",
    "    seed: -1\n",
    "    dataset_size: 1000\n",
    "\n",
    "fine:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_fine\n",
    "    discretisation_points: 4991\n",
    "\n",
    "float:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_float\n",
    "    dtype: float32\n",
    "\n",
    "medium: \n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_medium\n",
    "    seed: 1\n",
    "    dataset_size: 500\n",
    "\n",
    "large:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_large\n",
    "    seed: 1\n",
    "    dataset_size: 1000\n",
    "\n",
    "larger:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_larger\n",
    "    seed: 1\n",
    "    dataset_size: 10000\n",
    "\n",
    "huge:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_huge\n",
    "    seed: 1\n",
    "    dataset_size: 50000\n",
    "\n",
    "huger:\n",
    "    <<: *DEFAULT\n",
    "    filename: dataset_huger\n",
    "    seed: 1\n",
    "    dataset_size: 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_datasets(data_dir=data_dir, experiment='datasets', generate_names=['test', 'validate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPzE9AlqF3RF11P8hnBxNHY",
   "collapsed_sections": [],
   "name": "1a_generate_datasets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
