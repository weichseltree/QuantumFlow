{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3a_cnn_training_kenergy","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"co5FqBonrIoC","colab_type":"code","outputId":"a478f675-70f6-4aa3-a121-0e13e7abb13f","executionInfo":{"status":"ok","timestamp":1572902708184,"user_tz":-60,"elapsed":10465,"user":{"displayName":"Manuel Weichselbaum","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCzMR-Tpz-d20xDEyvFKqApTdbEtqQvSmddyJ_MQw=s64","userId":"06267406876874183847"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["notebook_path = \"Projects/QuantumFlow/notebooks\"\n","try:\n","    import os\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n","except:\n","    pass\n","\n","!pip install -q --upgrade tensorflow\n","!pip install -q ruamel.yaml\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import os\n","%matplotlib inline\n","\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","import sys\n","sys.path.append('../')\n","\n","from quantumflow.generate_datasets import generate_potentials\n","from quantumflow.calculus_utils import integrate, laplace, np_integrate\n","from quantumflow.numerov_solver import *\n","from quantumflow.colab_train_utils import load_hyperparameters, test_colab_devices, get_resolver, InputPipeline\n","\n","has_gpu, has_tpu = test_colab_devices()\n","if has_gpu: print(\"Found GPU\")\n","if has_tpu: print(\"Found TPU\")\n","\n","data_dir = \"../data\"\n","%load_ext tensorboard"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jGy1prthyNPS","colab_type":"code","colab":{}},"source":["def learning_rate_schedule(params, global_step):\n","    batches_per_epoch = params['train_total_size'] / params['train_batch_size']\n","    current_epoch = tf.cast((tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n","\n","    initial_learning_rate = params['learning_rate']\n","\n","    if params['use_learning_rate_warmup']:\n","        warmup_decay = params['learning_rate_decay']**(\n","        (params['warmup_epochs'] + params['cold_epochs']) /\n","        params['learning_rate_decay_epochs'])\n","        adj_initial_learning_rate = initial_learning_rate * warmup_decay\n","\n","    final_learning_rate = params['final_learning_rate_factor'] * initial_learning_rate\n","\n","    learning_rate = tf.train.exponential_decay(\n","        learning_rate=initial_learning_rate,\n","        global_step=global_step,\n","        decay_steps=int(params['learning_rate_decay_epochs'] * batches_per_epoch),\n","        decay_rate=params['learning_rate_decay'],\n","        staircase=True)\n","\n","    if params['use_learning_rate_warmup']:\n","        wlr = 0.1 * adj_initial_learning_rate\n","        wlr_height = tf.cast(0.9 * adj_initial_learning_rate / \n","                                (params['warmup_epochs'] + params['learning_rate_decay_epochs'] - 1), tf.float32)\n","        \n","        epoch_offset = tf.cast(params['cold_epochs'] - 1, tf.int32)\n","        exp_decay_start = (params['warmup_epochs'] + params['cold_epochs'] + params['learning_rate_decay_epochs'])\n","\n","        lin_inc_lr = tf.add(wlr, tf.multiply(tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32), wlr_height))\n","\n","        learning_rate = tf.where(\n","            tf.greater_equal(current_epoch, params['cold_epochs']),\n","            (tf.where(tf.greater_equal(current_epoch, exp_decay_start), learning_rate, lin_inc_lr)), \n","            tf.ones_like(learning_rate)*wlr)\n","\n","    # Set a minimum boundary for the learning rate.\n","    learning_rate = tf.maximum(learning_rate, final_learning_rate, name='learning_rate')\n","\n","    return learning_rate\n","\n","def conv_nn(features, params):\n","    layers = len(params['kwargs']['filters'])\n","\n","    value = tf.expand_dims(features['density'], axis=-1)\n","    value = tf.expand_dims(value, axis=2)\n","\n","    for l in range(layers):\n","        value = tf.keras.layers.Conv2D(filters=params['kwargs']['filters'][l], \n","                                       kernel_size=(params['kwargs']['kernel_size'][l], 1), \n","                                       strides=params['kwargs'].get('strides', [1]*layers)[l], \n","                                       dilation_rate=params['kwargs'].get('dilation_rate', [1]*layers)[l], \n","                                       activation=params['kwargs'].get('activation', 'softplus'), \n","                                       padding=params['kwargs'].get('padding'))(value)\n","\n","    value = tf.keras.layers.Flatten()(value)\n","    value = tf.keras.layers.Dense(1)(value)\n","                          \n","    return {'kinetic_energy': tf.reduce_mean(value, axis=-1)}\n","\n","def deriv_conv_nn(features, params):\n","    predictions = conv_nn(features, params)\n","    predictions['derivative'] = 1/params['h']*tf.gradients(predictions['kinetic_energy'], features['density'])[0]\n","    return predictions\n","\n","def model_fn(features, labels, mode, params):\n","\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","    is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n","    is_predict = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","    predictions = params['nn_fn'](features, params)\n","    \n","    if is_predict:\n","        return tf.estimator.EstimatorSpec(\n","            mode=mode,\n","            predictions=predictions,\n","            export_outputs={\n","                'regression': tf.estimator.export.PredictOutput(predictions)\n","            })\n","    \n","    losses = {}\n","    \n","    for key in labels.keys():\n","        loss_fn = getattr(tf.losses, params['loss'][key])\n","        weight = params['loss'].get('weight', {key: 1}).get(key, 1)\n","        \n","        losses['loss/' + key] = weight*loss_fn(labels[key], predictions[key])\n","\n","    if params['loss'].get('l2', 0.0) > 0.0:\n","        for v in tf.trainable_variables():\n","            if 'kernel' in v.name:\n","                losses['loss/' + v.name] = params['loss']['l2']*tf.nn.l2_loss(v)\n","\n","    loss = tf.add_n(list(losses.values()))\n","\n","    if not params['use_tpu']:\n","        for key, loss in losses.items():\n","            tf.summary.scalar(key, loss)\n","\n","        for v in tf.trainable_variables():\n","            tf.summary.histogram(v.name, v)\n","    \n","    host_call = None\n","    train_op = None\n","\n","    if is_training:\n","        batches_per_epoch = params['train_total_size'] / params['train_batch_size']\n","        global_step = tf.train.get_or_create_global_step()\n","        current_epoch = tf.cast((tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n","        learning_rate = learning_rate_schedule(params, global_step)\n","\n","        if not params['use_tpu']:\n","            tf.summary.scalar('learning_rate', learning_rate)\n","\n","        optimizer = getattr(tf.train, params['optimizer'])(learning_rate=learning_rate)\n","\n","        if params['use_tpu']:\n","            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n","\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","\n","        with tf.control_dependencies(update_ops):\n","            gradients, variables = zip(*optimizer.compute_gradients(loss))\n","\n","            if params['gradient_clipping']:\n","                gradients, global_norm = tf.clip_by_global_norm(gradients, params['gradient_clip_norm'])\n","\n","                if not params['use_tpu']:\n","                    tf.summary.scalar('global_norm', global_norm)\n","\n","            train_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n","\n","    eval_metrics = None\n","    if is_eval and len(params.get('metrics', {})) > 0:\n","\n","        def metric_fn(predictions, labels):\n","            return_dict = {}\n","            for key in params['metrics'].keys():\n","                return_dict['metrics/' + key] = getattr(tf.metrics, params['metrics'][key])(predictions[key], labels[key])\n","            return return_dict\n","\n","        if params['use_tpu']:\n","            eval_metrics = (metric_fn, [{key: predictions[key] for key in params['metrics']}, {key: labels[key] for key in params['metrics']}])\n","        else:\n","            eval_metrics = metric_fn({key: predictions[key] for key in params['metrics']}, {key: labels[key] for key in params['metrics']})\n","\n","    if params['use_tpu']:\n","        return tf.contrib.tpu.TPUEstimatorSpec(\n","            mode=mode,\n","            loss=loss,\n","            train_op=train_op,\n","            host_call=host_call,\n","            eval_metrics=eval_metrics,\n","        )\n","    else:\n","        #training_hooks = []\n","\n","        #train_summary_hook = tf.train.SummarySaverHook(\n","        #   save_steps = params['save_summary_steps'],\n","        #    output_dir = params['model_dir'],\n","        #    summary_op=tf.summary.merge_all()\n","        #    )\n","        \n","        #training_hooks.append(train_summary_hook)\n","\n","        return tf.estimator.EstimatorSpec(\n","            mode=mode,\n","            loss=loss,\n","            train_op=train_op,\n","            eval_metric_ops=eval_metrics,\n","            #training_hooks=training_hooks\n","        )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6S42ff3p3WHK","colab_type":"code","colab":{}},"source":["data_dir = \"../data\"\n","experiment = 'ke_cnn'\n","\n","base_dir = os.path.join(data_dir, experiment)\n","log_dir = \"gs://quantumflow/\" + experiment if has_tpu else '/home/' + experiment\n","if not os.path.exists(base_dir): os.makedirs(base_dir)\n","file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MA7DIqN43YHG","colab_type":"code","outputId":"6918e7ec-735b-45b4-f5ad-f3deb77a614f","executionInfo":{"status":"ok","timestamp":1572902709121,"user_tz":-60,"elapsed":11375,"user":{"displayName":"Manuel Weichselbaum","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCzMR-Tpz-d20xDEyvFKqApTdbEtqQvSmddyJ_MQw=s64","userId":"06267406876874183847"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile $file_hyperparams\n","globals: [model_fn, conv_nn, deriv_conv_nn, has_tpu]\n","\n","default: &DEFAULT\n","    dataset_train: recreate/dataset_paper\n","    dataset_validate: recreate/dataset_validate\n","    N: 1\n","    seed: 0\n","    dtype: float32\n","\n","    model_fn: model_fn #globals\n","    nn_fn: deriv_conv_nn\n","\n","    features: ['density']\n","    targets: ['kinetic_energy', 'derivative']\n","\n","    loss: &DEFAULT_LOSS\n","        l2: 0.0 #0005\n","        kinetic_energy: mean_squared_error\n","        derivative: mean_squared_error\n","\n","        weight:\n","            kinetic_energy: 1\n","            derivative: 0.2\n","\n","    metrics:\n","        kinetic_energy: mean_absolute_error\n","        derivative: mean_absolute_error\n","\n","    kwargs: &DEFAULT_KWARGS\n","        filters: [32, 32, 32, 32, 32]\n","        kernel_size: [100, 100, 100, 100, 100]\n","        padding: valid\n","        activation: softplus\n","        kernel_initializer: null\n","        strides: [1, 1, 1, 1, 1]\n","        dilation_rate: [1, 1, 1, 1, 1]\n","\n","    use_tpu: has_tpu #globals\n","    skip_host_call: True # makes everything run much faster\n","    num_shards: 8\n","    log_device_placement: True\n","    save_summary_steps: 10\n","    iterations: 10 # run this many steps until returning to CPU\n","\n","    shuffle: False\n","    shuffle_buffer_size: 1000\n","    drop_remainder: False\n","    eval_batch_size: 250\n","    predict_batch_size: 1\n","    train_batch_size: 10\n","\n","    train_steps: 100000\n","    train_steps_per_eval: 10000\n","    optimizer: AdamOptimizer\n","    gradient_clipping: True\n","    gradient_clip_norm: 100.0\n","\n","    learning_rate: 0.0005\n","    learning_rate_decay: 0.99\n","    final_learning_rate_factor: 0.0001\n","    learning_rate_decay_epochs: 100\n","    use_learning_rate_warmup: True\n","    cold_epochs: 1000\n","    warmup_epochs: 1000\n","\n","    predict_on_tpu: False\n","    save_checkpoints_secs: 600\n","    keep_checkpoint_max: 5\n","    eval_timeout: None\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Overwriting ../data/ke_cnn/hyperparams.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BAm2yhAVEpdl","colab_type":"code","outputId":"7f352173-33bb-4040-b0b6-363f61c568e5","executionInfo":{"status":"error","timestamp":1572902712786,"user_tz":-60,"elapsed":15026,"user":{"displayName":"Manuel Weichselbaum","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCzMR-Tpz-d20xDEyvFKqApTdbEtqQvSmddyJ_MQw=s64","userId":"06267406876874183847"}},"colab":{"base_uri":"https://localhost:8080/","height":766}},"source":["training_run_names = ['default']\n","\n","remote_file_hyperparams = os.path.join(log_dir, \"hyperparams.config\")\n","if log_dir.startswith('gs://'):\n","    !gsutil -q cp $file_hyperparams $remote_file_hyperparams\n","else:\n","    if not os.path.exists(log_dir): os.makedirs(log_dir)\n","    !cp $file_hyperparams $remote_file_hyperparams\n","\n","for run_name in training_run_names:\n","    params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n","\n","    dataset_train = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_train'] + '.pkl'), is_training=True)\n","    dataset_eval = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_validate'] + '.pkl'))\n","\n","    params['h'] = dataset_train.h\n","    params['train_total_size'] = dataset_train.dataset_size\n","    params['eval_total_size'] = dataset_eval.dataset_size\n","    params['model_dir'] = os.path.join(log_dir, run_name)\n","\n","    if params['use_tpu']:\n","        tpu_config = tf.contrib.tpu.TPUConfig(iterations_per_loop=params['iterations'], num_shards=params['num_shards'])\n","\n","        run_config = tf.contrib.tpu.RunConfig(\n","            cluster=get_resolver(),\n","            model_dir=params['model_dir'],\n","            tf_random_seed=params['seed'],\n","            save_checkpoints_secs=params['save_checkpoints_secs'],\n","            keep_checkpoint_max=params['keep_checkpoint_max'],\n","            save_summary_steps=params['save_summary_steps'],\n","            session_config=tf.ConfigProto(\n","                allow_soft_placement=True,\n","                log_device_placement=params['log_device_placement']),\n","            tpu_config=tpu_config)\n","\n","        model = tf.contrib.tpu.TPUEstimator(\n","            model_fn=params['model_fn'],\n","            use_tpu=params['use_tpu'],\n","            config=run_config,\n","            params=params,\n","            train_batch_size=params['train_batch_size'],\n","            eval_batch_size=params['eval_batch_size'])\n","    else:\n","        params['batch_size'] = params['train_batch_size']\n","\n","        run_config = tf.estimator.RunConfig(\n","            model_dir=params['model_dir'],\n","            tf_random_seed=params['seed'],\n","            save_checkpoints_secs=params['save_checkpoints_secs'],\n","            keep_checkpoint_max=params['keep_checkpoint_max'],\n","            save_summary_steps=params['save_summary_steps'],\n","            )\n","\n","        model = tf.estimator.Estimator(\n","            model_fn=params['model_fn'],\n","            params=params,\n","            config=run_config)\n","    \n","    latest_checkpoint = model.latest_checkpoint()\n","    current_step = int(latest_checkpoint.split('-')[-1]) if latest_checkpoint is not None else 0\n","\n","    try:\n","        while current_step < params['train_steps']:\n","            train_steps = params['train_steps_per_eval'] if current_step % params['train_steps_per_eval'] == 0 else \\\n","                                                            params['train_steps_per_eval'] - current_step % params['train_steps_per_eval']\n","            cycle = current_step // params['train_steps_per_eval']\n","            print('Starting training cycle {} - training for {} steps.'.format(cycle, train_steps))\n","            model.train(input_fn=dataset_train.input_fn, steps=train_steps)\n","            current_step += train_steps\n","\n","            print('Starting evaluation cycle {}.'.format(cycle))\n","            eval_results = model.evaluate(input_fn=dataset_eval.input_fn, steps=params['eval_total_size'] // params['eval_batch_size'])\n","            print('Evaluation results: {}'.format(eval_results))\n","\n","        params['batch_size'] = 1\n","        features_spec, labels_spec = dataset_eval.input_fn(params).element_spec\n","\n","        def serving_input_receiver_fn():\n","            features = {}\n","            for key in params['features']:\n","                features[key] = tf.placeholder(dtype=params['dtype'], shape=features_spec[key].shape)\n","\n","            return tf.estimator.export.ServingInputReceiver(features, features)\n","\n","        export_path = os.path.join(params['model_dir'], 'saved_model')\n","        print(\"Exporting model to {} with input placeholders {}\".format(export_path, features_spec))\n","        model.export_saved_model(export_path, serving_input_receiver_fn)\n","    except KeyboardInterrupt:\n","        pass\n","\n","    remote_model_dir = params['model_dir']\n","    if log_dir.startswith('gs://'):\n","        !gsutil -m cp -r $remote_model_dir $local_model_dir\n","    else:\n","        !cp -r $remote_model_dir $base_dir"],"execution_count":5,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': '/home/ke_cnn/default', '_tf_random_seed': 0, '_save_summary_steps': 10, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb033807550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","Starting training cycle 7 - training for 10000 steps.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","ordereddict([('dataset_train', 'recreate/dataset_paper'), ('dataset_validate', 'recreate/dataset_validate'), ('N', 1), ('seed', 0), ('dtype', 'float32'), ('model_fn', <function model_fn at 0x7fb036b5a378>), ('nn_fn', <function deriv_conv_nn at 0x7fb036b5a2f0>), ('features', ['density']), ('targets', ['kinetic_energy', 'derivative']), ('loss', ordereddict([('l2', 0.0), ('kinetic_energy', 'mean_squared_error'), ('derivative', 'mean_squared_error'), ('weight', ordereddict([('kinetic_energy', 1), ('derivative', 0.2)]))])), ('metrics', ordereddict([('kinetic_energy', 'mean_absolute_error'), ('derivative', 'mean_absolute_error')])), ('kwargs', ordereddict([('filters', [32, 32, 32, 32, 32]), ('kernel_size', [100, 100, 100, 100, 100]), ('padding', 'valid'), ('activation', 'softplus'), ('kernel_initializer', None), ('strides', [1, 1, 1, 1, 1]), ('dilation_rate', [1, 1, 1, 1, 1])])), ('use_tpu', False), ('skip_host_call', True), ('num_shards', 8), ('log_device_placement', True), ('save_summary_steps', 10), ('iterations', 10), ('shuffle', False), ('shuffle_buffer_size', 1000), ('drop_remainder', False), ('eval_batch_size', 250), ('predict_batch_size', 1), ('train_batch_size', 10), ('train_steps', 100000), ('train_steps_per_eval', 10000), ('optimizer', 'AdamOptimizer'), ('gradient_clipping', True), ('gradient_clip_norm', 100.0), ('learning_rate', 0.0005), ('learning_rate_decay', 0.99), ('final_learning_rate_factor', 0.0001), ('learning_rate_decay_epochs', 100), ('use_learning_rate_warmup', True), ('cold_epochs', 1000), ('warmup_epochs', 1000), ('predict_on_tpu', False), ('save_checkpoints_secs', 600), ('keep_checkpoint_max', 5), ('eval_timeout', 'None'), ('h', 0.0020040080416947603), ('train_total_size', 100), ('eval_total_size', 800), ('model_dir', '/home/ke_cnn/default'), ('batch_size', 10)])\n","INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'AddN' (op: 'AddN') with input shapes: [], [?].","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e2d3caf19d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mcycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_steps_per_eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting training cycle {} - training for {} steps.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1188\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1190\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1191\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-0b79a4c1533f>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_tpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36madd_n\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m   3016\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3018\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_n\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    475\u001b[0m   \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 477\u001b[0;31m         \"AddN\", inputs=inputs, name=name)\n\u001b[0m\u001b[1;32m    478\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3358\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3360\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m   def _create_op_internal(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1773\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1774\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'AddN' (op: 'AddN') with input shapes: [], [?]."]}]},{"cell_type":"code","metadata":{"id":"C02EY3ZzRttT","colab_type":"code","colab":{}},"source":["!ls -R $log_dir"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZ90uIJ_RxTy","colab_type":"code","colab":{}},"source":["%tensorboard --logdir $log_dir"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYImYlUivEHN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}