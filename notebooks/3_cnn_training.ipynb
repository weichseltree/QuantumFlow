{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "try:\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from quantumflow.colab_train_utils import load_hyperparameters, test_colab_devices, unpack_dataset, get_resolver, InputPipeline\n",
    "from quantumflow.cnn_tpu_training import SineWaveInitializer, learning_rate_schedule, deriv_conv_nn_model_fn, conv_nn, train\n",
    "has_gpu, has_tpu = test_colab_devices()\n",
    "if has_gpu: print(\"Found GPU\")\n",
    "if has_tpu: print(\"Found TPU\")\n",
    "\n",
    "if has_tpu:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "!pip install -q ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "experiment = 'ke_strides'\n",
    "training_run_names = ['strides_deep', 'strides_deep_1', 'strides_deep_2', 'strides_deeper', 'strides_deeper_1', 'strides_deeper_2']\n",
    "#['strides_test', 'strides_test_fast', 'strides_test_1', 'strides_test_2', 'strides_test_3']\n",
    "#['clip_1', 'clip_0.01', 'clip_10', 'clip_0.0001']\n",
    "#['deep_{}'.format(i) for i in range(10)]\n",
    "#['nrdm_5', 'nrdm_6', 'nrdm_7', 'nrdm_8', 'nrdm_9']#['seed_0', 'seed_1', 'seed_2', 'seed_3', 'seed_4', 'seed_5', 'seed_6', 'seed_7', 'seed_8', 'seed_9']\n",
    "\n",
    "base_dir = \"gs://quantumflow/\" +  experiment\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, experiment)): os.makedirs(os.path.join(data_dir, experiment))\n",
    "file_hyperparams = os.path.join(data_dir, experiment, \"hyperparams.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_hyperparams\n",
    "globals: [deriv_conv_nn_model_fn, SineWaveInitializer, has_tpu]\n",
    "\n",
    "seed_0: &DEFAULT\n",
    "    dataset_train: recreate_paper\n",
    "    dataset_val: recreate\n",
    "    N: 1\n",
    "    seed: 0\n",
    "\n",
    "    model_fn: deriv_conv_nn_model_fn #globals\n",
    "    kwargs: &DEFAULT_KWARGS\n",
    "        filters: [16, 8, 1]\n",
    "        kernel_size: [121, 121, 121]\n",
    "        padding: valid\n",
    "        activation: softplus\n",
    "        kernel_initializer: SineWaveInitializer #globals\n",
    "\n",
    "    shuffle: False\n",
    "    shuffle_buffer_size: 1000\n",
    "    eval_batch_size: 248\n",
    "    predict_batch_size: 248\n",
    "    train_batch_size: 96\n",
    "\n",
    "    train_steps: 1000000\n",
    "    train_steps_per_eval: 100000\n",
    "    optimizer: Adam\n",
    "    gradient_clipping: False\n",
    "    gradient_clip_norm: None\n",
    "    iterations: 100 # run this many steps until returning to CPU\n",
    "    balance: 0.005\n",
    "    l2_loss: 0.0\n",
    "\n",
    "    learning_rate: 0.0005\n",
    "    learning_rate_decay: 0.99\n",
    "    final_learning_rate_factor: 0.0001\n",
    "    learning_rate_decay_epochs: 1000\n",
    "    use_learning_rate_warmup: True\n",
    "    cold_epochs: 1000\n",
    "    warmup_epochs: 1000\n",
    "\n",
    "    predict_on_tpu: False\n",
    "    save_checkpoints_secs: 600\n",
    "    keep_checkpoint_max: 5\n",
    "    eval_timeout: None\n",
    "    \n",
    "    use_tpu: has_tpu #globals\n",
    "    skip_host_call: True # makes everything run much faster\n",
    "    num_shards: 8\n",
    "    log_device_placement: True\n",
    "    save_summary_steps: 100\n",
    "\n",
    "seed_1:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_1\n",
    "    seed: 1\n",
    "\n",
    "seed_2:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_2\n",
    "    seed: 2\n",
    "\n",
    "seed_3:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_3\n",
    "    seed: 3\n",
    "\n",
    "seed_4:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_4\n",
    "    seed: 4\n",
    "\n",
    "seed_5:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_5\n",
    "    seed: 5\n",
    "\n",
    "seed_6:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_6\n",
    "    seed: 6\n",
    "\n",
    "seed_7:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_7\n",
    "    seed: 7\n",
    "\n",
    "seed_8:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_8\n",
    "    seed: 8\n",
    "\n",
    "seed_9:\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate/seed_9\n",
    "    seed: 9\n",
    "\n",
    "nrdm_0: &NRDM_DEFAULT\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate_paper\n",
    "    seed: 0\n",
    "    kwargs: \n",
    "        <<: *DEFAULT_KWARGS\n",
    "        kernel_initializer: null\n",
    "\n",
    "nrdm_1:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_1\n",
    "    seed: 1\n",
    "\n",
    "nrdm_2:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_2\n",
    "    seed: 2\n",
    "\n",
    "nrdm_3:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_3\n",
    "    seed: 3\n",
    "\n",
    "nrdm_4:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_4\n",
    "    seed: 4\n",
    "\n",
    "nrdm_5:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_5\n",
    "    seed: 5\n",
    "\n",
    "nrdm_6:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_6\n",
    "    seed: 6\n",
    "\n",
    "nrdm_7:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_7\n",
    "    seed: 7\n",
    "\n",
    "nrdm_8:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_8\n",
    "    seed: 8\n",
    "\n",
    "nrdm_9:\n",
    "    <<: *NRDM_DEFAULT\n",
    "    dataset_train: recreate/seed_9\n",
    "    seed: 9\n",
    "\n",
    "deep_0: &DEEP_DEFAULT\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate_paper\n",
    "    learning_rate: 0.0002\n",
    "    seed: 0\n",
    "    kwargs:\n",
    "        filters: [16, 8, 4, 1]\n",
    "        kernel_size: [81, 81, 81, 81]\n",
    "        padding: valid\n",
    "        activation: softplus\n",
    "        kernel_initializer: SineWaveInitializer #globals\n",
    "\n",
    "deep_1:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_1\n",
    "    seed: 1\n",
    "\n",
    "deep_2:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_2\n",
    "    seed: 2\n",
    "    \n",
    "deep_3:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_3\n",
    "    seed: 3\n",
    "    \n",
    "deep_4:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_4\n",
    "    seed: 4\n",
    "    \n",
    "deep_5:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_5\n",
    "    seed: 5\n",
    "    \n",
    "deep_6:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_6\n",
    "    seed: 6\n",
    "    \n",
    "deep_7:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_7\n",
    "    seed: 7\n",
    "    \n",
    "deep_8:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_8\n",
    "    seed: 8\n",
    "    \n",
    "deep_9:\n",
    "    <<: *DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_9\n",
    "    seed: 9\n",
    "\n",
    "no_clip: \n",
    "    <<: *DEEP_DEFAULT\n",
    "    learning_rate: 0.0005\n",
    "    gradient_clipping: False\n",
    "    gradient_clip_norm: None\n",
    "\n",
    "clip_1: \n",
    "    <<: *DEEP_DEFAULT\n",
    "    learning_rate: 0.0005\n",
    "    gradient_clipping: True\n",
    "    gradient_clip_norm: 1.0\n",
    "\n",
    "clip_10: \n",
    "    <<: *DEEP_DEFAULT\n",
    "    learning_rate: 0.0005\n",
    "    gradient_clipping: True\n",
    "    gradient_clip_norm: 10.0\n",
    "\n",
    "clip_0.01: \n",
    "    <<: *DEEP_DEFAULT\n",
    "    learning_rate: 0.0005\n",
    "    gradient_clipping: True\n",
    "    gradient_clip_norm: 0.01\n",
    "\n",
    "clip_0.0001: \n",
    "    <<: *DEEP_DEFAULT\n",
    "    learning_rate: 0.0005\n",
    "    gradient_clipping: True\n",
    "    gradient_clip_norm: 0.0001\n",
    "\n",
    "strides_test:  &STRIDES_DEFAULT\n",
    "    <<: *DEFAULT\n",
    "    dataset_train: recreate_paper\n",
    "    learning_rate: 0.0005\n",
    "    seed: 0\n",
    "    gradient_clipping: True\n",
    "    gradient_clip_norm: 100.0\n",
    "\n",
    "    kwargs:\n",
    "        filters: [64, 16, 1]\n",
    "        kernel_size: [5, 25, 75]\n",
    "        padding: same\n",
    "        activation: softplus\n",
    "        kernel_initializer: null\n",
    "        strides: [2, 2, 2]\n",
    "\n",
    "strides_test_fast:\n",
    "    <<: *STRIDES_DEFAULT\n",
    "    learning_rate: 0.005\n",
    "\n",
    "strides_test_1:\n",
    "    <<: *STRIDES_DEFAULT\n",
    "    dataset_train: recreate/seed_1\n",
    "    seed: 1\n",
    "\n",
    "strides_test_2:\n",
    "    <<: *STRIDES_DEFAULT\n",
    "    dataset_train: recreate/seed_2\n",
    "    seed: 2\n",
    "    \n",
    "strides_test_3:\n",
    "    <<: *STRIDES_DEFAULT\n",
    "    dataset_train: recreate/seed_3\n",
    "    seed: 3\n",
    "    \n",
    "strides_deep: &STRIDES_DEEP_DEFAULT\n",
    "    <<: *STRIDES_DEFAULT\n",
    "    kwargs:\n",
    "        filters: [64, 32, 16, 1]\n",
    "        kernel_size: [5, 15, 25, 75]\n",
    "        padding: same\n",
    "        activation: softplus\n",
    "        kernel_initializer: null\n",
    "        strides: [2, 2, 2, 2]\n",
    "\n",
    "strides_deep_1:\n",
    "    <<: *STRIDES_DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_1\n",
    "    seed: 1\n",
    "\n",
    "strides_deep_2:\n",
    "    <<: *STRIDES_DEEP_DEFAULT\n",
    "    dataset_train: recreate/seed_2\n",
    "    seed: 2\n",
    "\n",
    "strides_deeper: &DEEPER_DEFAULT\n",
    "    <<: *STRIDES_DEFAULT\n",
    "    kwargs:\n",
    "        filters: [64, 48, 32, 16, 1]\n",
    "        kernel_size: [5, 10, 15, 25, 75]\n",
    "        padding: same\n",
    "        activation: softplus\n",
    "        kernel_initializer: null\n",
    "        strides: [2, 2, 2, 2, 2]\n",
    "\n",
    "strides_deeper_1:\n",
    "    <<: *DEEPER_DEFAULT\n",
    "    dataset_train: recreate/seed_1\n",
    "    seed: 1\n",
    "\n",
    "strides_deeper_2:\n",
    "    <<: *DEEPER_DEFAULT\n",
    "    dataset_train: recreate/seed_2\n",
    "    seed: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_dir.startswith('gs://'):\n",
    "    remote_file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "    !gsutil -q cp $file_hyperparams $remote_file_hyperparams\n",
    "\n",
    "for run_name in training_run_names:\n",
    "    params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "\n",
    "    dataset_train = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_train'], 'dataset_training.pkl'), is_training=True)\n",
    "    dataset_eval = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_val'], 'dataset_validation.pkl'))\n",
    "\n",
    "    params['h'] = dataset_train.h\n",
    "    params['train_total_size'] = dataset_train.M\n",
    "    params['eval_total_size'] = dataset_eval.M\n",
    "    params['model_dir'] = os.path.join(base_dir, run_name)\n",
    "\n",
    "    train(params, get_resolver(), dataset_train, dataset_eval)\n",
    "\n",
    "    if base_dir.startswith('gs://'):\n",
    "        local_model_dir = os.path.join(data_dir, experiment, run_name)\n",
    "        if not os.path.exists(local_model_dir): os.makedirs(local_model_dir)\n",
    "\n",
    "        local_base_dir = os.path.join(data_dir, experiment)\n",
    "        remote_model_dir = params['model_dir']\n",
    "        \n",
    "        !gsutil -m cp -r $remote_model_dir $local_base_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'strides_test'\n",
    "kcalmol_per_hartree = 627.51\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "\n",
    "dataset_train = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_train'], 'dataset_training.pkl'), is_training=True)\n",
    "dataset_eval = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_val'], 'dataset_validation.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['train_total_size'] = dataset_train.M\n",
    "lr_schedule = tf.Session().run(learning_rate_schedule(params, tf.constant(np.arange(params['train_steps']))))\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(lr_schedule)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(np.log10(lr_schedule))\n",
    "plt.show()\n",
    "print('final learning rate: ', lr_schedule[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = os.path.join(data_dir, experiment, run_name, 'saved_model')\n",
    "latest_export = sorted(os.listdir(export_dir))[-1]\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(os.path.join(export_dir, latest_export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_analysis(kenergies, derivatives, dataset):\n",
    "    \n",
    "    kenergies_err = np.abs(dataset.kenergies - kenergies)*kcalmol_per_hartree\n",
    "    derivative_err = np.max(np.abs(dataset.derivatives - derivatives), axis=1)\n",
    "\n",
    "    print('MAE:', np.mean(kenergies_err))\n",
    "    print('AE_std:', np.std(kenergies_err))\n",
    "    print('AE_max:', np.max(kenergies_err))\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.hist(kenergies_err, bins=100)\n",
    "    plt.title('Kinetic Energy Absolute Error')\n",
    "    plt.xlabel('kcal/mol')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.hist(derivative_err, bins=100)\n",
    "    plt.title('Functional Derivative Absolute Error')\n",
    "    plt.xlabel('hartree')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "output = predict_fn({'features': dataset_eval.densities})\n",
    "kenergies = output['value']\n",
    "derivatives = output['derivative']\n",
    "plot_prediction_analysis(kenergies, derivatives, dataset_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Time Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_flip = predict_fn({'features': np.flip(dataset_eval.densities, axis=1)})\n",
    "kenergies_flip = output_flip['value']\n",
    "derivatives_flip = output_flip['derivative']\n",
    "\n",
    "kenergies_augm = (kenergies + kenergies_flip)/2\n",
    "derivatives_augm = (derivatives + np.flip(derivatives_flip, axis=1))/2\n",
    "\n",
    "plot_prediction_analysis(kenergies_augm, derivatives_augm, dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = InputPipeline(N=params['N'], dataset_file=os.path.join(data_dir, 'recreate_paper', 'dataset_sample.pkl'))\n",
    "\n",
    "output = predict_fn({'features': dataset_sample.densities})\n",
    "kinetic_energy_sample = output['value']\n",
    "derivative_sample = output['derivative']\n",
    "\n",
    "print('Energy prediction:')\n",
    "print(dataset_sample.kenergies[0], kinetic_energy_sample[0], \" =\", np.abs(dataset_sample.kenergies[0]-kinetic_energy_sample[0])*kcalmol_per_hartree, \"kcal/mol\")\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(dataset_sample.x, dataset_sample.derivatives[0], 'C0', linestyle='--')\n",
    "plt.plot(dataset_sample.x, derivative_sample[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_conv_nn_model_fn_debug(features, labels, mode, params):\n",
    "    \n",
    "    if isinstance(features, dict):\n",
    "        features = features['feature']\n",
    "\n",
    "    target_prediction, layers = conv_nn(features, **params['kwargs'], return_layers=True)\n",
    "    derivative_prediction = 1/params['h']*tf.gradients(target_prediction, features)[0]\n",
    "\n",
    "    predictions = {\n",
    "        'value': target_prediction,\n",
    "        'derivative': derivative_prediction\n",
    "    }\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        predictions['layer' + str(i)] = layer\n",
    "        \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        export_outputs={\n",
    "        'regression': tf.estimator.export.PredictOutput(predictions)\n",
    "        })\n",
    "\n",
    "params['h'] = dataset_train.h\n",
    "params['train_total_size'] = dataset_train.M\n",
    "params['eval_total_size'] = dataset_eval.M\n",
    "params['model_dir'] = os.path.join(data_dir, experiment, run_name)\n",
    "params['batch_size'] = 1\n",
    "\n",
    "model = tf.estimator.Estimator(\n",
    "    model_fn=deriv_conv_nn_model_fn_debug,\n",
    "    model_dir=params['model_dir'],\n",
    "    params=params)\n",
    "\n",
    "weights = {name:model.get_variable_value(name) for name in model.get_variable_names()}\n",
    "results = next(model.predict(input_fn=dataset_sample.input_fn))\n",
    "layers  = [results[key] for key in results.keys() if 'layer' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetic_energy_sample = output['value']\n",
    "derivative_sample = output['derivative']\n",
    "\n",
    "print('Energy prediction:')\n",
    "print(dataset_sample.kenergies[0], kinetic_energy_sample[0], \" =\", np.abs(dataset_sample.kenergies[0]-kinetic_energy_sample[0])*kcalmol_per_hartree, \"kcal/mol\")\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(dataset_sample.x, dataset_sample.derivatives[0], 'C0', linestyle='--')\n",
    "plt.plot(dataset_sample.x, derivative_sample[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1\n",
    "for kernel, value in weights.items():\n",
    "    if 'kernel' not in kernel or 'Adam' in kernel:\n",
    "        continue\n",
    "    \n",
    "    #fig, axs = plt.subplots(value.shape[3], 1, figsize=(6, 2 + value.shape[3]), dpi=200)\n",
    "    fig, axs = plt.subplots(value.shape[3], 1, figsize=(10, 3 + value.shape[3]))\n",
    "    if value.shape[3] == 1:\n",
    "        axs = [axs]\n",
    "        \n",
    "    limit = np.max(np.abs(value))*1.1\n",
    "    \n",
    "    for i, kern in enumerate(np.moveaxis(value, 3, 0)):\n",
    "        axs[i].plot(np.arange(len(kern[:, 0]))+1, kern[:, 0], '.-', markersize=2, linewidth=0.5)\n",
    "        axs[i].plot([1, len(kern[:, 0])], [0, 0], 'k', linewidth=0.5, zorder=-3)\n",
    "        axs[i].text(6, limit*0.6, 'bias: ' + str(weights[kernel.replace('kernel', 'bias')][i]))\n",
    "        axs[i].set_ylabel('ch{} out '.format('.' if value.shape[3] > 6 else 'annel') + str(i+1))\n",
    "        axs[i].set_ylim([-limit, limit])\n",
    "        axs[i].set_xlim([1, len(kern[:, 0])])\n",
    "        \n",
    "        if i == value.shape[3]-1: \n",
    "            axs[i].set_xlabel('index')\n",
    "            axs[i].legend(['channel in ' + str(j+1) for j in range(kern.shape[2])], \n",
    "                          bbox_to_anchor=[1.0, 0.0], loc='lower right')\n",
    "        else:\n",
    "            axs[i].get_xaxis().set_visible(False)\n",
    "        \n",
    "        if i == 0:\n",
    "            axs[i].set_title('layer {}: kernel_size: {}, filters: {}, activation: {}, padding: {}'.format(\n",
    "            l, value.shape[0], value.shape[3], 'softplus' if l != 3 else 'sum', 'valid'), fontsize=10)\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 500\n",
    "\n",
    "\n",
    "#fig, axs = plt.subplots(value.shape[3], 1, figsize=(6, 2 + value.shape[3]), dpi=200)\n",
    "fig, axs = plt.subplots(len(layers)-1, 1, figsize=(10, (len(layers)-1)*2), dpi=200)\n",
    "\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "        \n",
    "    length = layer.shape[0]\n",
    "    cutoff = (G - length)//2\n",
    "    \n",
    "    limit = np.max(np.abs(layer))*1.1\n",
    "\n",
    "    axs[i].plot(np.arange(G-cutoff*2)+1, layer)#, '.-', markersize=2, linewidth=0.5)\n",
    "    axs[i].set_ylim(0, limit)\n",
    "    axs[i].set_xlim([-cutoff+1, G-cutoff])\n",
    "    axs[i].grid(which='major', axis='y', linestyle='--')\n",
    "    axs[i].set_xticks([1, length])\n",
    "    \n",
    "    if i == len(layers)-2:\n",
    "        axs[i].plot([1, G-cutoff*2+1], np.ones(shape=(2,))*layers[-1]/length, 'k')\n",
    "        axs[i].text(G-cutoff*2+8, layers[-1]/length, 'sum: ' + str(layers[-1]))\n",
    "        axs[i].set_xlabel('index')\n",
    "    axs[i].set_ylabel('layer ' + str(i) + ' output' if i > 0 else 'input')\n",
    "    axs[i].legend(['channel ' + str(j+1) if j < 4 else '...' for j in range(layer.shape[1])][:5], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_run_names = ['seed_0', 'seed_1', 'seed_2', 'seed_3', 'seed_4', 'seed_5', 'seed_6', 'seed_7', 'seed_8', 'seed_9']\n",
    "\n",
    "kcalmol_per_hartree = 627.51\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=ensemble_run_names[0], globals=globals())\n",
    "\n",
    "dataset_train = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_train'], 'dataset_training.pkl'), is_training=True)\n",
    "dataset_eval = InputPipeline(params['N'], os.path.join(data_dir, params['dataset_val'], 'dataset_validation.pkl'))\n",
    "\n",
    "predict_fns = []\n",
    "\n",
    "for run_name in ensemble_run_names:\n",
    "    export_dir = os.path.join(data_dir, experiment, run_name, 'saved_model')\n",
    "    latest_export = sorted(os.listdir(export_dir))[-1]\n",
    "    predict_fns.append(tf.contrib.predictor.from_saved_model(os.path.join(export_dir, latest_export)))\n",
    "\n",
    "def ensemble_predict_fn(features_dict):\n",
    "    kenergies_n = []\n",
    "    derivatives_n = []\n",
    "\n",
    "    for predict_fn in predict_fns:\n",
    "        output = predict_fn(features_dict)\n",
    "        kenergies_n.append(output['value'])\n",
    "        derivatives_n.append(output['derivative'])\n",
    "\n",
    "    return {'value': np.mean(kenergies_n, axis=0), 'derivative': np.mean(derivatives_n, axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ensemble_predict_fn({'features': dataset_eval.densities})\n",
    "kenergies_ens = output['value']\n",
    "derivatives_ens = output['derivative']\n",
    "\n",
    "plot_prediction_analysis(kenergies_ens, derivatives_ens, dataset_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Time Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ensemble_predict_fn({'features': np.flip(dataset_eval.densities, axis=1)})\n",
    "kenergies_ens_flip = output['value']\n",
    "derivatives_ens_flip = output['derivative']\n",
    "\n",
    "kenergies_ens_augm = (kenergies_ens + kenergies_ens_flip)/2\n",
    "derivatives_ens_augm = (derivatives_ens + np.flip(derivatives_ens_flip, axis=1))/2\n",
    "\n",
    "plot_prediction_analysis(kenergies_ens_augm, derivatives_ens_augm, dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = InputPipeline(N=params['N'], dataset_file=os.path.join(data_dir, 'recreate_paper', 'dataset_sample.pkl'))\n",
    "\n",
    "output = ensemble_predict_fn({'features': dataset_sample.densities})\n",
    "kinetic_energy_sample = output['value']\n",
    "derivative_sample = output['derivative']\n",
    "\n",
    "print('Energy prediction:')\n",
    "print(dataset_sample.kenergies[0], kinetic_energy_sample[0], \" =\", np.abs(dataset_sample.kenergies[0]-kinetic_energy_sample[0])*kcalmol_per_hartree, \"kcal/mol\")\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(dataset_sample.x, dataset_sample.derivatives[0], 'C0', linestyle='--')\n",
    "plt.plot(dataset_sample.x, derivative_sample[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_summaries(event_dir):\n",
    "    import pandas as pd\n",
    "\n",
    "    event_files = [file for file in os.listdir(event_dir) if '.tfevents' in file]\n",
    "\n",
    "    values = {'wall_time': ([], [])}\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for summary in tf.train.summary_iterator(os.path.join(event_dir, event_file)):\n",
    "            if summary.summary.value.__len__() == 0: continue      \n",
    "\n",
    "            if summary.step not in values['wall_time'][0]:\n",
    "                values['wall_time'][0].append(summary.step)\n",
    "                values['wall_time'][1].append(summary.wall_time)\n",
    "\n",
    "            for entry in summary.summary.value:\n",
    "                try:\n",
    "                    if summary.step not in values[entry.tag][0]:\n",
    "                        values[entry.tag][0].append(summary.step)\n",
    "                        values[entry.tag][1].append(entry.simple_value)\n",
    "\n",
    "                except KeyError:\n",
    "                    values[entry.tag] = ([summary.step], [entry.simple_value])\n",
    "                    \n",
    "    for key in values.keys():        \n",
    "        values[key] = pd.DataFrame(values[key][1], index=values[key][0], columns=[key])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_train = load_summaries(os.path.join(data_dir, experiment, run_name))\n",
    "summary_eval = load_summaries(os.path.join(data_dir, experiment, run_name, 'eval'))\n",
    "print(summary_train.keys(), summary_eval.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2), dpi=200)\n",
    "plt.plot(np.log10(summary_train['loss']))\n",
    "plt.plot(np.log10(summary_train['loss'].ewm(com=10).mean()))\n",
    "plt.plot(np.log10(summary_eval['loss']))\n",
    "plt.show()\n",
    "\n",
    "print(summary_train['global_step/sec'].mean()[0], 'global_step/sec')\n",
    "print(summary_train['examples/sec'].mean()[0], 'examples/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_train_ens = [load_summaries(os.path.join(data_dir, experiment, run_name)) for run_name in ensemble_run_names]\n",
    "summary_eval_ens = [load_summaries(os.path.join(data_dir, experiment, run_name, 'eval')) for run_name in ensemble_run_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=200)\n",
    "for i, summary_e in enumerate(summary_train_ens): \n",
    "    plt.plot(summary_e['loss'].ewm(com=20).mean(), 'C' + str(i))\n",
    "for i, summary_e in enumerate(summary_eval_ens): \n",
    "    plt.plot(summary_e['loss'], '--C' + str(i))\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim([10**(-4.5), 10**-2])\n",
    "plt.xlim([0, 1000000])\n",
    "plt.legend([name.replace('_', ': ') for name in ensemble_run_names] + [name.replace('_', ': ') + ', eval' for name in ensemble_run_names], ncol=2)\n",
    "plt.grid()\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "for i, summary_e in enumerate(summary_eval_ens): \n",
    "    plt.plot(summary_e['value_mae']*kcalmol_per_hartree, '--C' + str(i))\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.ylim([0, 6])\n",
    "plt.xlim([0, 1000000])\n",
    "plt.legend([name.replace('_', ': ') + ', eval' for name in ensemble_run_names])\n",
    "plt.grid()\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('kinetic energy: mean absolute error / kcal/mol')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "for i, summary_e in enumerate(summary_eval_ens): \n",
    "    plt.plot(summary_e['derivative_mae'], '--C' + str(i))\n",
    "\n",
    "plt.xlim([0, 1000000])\n",
    "plt.legend([name.replace('_', ': ') + ', eval' for name in ensemble_run_names])\n",
    "plt.grid()\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('derivative: mean absolute error / hartree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_cnn_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
