{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create shared functions\n",
    "\n",
    "Please copy all the notebooks of this project to your Google Drive into a folder named \"notebooks\" inside a folder of your choice. Please also set the \"notebook_path\" variable in every notebook to this folder. This script will generate the folder structure for the data generated by the project.\n",
    "\n",
    "- base folder\n",
    "    - notebooks\n",
    "        - 0_define_helper_functions.ipynb = THIS NOTEBOOK\n",
    "        - 1a_generate_datasets.ipynb\n",
    "        - ...\n",
    "    - data (will be created)\n",
    "    - quantumflow (will be created / overwritten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup notebook if it is run on Google Colab, cwd = notebook file location\n",
    "try:\n",
    "    # change notebook_path if this notebook is in a different subfolder of Google Drive\n",
    "    notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "if not os.path.exists('../quantumflow'):\n",
    "    os.makedirs('../quantumflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/generate_datasets.py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from quantumflow.utils import load_hyperparameters, integrate, laplace\n",
    "from quantumflow.numerov_solver import solve_schroedinger\n",
    "\n",
    "@tf.function\n",
    "def generate_potentials(return_x=False,\n",
    "                        return_h=False,\n",
    "                        dataset_size=100, \n",
    "                        discretisation_points=500, \n",
    "                        n_gauss=3, \n",
    "                        interval_length=1.0,\n",
    "                        a_minmax=(0.0, 3*10.0), \n",
    "                        b_minmax=(0.4, 0.6), \n",
    "                        c_minmax=(0.03, 0.1), \n",
    "                        n_method='sum',\n",
    "                        dtype='float64',\n",
    "                        **kwargs):\n",
    "    \n",
    "    if dtype == 'double' or dtype == 'float64':\n",
    "        dtype = tf.float64\n",
    "    elif dtype == 'float' or dtype == 'float32':\n",
    "        dtype = tf.float32\n",
    "    else:\n",
    "        raise ValueError('unknown dtype {}'.format(dtype))\n",
    "\n",
    "    x = tf.linspace(tf.constant(0.0, dtype=dtype), interval_length, discretisation_points, name=\"x\")\n",
    "\n",
    "    a = tf.random.uniform((dataset_size, 1, n_gauss), minval=a_minmax[0], maxval=a_minmax[1], dtype=dtype, name=\"a\")\n",
    "    b = tf.random.uniform((dataset_size, 1, n_gauss), minval=b_minmax[0]*interval_length, maxval=b_minmax[1]*interval_length, dtype=dtype, name=\"b\")\n",
    "    c = tf.random.uniform((dataset_size, 1, n_gauss), minval=c_minmax[0]*interval_length, maxval=c_minmax[1]*interval_length, dtype=dtype, name=\"c\")\n",
    "\n",
    "    curves = -tf.square(tf.expand_dims(tf.expand_dims(x, 0), 2) - b)/(2*tf.square(c))\n",
    "    curves = -a*tf.exp(curves)\n",
    "\n",
    "    if n_method == 'sum':\n",
    "        potentials = tf.reduce_sum(curves, -1, name=\"potentials\")\n",
    "    elif n_method == 'mean':\n",
    "        potentials = tf.reduce_mean(curves, -1, name=\"potentials\")\n",
    "    else:\n",
    "        raise NotImplementedError('Method {} is not implemented.'.format(n_method))\n",
    "\n",
    "    returns = [potentials]\n",
    "\n",
    "    if return_x:\n",
    "        returns += [x]\n",
    "    \n",
    "    if return_h:\n",
    "        h = tf.cast(interval_length/(discretisation_points-1), dtype=dtype) # discretisation interval\n",
    "        returns += [h]\n",
    "   \n",
    "    return returns\n",
    "\n",
    "def generate_datasets(data_dir, experiment, generate_names):\n",
    "    if not isinstance(generate_names, list):\n",
    "        generate_names = [generate_names]\n",
    "\n",
    "    base_dir = os.path.join(data_dir, experiment)\n",
    "    file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "\n",
    "    for run_name in generate_names:\n",
    "        params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(params['seed'])\n",
    "        potential, x, h = generate_potentials(return_x=True, return_h=True, **params)\n",
    "\n",
    "        params['h'] = h\n",
    "        energies, wavefunctions = solve_schroedinger(potential, params)\n",
    "\n",
    "        save_dataset(base_dir, params['filename'], params['format'], x.numpy(), h.numpy(), potential.numpy(), wavefunctions.numpy(), energies.numpy())\n",
    "        print(\"dataset\", params['filename'] + '.' + params['format'].replace('pickle', 'pkl'), \"saved to\", base_dir)\n",
    "\n",
    "def save_dataset(directory, filename, format, x, h, potential, wavefunctions, energies):\n",
    "        if format in ['pickle', 'pkl']:\n",
    "            import pickle\n",
    "            with open(os.path.join(directory, filename + '.pkl'), 'wb') as f:\n",
    "                pickle.dump({'x': x, 'h': h, 'potential': potential, 'wavefunctions': wavefunctions, 'energies': energies}, f)\n",
    "            \n",
    "        elif format in ['hdf5', 'h5']:\n",
    "            import h5py\n",
    "            with h5py.File(os.path.join(directory, filename + '.hdf5'), \"w\") as f:\n",
    "                f.attrs['x'] = x\n",
    "                f.attrs['h'] = h\n",
    "                f.create_dataset('potential', data=potential, compression=\"gzip\")\n",
    "                f.create_dataset('wavefunctions', data=wavefunctions, compression=\"gzip\")\n",
    "                f.create_dataset('energies', data=energies, compression=\"gzip\")\n",
    "        else:\n",
    "            raise KeyError('Unknown format {} to save dataset.'.format(params['format']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/numerov_solver.py\n",
    "import tensorflow as tf\n",
    "from quantumflow.utils import integrate\n",
    "\n",
    "# recurrent tensorflow cell for solving the numerov equation recursively\n",
    "class ShootingNumerovCell(tf.keras.layers.AbstractRNNCell):\n",
    "    def __init__(self, shape, h, **kwargs):\n",
    "        super(ShootingNumerovCell, self).__init__(**kwargs)\n",
    "        self._h2_scaled = 1 / 12 * h ** 2\n",
    "        self.shape = shape\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.shape + (4,)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.shape + (1,)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        k_m2, k_m1, y_m2, y_m1 = tf.unstack(states[0], axis=-1)\n",
    "        \n",
    "        y = (2 * (1 - 5 * self._h2_scaled * k_m1) * y_m1 - (1 + self._h2_scaled * k_m2) * y_m2) / (1 + self._h2_scaled * inputs)\n",
    "\n",
    "        new_state = tf.stack([k_m1, inputs, y_m1, y], axis=-1)\n",
    "        return y, new_state\n",
    "\n",
    "# tf function for using the shooting numerov method\n",
    "#\n",
    "# the numerov_init_slope is the slope of the solution at x=0\n",
    "# it can be constant>0 because it's actual value will be determined when the wavefunction is normalized\n",
    "#\n",
    "def shooting_numerov(k_squared, params):\n",
    "    h = params['h']\n",
    "    numerov_init_slope = params['numerov_init_slope']\n",
    "    init_values = tf.zeros_like(k_squared[:, 0])\n",
    "    one_step_values = numerov_init_slope * h * tf.ones_like(k_squared[:, 0])\n",
    "    init_state = tf.stack([k_squared[:, 0], k_squared[:, 1], init_values, one_step_values], axis=-1)\n",
    "    outputs = tf.keras.layers.RNN(ShootingNumerovCell(k_squared.shape[2:], h), return_sequences=True, dtype=params['dtype'])(k_squared[:, 2:], initial_state=init_state)\n",
    "    output = tf.concat([tf.expand_dims(init_values, axis=1), tf.expand_dims(one_step_values, axis=1), outputs], axis=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "# returns the rearranged schroedinger equation term in the numerov equation\n",
    "# k_squared = 2*m_e/h_bar**2*(E - V(x))\n",
    "def numerov_k_squared(potentials, energies):\n",
    "    return 2 * (tf.expand_dims(energies, axis=1) - tf.tile(tf.expand_dims(potentials, axis=2), [1, 1, energies.shape[1]]))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def find_split_energies(potentials, params):\n",
    "    M = potentials.shape[0]\n",
    "    N = params['n_orbitals']\n",
    "\n",
    "    # Knotensatz: number of roots = quantum state\n",
    "    # so target root = target excited state quantum number\n",
    "    target_roots = tf.tile(tf.expand_dims(tf.range(N + 1), axis=0), [M, 1])\n",
    "\n",
    "    # lowest value of potential as lower bound\n",
    "    E_split = tf.tile(tf.expand_dims(tf.reduce_min(potentials, axis=1), axis=1), [1, N + 1])\n",
    "\n",
    "    solutions_split = tf.zeros((potentials.shape[0], potentials.shape[1], N + 1), dtype=potentials.dtype)\n",
    "    not_converged = tf.ones(potentials.shape[0], dtype=tf.bool)\n",
    "    search_boost = tf.ones_like(E_split, dtype=tf.bool)\n",
    "    E_delta = tf.ones_like(E_split)\n",
    "\n",
    "    while tf.math.reduce_any(not_converged):\n",
    "        V_split = numerov_k_squared(tf.boolean_mask(potentials, not_converged), tf.boolean_mask(E_split, not_converged))\n",
    "\n",
    "        solutions_split_new = shooting_numerov(V_split, params)\n",
    "\n",
    "        partitioned_data = tf.dynamic_partition(solutions_split, tf.cast(not_converged, tf.int32) , 2)\n",
    "        condition_indices = tf.dynamic_partition(tf.range(tf.shape(solutions_split)[0]), tf.cast(not_converged, tf.int32) , 2)\n",
    "\n",
    "        solutions_split = tf.dynamic_stitch(condition_indices, [partitioned_data[0], solutions_split_new])\n",
    "        solutions_split.set_shape((potentials.shape[0], potentials.shape[1], N + 1))\n",
    "\n",
    "        roots_split = tf.reduce_sum(tf.cast(detect_roots(solutions_split), tf.int32), axis=1)\n",
    "\n",
    "        not_converged = tf.logical_and(tf.logical_not(tf.reduce_all(tf.equal(roots_split, target_roots), axis=1)), not_converged)\n",
    "\n",
    "        search_direction = tf.cast(roots_split < target_roots, potentials.dtype) - tf.cast(roots_split > target_roots, potentials.dtype)\n",
    "        boost = tf.logical_and(tf.equal(search_direction, tf.sign(E_delta)), search_boost)\n",
    "\n",
    "        E_delta += tf.cast(boost, potentials.dtype)*E_delta\n",
    "        stop_boost = search_direction * tf.sign(E_delta) < 0\n",
    "        search_boost &= tf.logical_not(stop_boost)\n",
    "        E_delta += -1.5*E_delta*tf.cast(stop_boost, potentials.dtype)\n",
    "\n",
    "        E_split += E_delta*tf.expand_dims(tf.cast(not_converged, potentials.dtype), axis=-1)\n",
    "\n",
    "    return E_split\n",
    "\n",
    "\n",
    "def detect_roots(array):\n",
    "    return tf.logical_or(tf.equal(array[:, 1:], 0), array[:, 1:] * array[:, :-1] < 0)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def solve_numerov(potentials, target_roots, split_energies, params):\n",
    "    E_low = split_energies[:, :-1]\n",
    "    E_high = split_energies[:, 1:]\n",
    "\n",
    "    # because the search interval is halved at every step\n",
    "    # 32 iterations will always converge to the best numerically possible accuracy of E\n",
    "    # (empirically ~25 steps)\n",
    "\n",
    "    E = 0.5 * (E_low + E_high)\n",
    "    E_last = E * 2\n",
    "    \n",
    "    while tf.reduce_any(tf.logical_not(tf.equal(E_last, E))):\n",
    "        V = numerov_k_squared(potentials, E)\n",
    "\n",
    "        solutions = shooting_numerov(V, params)\n",
    "        roots = tf.reduce_sum(tf.cast(detect_roots(solutions), tf.int32), axis=1)\n",
    "\n",
    "        update_low = roots <= target_roots\n",
    "        update_high = tf.logical_not(update_low)\n",
    "\n",
    "        E_low = tf.where(update_low, E, E_low)\n",
    "        E_high = tf.where(update_high, E, E_high)\n",
    "\n",
    "        E_last = E\n",
    "        E = 0.5 * (E_low + E_high)\n",
    "\n",
    "    solutions_low = shooting_numerov(numerov_k_squared(potentials, E_low), params)\n",
    "    roots_low = tf.cast(detect_roots(solutions_low), tf.double)\n",
    "\n",
    "    solutions_high = shooting_numerov(numerov_k_squared(potentials, E_high), params)\n",
    "    roots_high = tf.cast(detect_roots(solutions_high), tf.double)\n",
    "\n",
    "    roots_diff = tf.abs(roots_high - roots_low)  \n",
    "\n",
    "    roots_cumsum = tf.cumsum(tf.pad(roots_diff, ((0, 0), (1, 0), (0, 0)), 'constant'), axis=1)\n",
    "\n",
    "    invalid = tf.equal(roots_cumsum, tf.expand_dims(roots_cumsum[:, -1], axis=1))\n",
    "\n",
    "    return solutions_low, E, invalid\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def solve_schroedinger(potentials, params):\n",
    "    M = potentials.shape[0]\n",
    "    G = potentials.shape[1]\n",
    "    N = params['n_orbitals']\n",
    "    \n",
    "    E_split = find_split_energies(potentials, params)\n",
    "\n",
    "    target_roots = tf.tile(tf.expand_dims(tf.range(N), axis=0), [M, 1])\n",
    "    solutions_forward, E_forward, invalid_forward = solve_numerov(potentials, target_roots, E_split, params)\n",
    "    #solutions_forward /= tf.expand_dims(tf.reduce_max(tf.abs(solutions_forward)*tf.cast(tf.logical_not(invalid_forward), tf.double), axis=1), axis=1)\n",
    "\n",
    "    solutions_backward, E_backward, invalid_backward = solve_numerov(tf.reverse(potentials, axis=[1]), target_roots, E_split, params)\n",
    "    solutions_backward = tf.reverse(solutions_backward, axis=[1])\n",
    "    invalid_backward = tf.reverse(invalid_backward, axis=[1])\n",
    "    #solutions_backward /= tf.expand_dims(tf.reduce_max(tf.abs(solutions_backward)*tf.cast(tf.logical_not(invalid_backward), tf.double), axis=1), axis=1)\n",
    "\n",
    "    n_invalid_forward = tf.reduce_sum(tf.cast(invalid_forward, tf.int32), axis=1)\n",
    "    n_invalid_backward = tf.reduce_sum(tf.cast(invalid_backward, tf.int32), axis=1)\n",
    "    merge_index = (G - n_invalid_forward - n_invalid_backward)//2 + n_invalid_forward\n",
    "\n",
    "    merge_value_forward = tf.reduce_sum(tf.gather(tf.transpose(solutions_forward, perm=[0, 2, 1]), tf.expand_dims(merge_index, axis=2), batch_dims=2), axis=2)\n",
    "    merge_value_backward = tf.reduce_sum(tf.gather(tf.transpose(solutions_backward, perm=[0, 2, 1]), tf.expand_dims(merge_index, axis=2), batch_dims=2), axis=2)\n",
    "\n",
    "    factor = merge_value_forward/merge_value_backward\n",
    "    solutions_backward *= tf.expand_dims(factor, axis=1)\n",
    "\n",
    "    join_mask = tf.expand_dims(tf.expand_dims(tf.range(G), axis=0), axis=2) < tf.expand_dims(merge_index, axis=1)\n",
    "\n",
    "    solutions = tf.where(join_mask, solutions_forward, solutions_backward)\n",
    "\n",
    "    #normalization\n",
    "    density = solutions ** 2\n",
    "    norm = integrate(density, params['h'])\n",
    "    solutions *= 1 / tf.sqrt(tf.expand_dims(norm, axis=1))\n",
    "\n",
    "    E = 0.5*(E_forward + E_backward)\n",
    "    \n",
    "    return E, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/utils.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def integrate(y, h):\n",
    "    return h*tf.reduce_sum((y[:, :-1] + y[:, 1:])/2., axis=1, name='trapezoidal_integral_approx')\n",
    "\n",
    "def laplace(data, h):  # time_axis=1\n",
    "    temp_laplace = 1 / h ** 2 * (data[:, :-2, :] + data[:, 2:, :] - 2 * data[:, 1:-1, :])\n",
    "    return tf.pad(temp_laplace, ((0, 0), (1, 1), (0, 0)), 'constant')\n",
    "\n",
    "\n",
    "def derivative_five_point(density, h):\n",
    "    return tf.concat([1/(2*h)*(density[:, 2:3] - density[:, 0:1]), \n",
    "                      1/(12*h)*(-density[:, 4:] + 8*density[:, 3:-1] - 8*density[:, 1:-3] + density[:, 0:-4]),\n",
    "                      1/(2*h)*(density[:, -1:] - density[:, -3:-2])], axis=1)\n",
    "\n",
    "def laplace_five_point(density, h):\n",
    "    return 1/(12*h**2)*(-density[:, 4:] + 16*density[:, 3:-1] - 30*density[:, 2:-2] + 16*density[:, 1:-3] - density[:, 0:-4])\n",
    "\n",
    "\n",
    "def weizsaecker_functional(density, h):\n",
    "    derivative_density = derivative_five_point(density, h)\n",
    "    inverse_density = 1/density[:, 1:-1]\n",
    "\n",
    "    weizsaecker_kinetic_energy_density = wked = 1/8*derivative_density**2*inverse_density\n",
    "    weizsaecker_kinetic_energy_density = tf.concat([2*wked[:, 0:1] - wked[:, 1:2], wked, 2*wked[:, -1:] - wked[:, -2:-1]], axis=1)\n",
    "\n",
    "    return integrate(weizsaecker_kinetic_energy_density, h)\n",
    "\n",
    "def weizsaecker_functional_derivative(density, h):\n",
    "    derivative_density = derivative_five_point(density, h)[:, 1:-1]\n",
    "    laplace_density = laplace_five_point(density, h)\n",
    "    inverse_density = 1/density[:, 2:-2]\n",
    "\n",
    "    weizsaecker_kinetic_energy_functional_derivative = wkefd = 1/8*(derivative_density*inverse_density)**2 - 1/4*laplace_density*inverse_density\n",
    "    weizsaecker_kinetic_energy_functional_derivative = tf.concat([3*wkefd[:, 0:1] - 2*wkefd[:, 1:2], 2*wkefd[:, 0:1] - wkefd[:, 1:2], wkefd, 2*wkefd[:, -1:] - wkefd[:, -2:-1], 3*wkefd[:, -1:] - 2*wkefd[:, -2:-1]], axis=1)\n",
    "\n",
    "    return weizsaecker_kinetic_energy_functional_derivative\n",
    "\n",
    "\n",
    "def load_hyperparameters(file_hyperparams, run_name='default', globals=None):\n",
    "    from ruamel.yaml import YAML\n",
    "\n",
    "    if globals is not None:\n",
    "        with open(file_hyperparams) as f:\n",
    "            globals_list = YAML().load(f)['globals']\n",
    "\n",
    "    with open(file_hyperparams) as f:\n",
    "        hparams = YAML().load(f)[run_name]\n",
    "\n",
    "    if globals is None:\n",
    "        return hparams\n",
    "\n",
    "    dicts = [hparams]\n",
    "    while len(dicts) > 0:\n",
    "        data = dicts[0]\n",
    "        for idx, obj in enumerate(data):\n",
    "            if isinstance(data[obj], dict):\n",
    "                dicts.append(data[obj])\n",
    "                continue\n",
    "\n",
    "            if data[obj] in globals_list:\n",
    "                data[obj] = globals[data[obj]]\n",
    "        del dicts[0]\n",
    "    return hparams\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "def anim_plot(array, x=None, interval=100, bar=\"\", figsize=(15, 3), **kwargs):\n",
    "    frames = len(array)\n",
    "    \n",
    "    if not bar == \"\":\n",
    "        import ipywidgets as widgets\n",
    "        widget = widgets.IntProgress(min=0, max=frames, description=bar, bar_style='success',\n",
    "                                     layout=widgets.Layout(width='92%'))\n",
    "        display(widget)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if x is None:\n",
    "        plt_h = ax.plot(array[0], **kwargs)\n",
    "    else:\n",
    "        plt_h = ax.plot(x, array[0], **kwargs) \n",
    "        \n",
    "    min_last = np.min(array[-1])\n",
    "    max_last = np.max(array[-1])\n",
    "    span_last = max_last - min_last\n",
    "        \n",
    "    ax.set_ylim([min_last - span_last*0.2, max_last + span_last*0.2])\n",
    "\n",
    "    def init():\n",
    "        return plt_h\n",
    "\n",
    "    def animate(f):\n",
    "        if not bar == \"\":\n",
    "            widget.value = f\n",
    "\n",
    "        for i, h in enumerate(plt_h):\n",
    "            if x is None:\n",
    "                h.set_data(np.arange(len(array[f][:, i])), array[f][:, i], **kwargs)\n",
    "            else:\n",
    "                h.set_data(x, array[f][:, i], **kwargs)\n",
    "        return plt_h\n",
    "\n",
    "    # call the animator. blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=frames, interval=interval,\n",
    "                                   blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "    rc('animation', html='html5')\n",
    "    display(HTML(anim.to_html5_video(embed_limit=1024)))\n",
    "\n",
    "    if not bar == \"\":\n",
    "        widget.close()\n",
    "\n",
    "\n",
    "def calculate_density_and_energies(potential, wavefunctions, energies, N, h):\n",
    "    assert(N <= wavefunctions.shape[2])\n",
    "    density = np.sum(np.square(wavefunctions)[:, :, :N], axis=-1)\n",
    "\n",
    "    lpwf = 1/(12*h**2)*(-wavefunctions[:, 4:] + 16*wavefunctions[:, 3:-1] - 30*wavefunctions[:, 2:-2] + 16*wavefunctions[:, 1:-3] - wavefunctions[:, 0:-4])\n",
    "    laplace_wavefunctions = tf.concat([3*lpwf[:, 0:1] - 2*lpwf[:, 1:2], 2*lpwf[:, 0:1] - lpwf[:, 1:2], lpwf, 2*lpwf[:, -1:] - lpwf[:, -2:-1], 3*lpwf[:, -1:] - 2*lpwf[:, -2:-1]], axis=1) \n",
    "\n",
    "    kinetic_energy_densities = -0.5*wavefunctions*laplace_wavefunctions\n",
    "    potential_energy_densities = np.expand_dims(potential, axis=2)*wavefunctions**2\n",
    "    \n",
    "    potential_energies = h * (np.sum(potential_energy_densities, axis=1) - 0.5 * (np.take(potential_energy_densities, 0, axis=1) + np.take(potential_energy_densities, -1, axis=1)))\n",
    "    kinetic_energies = h * (np.sum(kinetic_energy_densities, axis=1) - 0.5 * (np.take(kinetic_energy_densities, 0, axis=1) + np.take(kinetic_energy_densities, -1, axis=1)))\n",
    "\n",
    "    energy = np.sum(energies[:, :N], axis=-1)\n",
    "    potential_energy = np.sum(potential_energies[:, :N], axis=-1)\n",
    "    kinetic_energy = np.sum(kinetic_energies[:, :N], axis=-1)\n",
    "    \n",
    "    kinetic_energy_density = np.sum(kinetic_energy_densities[:, :, :N], axis=-1)\n",
    "    potential_energy_density = np.sum(potential_energy_densities[:, :, :N], axis=-1)\n",
    "\n",
    "    return density, energy, potential_energy, kinetic_energy, potential_energy_density, kinetic_energy_density\n",
    "\n",
    "\n",
    "def calculate_system_properties(potential, wavefunctions, energies, N, h):\n",
    "    assert(N <= wavefunctions.shape[2])\n",
    "\n",
    "    density, energy, potential_energy, kinetic_energy, potential_energy_density, kinetic_energy_density = calculate_density_and_energies(potential, wavefunctions, energies, N, h)\n",
    "    derivative = -potential\n",
    "\n",
    "    vW_kinetic_energy = weizsaecker_functional(density, h).numpy()\n",
    "    vW_derivative = weizsaecker_functional_derivative(density, h).numpy()\n",
    "\n",
    "    return density, energy, potential_energy, kinetic_energy, potential_energy_density, kinetic_energy_density, derivative, vW_kinetic_energy, vW_derivative\n",
    "\n",
    "\n",
    "class QFDataset():\n",
    "    def __init__(self, dataset_file, params):\n",
    "        extension = dataset_file.split('.')[-1]\n",
    "        \n",
    "        if extension in ['pkl', 'pickle']:\n",
    "            import pickle\n",
    "            with open(dataset_file, 'rb') as f:\n",
    "                x, h, potential, wavefunctions, energies = pickle.load(f).values()\n",
    "\n",
    "        elif extension in ['hdf5', 'h5']:\n",
    "            import h5py\n",
    "            with h5py.File(dataset_file, 'r') as f:\n",
    "                x = f.attrs['x']\n",
    "                h = f.attrs['h']\n",
    "                potential = f['potential'][()]\n",
    "                wavefunctions = f['wavefunctions'][()]\n",
    "                energies = f['energies'][()]\n",
    "        else:\n",
    "            raise NotImplementedError('File extension missing or not supported.')  \n",
    "\n",
    "        if params['N'] == 'all':\n",
    "            all_data = [calculate_system_properties(potential, wavefunctions, energies, N, h) for N in range(1, energies.shape[1]+1)]\n",
    "            density, energy, potential_energy, kinetic_energy, potential_energy_density, kinetic_energy_density, derivative, vW_kinetic_energy, vW_derivative = \\\n",
    "                [np.concatenate([all_data[i][j] for i in range(len(all_data))], axis=0) for j in range(len(all_data[0]))]\n",
    "        else:\n",
    "            density, energy, potential_energy, kinetic_energy, potential_energy_density, kinetic_energy_density, derivative, vW_kinetic_energy, vW_derivative = \\\n",
    "                calculate_system_properties(potential, wavefunctions, energies, params['N'], h)\n",
    "            \n",
    "        self.dataset_size, self.discretisation_points = density.shape\n",
    "\n",
    "        if params.get('subtract_von_weizsaecker', False):\n",
    "            kinetic_energy -= params.get('von_weizsaecker_factor', 1.0)*vW_kinetic_energy\n",
    "            derivative -= params.get('von_weizsaecker_factor', 1.0)*vW_derivative\n",
    "            \n",
    "        if params['dtype'] == 'double' or params['dtype'] == 'float64':\n",
    "            if potential.dtype == np.float32:\n",
    "                raise ImportError(\"requested dtype={}, but dataset is saved with dtype={}, which is less precise.\".format(params['dtype'], potential.dtype))\n",
    "            self.dtype = np.float64\n",
    "        elif params['dtype'] == 'float' or params['dtype'] == 'float32':\n",
    "            self.dtype = np.float32\n",
    "        else:\n",
    "            raise ValueError('unknown dtype {}'.format(params['dtype']))\n",
    "\n",
    "        self.x = x.astype(self.dtype)\n",
    "        self.h = h.astype(self.dtype)\n",
    "        self.potential = potential.astype(self.dtype)\n",
    "        self.density = density.astype(self.dtype)\n",
    "        self.energy = energy.astype(self.dtype)\n",
    "        self.potential_energy = potential_energy.astype(self.dtype)\n",
    "        self.kinetic_energy = kinetic_energy.astype(self.dtype)\n",
    "        self.potential_energy_density = potential_energy_density.astype(self.dtype)\n",
    "        self.kinetic_energy_density = kinetic_energy_density.astype(self.dtype)\n",
    "        self.derivative = derivative.astype(self.dtype)\n",
    "        self.vW_kinetic_energy = vW_kinetic_energy.astype(self.dtype)\n",
    "        self.vW_derivative = vW_derivative.astype(self.dtype)\n",
    "\n",
    "        if not 'features' in params or not 'targets' in params: \n",
    "            return\n",
    "\n",
    "        self.features = {}\n",
    "        self.targets = {}\n",
    "\n",
    "        def add_by_name(dictionary, name):\n",
    "            if name == 'density':\n",
    "                dictionary['density'] = self.density\n",
    "            elif name == 'derivative':\n",
    "                dictionary['derivative'] = self.derivative\n",
    "            elif name == 'potential':\n",
    "                dictionary['potential'] = self.potential\n",
    "            elif name == 'kinetic_energy':\n",
    "                dictionary['kinetic_energy'] = self.kinetic_energy\n",
    "            elif name == 'kinetic_energy_density':\n",
    "                dictionary['kinetic_energy_density'] = self.kinetic_energy_density\n",
    "            else:\n",
    "                raise KeyError('feature/target {} does not exist or is not implemented.'.format(name))\n",
    "\n",
    "        for feature in params['features']:\n",
    "            add_by_name(self.features, feature)\n",
    "\n",
    "        for target in params['targets']:\n",
    "            add_by_name(self.targets, target)\n",
    "\n",
    "    def get_params(self, shapes=True, h=True, mean=False):\n",
    "        import numpy as np\n",
    "\n",
    "        params = {}\n",
    "        if h:\n",
    "            params['h'] = self.h\n",
    "\n",
    "        if shapes:\n",
    "            params['features_shape'] = {name:feature.shape[1:] for name, feature in self.features.items()}\n",
    "            params['targets_shape'] = {name:target.shape[1:] for name, target in self.targets.items()}\n",
    "\n",
    "        if mean:\n",
    "            params['features_mean'] = {name:np.mean(feature, axis=0) for name, feature in self.features.items()}\n",
    "            params['targets_mean'] = {name:np.mean(target, axis=0) for name, target in self.targets.items()}\n",
    "\n",
    "        return params\n",
    "\n",
    "def run_experiment(experiment, run_name, data_dir='../data'): \n",
    "    base_dir = os.path.join(data_dir, experiment)\n",
    "    model_dir = os.path.join(base_dir, run_name)\n",
    "\n",
    "    file_model = os.path.join(base_dir, \"model.py\")\n",
    "    exec(open(file_model).read(), globals())\n",
    "\n",
    "    file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "    params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "\n",
    "    train(params, model_dir, data_dir)\n",
    "\n",
    "def run_multiple(experiment, run_name, data_dir='../data'): \n",
    "    import copy\n",
    "\n",
    "    base_dir = os.path.join(data_dir, experiment)\n",
    "    model_dir = os.path.join(base_dir, run_name)\n",
    "\n",
    "    file_model = os.path.join(base_dir, \"model.py\")\n",
    "    exec(open(file_model).read(), globals())\n",
    "\n",
    "    file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "    params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "\n",
    "    def apply_configuration(hparams, configuration):\n",
    "        dicts = [hparams]\n",
    "        while len(dicts) > 0:\n",
    "            data = dicts[0]\n",
    "            for idx, obj in enumerate(data):\n",
    "                if obj in ['int_min', 'int_max']:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(data[obj], dict):\n",
    "                    dicts.append(data[obj])\n",
    "                    continue\n",
    "\n",
    "                if obj in configuration.keys():\n",
    "                    data[obj] = configuration[obj]\n",
    "            del dicts[0]\n",
    "        return hparams\n",
    "\n",
    "    def extend_configurations(configurations_out, run_appendices_out, configurations_in, run_appendices_in):\n",
    "        if len(configurations_out) == 0:\n",
    "            return configurations_in, run_appendices_in\n",
    "\n",
    "        merged_configurations = []\n",
    "        merged_run_appendices = []\n",
    "        for configuration_out, run_appendix_out in zip(configurations_out, run_appendices_out):\n",
    "            for configuration_in, run_appendix_in in zip(configurations_in, run_appendices_in):\n",
    "                merged_configurations.append(configuration_out.copy().update(configuration_in))\n",
    "                merged_run_appendices.append(run_appendix_out + '_' + run_appendix_in)\n",
    "\n",
    "        return merged_configurations, merged_run_appendices\n",
    "\n",
    "    configurations = []\n",
    "    run_appendices = []\n",
    "\n",
    "    if 'int_min' in params and 'int_max' in params:\n",
    "        for (int_key, int_min), (int_key_max, int_max) in zip(params['int_min'].items(), params['int_max'].items()):\n",
    "            assert int_key == int_key_max \n",
    "            int_configurations = []\n",
    "            int_run_appendices = []\n",
    "            for int_value in range(int_min, int_max+1):\n",
    "                int_configurations.append({int_key: int_value})\n",
    "                int_run_appendices.append(('{}{:0' + str(len(str(int_max))) + 'd}').format(int_key, int_value)) # TODO: support negative values\n",
    "            configurations, run_appendices = extend_configurations(configurations, run_appendices, int_configurations, int_run_appendices)\n",
    "    \n",
    "    for configuration, run_appendix in zip(configurations, run_appendices):\n",
    "        train(apply_configuration(copy.deepcopy(params), configuration), os.path.join(model_dir, run_appendix), data_dir)\n",
    "\n",
    "\n",
    "def build_model(params, data_dir='../data', dataset_train=None):\n",
    "    if dataset_train is None:\n",
    "        dataset_train = QFDataset(os.path.join(data_dir, params['dataset_train']), params)\n",
    "        params['dataset'] = dataset_train.get_params(shapes=True, h=True, mean=True)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    return params['model'](params)\n",
    "\n",
    "def train(params, model_dir=None, data_dir='../data', callbacks=None):\n",
    "    dataset_train = QFDataset(os.path.join(data_dir, params['dataset_train']), params)\n",
    "    dataset_validate = QFDataset(os.path.join(data_dir, params['dataset_validate']), params) if 'dataset_validate' in params else None\n",
    "    params['dataset'] = dataset_train.get_params(shapes=True, h=True, mean=True)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    if 'seed' in params:\n",
    "        tf.random.set_seed(params['seed'])\n",
    "        \n",
    "    model = build_model(params, data_dir=data_dir, dataset_train=dataset_train)\n",
    "\n",
    "    optimizer_kwargs = params['optimizer_kwargs'].copy()\n",
    "    if isinstance(params['optimizer_kwargs']['learning_rate'], float):\n",
    "        learning_rate = params['optimizer_kwargs']['learning_rate']\n",
    "    elif isinstance(params['optimizer_kwargs']['learning_rate'], str):\n",
    "        optimizer_kwargs['learning_rate'] = learning_rate = getattr(tf.keras.optimizers.schedules, params['optimizer_kwargs']['learning_rate'])(**params['optimizer_kwargs']['learning_rate_kwargs'])\n",
    "        del optimizer_kwargs['learning_rate_kwargs']\n",
    "    elif issubclass(params['optimizer_kwargs']['learning_rate'], tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        optimizer_kwargs['learning_rate'] = learning_rate = params['optimizer_kwargs']['learning_rate'](**params['optimizer_kwargs']['learning_rate_kwargs'])\n",
    "        del optimizer_kwargs['learning_rate_kwargs']\n",
    "\n",
    "    optimizer = getattr(tf.keras.optimizers, params['optimizer'])(**optimizer_kwargs) if isinstance(params['optimizer'], str) else params['optimizer'](**optimizer_kwargs)\n",
    "    model.compile(optimizer, loss=params['loss'], loss_weights=params.get('loss_weights', None), metrics=params.get('metrics', None))\n",
    "\n",
    "    if params.get('load_checkpoint', None) is not None:\n",
    "        model.load_weights(os.path.join(data_dir, params['load_checkpoint']))\n",
    "        if params['fit_kwargs'].get('verbose', 0) > 0:\n",
    "            print(\"loading weights from \", os.path.join(data_dir, params['load_checkpoint']))\n",
    "\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "\n",
    "    if model_dir is not None and params.get('checkpoint', False):\n",
    "        checkpoint_params = params['checkpoint_kwargs'].copy()\n",
    "        checkpoint_params['filepath'] = os.path.join(model_dir, checkpoint_params.pop('filename', 'weights.{epoch:05d}.hdf5'))\n",
    "        checkpoint_params['verbose'] = checkpoint_params.get('verbose', min(1, params['fit_kwargs'].get('verbose', 1)))\n",
    "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(**checkpoint_params))\n",
    "\n",
    "    if model_dir is not None and params.get('tensorboard', False):\n",
    "        tensorboard_callback_class = params['tensorboard'] if callable(params['tensorboard']) else tf.keras.callbacks.TensorBoard\n",
    "        callbacks.append(tensorboard_callback_class(log_dir=model_dir, learning_rate=learning_rate, **params['tensorboard_kwargs']))\n",
    "\n",
    "    model.fit(x=dataset_train.features, \n",
    "              y=dataset_train.targets, \n",
    "              callbacks=callbacks,\n",
    "              validation_data=(dataset_validate.features, dataset_validate.targets) if dataset_validate is not None else None,\n",
    "              **params['fit_kwargs'])\n",
    "\n",
    "    if model_dir is not None and params['save_model'] is True:\n",
    "        model.save(os.path.join(model_dir, 'model.h5')) \n",
    "\n",
    "    if model_dir is not None and params['export'] is True:\n",
    "        export_model = getattr(model, params['export_model']) if not params.get('export_model', 'self') == 'self' else model\n",
    "        tf.saved_model.save(export_model, os.path.join(model_dir, 'saved_model'))\n",
    "\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../quantumflow/keras_utils.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class KineticEnergyFunctionalDerivativeModel(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = params['model_kwargs']['base_model'](params)\n",
    "        self.h = tf.constant(params['dataset']['h'], dtype=params['dtype'])\n",
    "\n",
    "        self.output_names = sorted(['derivative'] + self.model.output_names)\n",
    "        self.input_names = self.model.input_names\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, density):\n",
    "        density = tf.nest.flatten(density)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(density)\n",
    "            kinetic_energy = self.model(density)\n",
    "\n",
    "        derivative = tf.identity(1/self.h*tape.gradient(kinetic_energy, density), name='derivative')\n",
    "        return derivative, kinetic_energy\n",
    "\n",
    "    def fit(self, y=None, validation_data=None, **kwargs):\n",
    "        if isinstance(y, dict):\n",
    "            y = tf.nest.flatten(y)\n",
    "        \n",
    "        if isinstance(validation_data, (tuple, list)) and isinstance(validation_data[1], dict):\n",
    "            validation_data = (validation_data[0], tf.nest.flatten(validation_data[1]))\n",
    "\n",
    "        super().fit(y=y, validation_data=validation_data, **kwargs)\n",
    "\n",
    "    def _set_output_attrs(self, outputs):\n",
    "        super()._set_output_attrs(outputs)\n",
    "        self.output_names = sorted(['derivative'] + self.model.output_names)\n",
    "\n",
    "    def summary(self, *args, **kwargs):\n",
    "        return self.model.summary(*args, **kwargs)\n",
    "\n",
    "    def save(self, *args, **kwargs):\n",
    "        return self.model.save(*args, **kwargs)\n",
    "\n",
    "    def save_weights(self, *args, **kwargs):\n",
    "        self.model.optimizer = self.optimizer\n",
    "        returns = self.model.save_weights(*args, **kwargs)\n",
    "        self.model.optimizer = None\n",
    "        return returns\n",
    "\n",
    "    def load_weights(self, *args, **kwargs):\n",
    "        self.model.optimizer = self.optimizer\n",
    "        returns = self.model.load_weights(*args, **kwargs)\n",
    "        self.optimizer = self.model.optimizer\n",
    "        self.model.optimizer = None\n",
    "        return returns\n",
    "\n",
    "import time\n",
    "\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, *args, metrics_freq=0, learning_rate=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.last_time = None\n",
    "        self.last_step = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.metrics_freq = metrics_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Runs metrics and histogram summaries at epoch end.\"\"\"\n",
    "        if self.metrics_freq and epoch % self.metrics_freq == 0 or any(['val_' in key for key in logs.keys()]):\n",
    "            self._log_metrics(logs, prefix='epoch_', step=epoch)\n",
    "\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_weights(epoch)\n",
    "\n",
    "        if self.embeddings_freq and epoch % self.embeddings_freq == 0:\n",
    "            self._log_embeddings(epoch)\n",
    "\n",
    "    def _log_metrics(self, logs, prefix, step):\n",
    "        \n",
    "        if self.last_time is not None:\n",
    "            new_time = time.time()\n",
    "            logs['epochs_per_second'] = (step - self.last_step)/(new_time - self.last_time)\n",
    "            self.last_time = new_time\n",
    "            self.last_step = step\n",
    "        else:\n",
    "            self.last_time = time.time()\n",
    "            self.last_step = step\n",
    "\n",
    "        if self.learning_rate is not None and isinstance(self.learning_rate, float):\n",
    "            logs['learning_rate'] = self.learning_rate\n",
    "        elif isinstance(self.learning_rate, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            logs['learning_rate'] = self.learning_rate(self.model.optimizer.iterations)\n",
    "\n",
    "        def rename_key(key):\n",
    "            prepend_val = False\n",
    "            if 'val_' in key:\n",
    "                prepend_val = True\n",
    "                key = key.replace('val_', '')\n",
    "            if '_loss' in key:\n",
    "                key = 'loss/' + key.replace('_loss', '')\n",
    "            if '_mean_absolute_error' in key:\n",
    "                key = 'mean_absolute_error/' + key.replace('_mean_absolute_error', '')\n",
    "            if prepend_val:\n",
    "                key = 'val_' + key\n",
    "            return key\n",
    "\n",
    "        logs = {rename_key(key): value for key, value in logs.items()}\n",
    "        super()._log_metrics(logs, '', step)\n",
    "\n",
    "\n",
    "class WarmupExponentialDecay(tf.keras.optimizers.schedules.ExponentialDecay):\n",
    "    def __init__(self, warmup_steps=None, cold_steps=None, cold_factor=0.1, final_learning_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.cold_steps = cold_steps\n",
    "        self.cold_factor = cold_factor\n",
    "        self.final_learning_rate = final_learning_rate\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, step):\n",
    "        return tf.where(step <= self.cold_steps + self.warmup_steps, \n",
    "                        tf.where(step <= self.cold_steps, \n",
    "                                 self.initial_learning_rate*self.cold_factor,\n",
    "                                 self.initial_learning_rate*(self.cold_factor + tf.cast(step - self.cold_steps, tf.float32)*(1 - self.cold_factor)/tf.cast(self.warmup_steps, tf.float32))), \n",
    "                        tf.maximum(super().__call__(step - self.cold_steps - self.warmup_steps), self.final_learning_rate))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'warmup_steps': self.warmup_steps})\n",
    "        config.update({'cold_steps': self.cold_steps})\n",
    "        config.update({'cold_factor': self.cold_factor})\n",
    "        config.update({'final_learning_rate': self.final_learning_rate})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPuBdZ7oxiatD34DBbUFUqs",
   "collapsed_sections": [],
   "name": "0_create_shared_project_files.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
