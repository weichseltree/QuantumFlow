{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ruamel.yaml\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "project_path = '/content/drive/MyDrive/Colab Projects/QuantumFlow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "if not os.path.exists('experiments'): os.makedirs('experiments')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import quantumflow\n",
    "\n",
    "experiment = 'test'\n",
    "run_name = 'resnet_100'\n",
    "\n",
    "base_dir = os.path.join(project_path, \"experiments\", experiment)\n",
    "params = quantumflow.utils.load_yaml(os.path.join(base_dir, 'hyperparams.yaml'))[run_name]\n",
    "run_dir = os.path.join(base_dir, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=$base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = quantumflow.instantiate(params['dataset_train'], run_dir=run_dir)\n",
    "dataset_train.build()\n",
    "\n",
    "dataset_validate = quantumflow.instantiate(params['dataset_validate'], run_dir=run_dir)\n",
    "dataset_validate.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(params['seed'])\n",
    "\n",
    "model = quantumflow.instantiate(params['model'], run_dir=run_dir, dataset=dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = quantumflow.instantiate(params['optimizer'])\n",
    "\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    loss=params['loss'], \n",
    "    loss_weights=params.get('loss_weights', None), \n",
    "    metrics=params.get('metrics', None)\n",
    ")\n",
    "\n",
    "\n",
    "if params.get('load_checkpoint', None) is not None:\n",
    "    model.load_weights(os.path.join(data_dir, params['load_checkpoint']))\n",
    "    if params['fit_kwargs'].get('verbose', 0) > 0:\n",
    "        print(\"loading weights from \", os.path.join(data_dir, params['load_checkpoint']))\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "'''\n",
    "\n",
    "if model_dir is not None and params.get('checkpoint', False):\n",
    "    checkpoint_params = params['checkpoint_kwargs'].copy()\n",
    "    checkpoint_params['filepath'] = os.path.join(model_dir, checkpoint_params.pop('filename', 'weights.{epoch:05d}.hdf5'))\n",
    "    checkpoint_params['verbose'] = checkpoint_params.get('verbose', min(1, params['fit_kwargs'].get('verbose', 1)))\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(**checkpoint_params))\n",
    "\n",
    "if model_dir is not None and params.get('tensorboard', False):\n",
    "    tensorboard_callback_class = params['tensorboard'] if callable(params['tensorboard']) else tf.keras.callbacks.TensorBoard\n",
    "    callbacks.append(tensorboard_callback_class(log_dir=model_dir, learning_rate=learning_rate, **params['tensorboard_kwargs']))\n",
    "'''\n",
    "\n",
    "\n",
    "model.fit(x=dataset_train.features, \n",
    "          y=dataset_train.targets, \n",
    "          callbacks=callbacks,\n",
    "          validation_data=(dataset_validate.features, dataset_validate.targets) if dataset_validate is not None else None,\n",
    "          **params['fit'])\n",
    "\n",
    "'''\n",
    "if model_dir is not None and params['save_model'] is True:\n",
    "    model.save(os.path.join(model_dir, 'model.h5')) \n",
    "\n",
    "if model_dir is not None and params['export'] is True:\n",
    "    export_model = getattr(model, params['export_model']) if not params.get('export_model', 'self') == 'self' else model\n",
    "    tf.saved_model.save(export_model, os.path.join(model_dir, 'saved_model'))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise dsfsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SampleCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, sample_freq=1, merge_layers=None):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.predictions = []\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.epochs = []\n",
    "        self.metrics = []\n",
    "        self.additional = []\n",
    "        \n",
    "        self.merge_layers = merge_layers or {}\n",
    "\n",
    "        self.sample_freq = sample_freq\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % self.sample_freq == 0:\n",
    "            self.epochs.append(epoch)\n",
    "            self.predictions.append(self.model(self.dataset.features))\n",
    "            self.weights.append({weight.name: weight.numpy() for weight in self.model.trainable_variables})\n",
    "    \n",
    "            layers_dict = OrderedDict()\n",
    "\n",
    "            def add_layer(layer, value, name):\n",
    "                if layer.name in self.merge_layers:\n",
    "                    value = layer([value, layers_dict[self.merge_layers[layer.name]]])\n",
    "                else:\n",
    "                    value = layer(value)\n",
    "                layers_dict[name] = value.numpy()\n",
    "                return value\n",
    "\n",
    "            value = self.dataset.density #tf.nest.flatten(self.dataset.features)\n",
    "            for layer in self.model.layers:\n",
    "                if hasattr(layer, 'layers'): # sub-model\n",
    "                    for sub_layer in layer.layers:\n",
    "                        value = add_layer(sub_layer, value, layer.name + '/' + sub_layer.name)\n",
    "                else:\n",
    "                    value = add_layer(layer, value, layer.name)\n",
    "\n",
    "            self.layers.append(layers_dict)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.sample_freq == 0:\n",
    "            self.metrics.append(logs)\n",
    "            self.additional.append({'learning_rate': self.model.optimizer._decayed_lr(tf.float32).numpy(),\n",
    "                                    'adam_iterations': self.model.optimizer.iterations.numpy(),\n",
    "                                    'adam_m_' + self.model.trainable_variables[0].name: self.model.optimizer.get_slot(self.model.trainable_variables[0], 'm').numpy(),\n",
    "                                    'adam_v_' + self.model.trainable_variables[0].name: self.model.optimizer.get_slot(self.model.trainable_variables[0], 'v').numpy(),\n",
    "                                    'adam_beta_1': self.model.optimizer._get_hyper('beta_1', tf.float32).numpy()})\n",
    "    def get_metric(self, key):\n",
    "        return tf.stack([metric[key] for metric in self.metrics])\n",
    "\n",
    "    def get_prediction(self, key):\n",
    "        return tf.stack([prediction[key] for prediction in self.predictions])\n",
    "\n",
    "    def get_weight(self, key):\n",
    "        return tf.stack([weight[key] for weight in self.weights])\n",
    "\n",
    "    def get_layer(self, key):\n",
    "        return tf.stack([layer[key] for layer in self.layers])\n",
    "    \n",
    "    def get_additional(self, key):\n",
    "        return tf.stack([add[key] for add in self.additional])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantumflow.utils import anim_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'resnet_vW_N2_100000'\n",
    "load_checkpoint = 0\n",
    "run_epochs = 300\n",
    "sample_freq = 2\n",
    "model_dir = os.path.join(data_dir, experiment, run_name)\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "model = build_model(params)\n",
    "display(model.summary())\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "params['fit_kwargs']['initial_epoch'] = load_checkpoint\n",
    "params['load_checkpoint'] = ('{}/{}/' + params['checkpoint_kwargs']['filename']).format(experiment, run_name, epoch=load_checkpoint) if load_checkpoint else None\n",
    "params['fit_kwargs']['epochs'] = params['fit_kwargs']['initial_epoch'] + run_epochs\n",
    "\n",
    "merge_layers = {'add': 'model/lambda',\n",
    "                'add_1': 'model/activation',\n",
    "                'add_2': 'model/activation_1',\n",
    "                'add_3': 'model/activation_2',\n",
    "                'add_4': 'model/activation_3'}\n",
    "\n",
    "#model_dir = '../data/pop_test/' + run_name\n",
    "\n",
    "#params['dataset'] = {'h': 1/499}\n",
    "#params['loss'] = {'kinetic_energy': params['loss']['kinetic_energy'], 'derivative': params['loss']['derivative'](params)}\n",
    "\n",
    "sample_callback = SampleCallback(QFDataset(os.path.join(data_dir, 'recreate/dataset_sample.hdf5'), params), sample_freq=sample_freq, merge_layers=merge_layers)\n",
    "model, params = train(params, callbacks=[sample_callback], model_dir=globals().get('model_dir', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_callback.weights[0].keys())\n",
    "print(sample_callback.metrics[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(sample_callback.epochs, sample_callback.get_prediction('kinetic_energy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kedensity = sample_callback.get_prediction('kinetic_energy_density')\n",
    "kedensity = np.stack([kedensity, np.repeat(np.expand_dims(sample_callback.dataset.kinetic_energy_density, axis=0), len(kedensity), axis=0)], axis=2)\n",
    "print(kedensity.shape)\n",
    "anim_plot(np.moveaxis(kedensity[:, 0], 2, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative = sample_callback.get_prediction('derivative')\n",
    "derivative = np.stack([derivative[:, 0], np.repeat(np.expand_dims(sample_callback.dataset.derivative, axis=0), len(derivative), axis=0)], axis=2)\n",
    "print(derivative.shape)\n",
    "anim_plot(np.moveaxis(derivative[:, 0], 2, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(sample_callback.epochs, sample_callback.get_metric('loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = sample_callback.layers[0].keys()\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    print(layer_name)\n",
    "    value = sample_callback.get_layer(layer_name)\n",
    "    #frame = 0\n",
    "    #plt.figure(figsize=(10, 1))\n",
    "    #plt.plot(value[frame][0])\n",
    "    #plt.title(layer_name + ' ' + str(value[frame].shape))\n",
    "    #plt.show()\n",
    "\n",
    "    if len(value.shape) < 3:\n",
    "        continue\n",
    "    if len(value.shape) == 3:\n",
    "        value = tf.expand_dims(value, axis=-1)\n",
    "\n",
    "    anim_plot(value[:, 0], figsize=(10, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "for key in sample_callback.metrics[0].keys():\n",
    "    if 'loss' not in key:\n",
    "        continue\n",
    "    metric = sample_callback.get_metric(key)\n",
    "    while len(metric.shape) > 1:\n",
    "        metric = tf.reduce_mean(metric, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, metric, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "for key in sample_callback.metrics[0].keys():\n",
    "    if 'mean' not in key:\n",
    "        continue\n",
    "    metric = sample_callback.get_metric(key)\n",
    "    while len(metric.shape) > 1:\n",
    "        metric = tf.reduce_mean(metric, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, metric, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for key in sample_callback.weights[0].keys():\n",
    "    if not 'conv' in key:\n",
    "        continue\n",
    "    weight = sample_callback.get_weight(key)\n",
    "    while len(weight.shape) > 1:\n",
    "        weight = tf.reduce_mean(weight, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, weight, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sample_callback.get_weight('conv1d_4/kernel:0')\n",
    "anim_plot(kernel[:, :, 0], bar='Rendering')\n",
    "gradient = kernel[1:] - kernel[:-1]\n",
    "anim_plot(gradient[:, 0], bar='Rendering')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(np.log(tf.reduce_sum(tf.square(tf.reshape(gradient, [run_epochs//sample_freq-1, -1])), axis=-1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNfOGjW+504AHQhVWxo/SGW",
   "collapsed_sections": [],
   "name": "train_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
