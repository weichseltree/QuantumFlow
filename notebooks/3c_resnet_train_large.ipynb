{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup notebook if it is run on Google Colab, cwd = notebook file location\n",
    "try:\n",
    "    # change notebook_path if this notebook is in a different subfolder of Google Drive\n",
    "    notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q ruamel.yaml\n",
    "\n",
    "    %load_ext tensorboard\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# imports\n",
    "import tensorflow as tf\n",
    "\n",
    "# setup paths and variables for shared code (../quantumflow) and data (../data)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# import shared code, must run 0_create_shared_project_files.ipynb first!\n",
    "from quantumflow.utils import load_hyperparameters, train, build_model, QFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'models_large'\n",
    "\n",
    "base_dir = os.path.join(data_dir, experiment)\n",
    "if not os.path.exists(base_dir): os.makedirs(base_dir)\n",
    "\n",
    "file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")\n",
    "file_model = os.path.join(base_dir, \"model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_model\n",
    "import tensorflow as tf\n",
    "from quantumflow.keras_utils import CustomTensorBoard, WarmupExponentialDecay\n",
    "\n",
    "class IntegrateLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, h=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.h*tf.reduce_sum((inputs[:, :-1] + inputs[:, 1:])/2., axis=1, name='trapezoidal_integral_approx')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'h': self.h})\n",
    "        return config\n",
    "\n",
    "class SymmetricConv1D(tf.keras.layers.Conv1D):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.kernel.assign(tf.concat([self.kernel[:(self.kernel_size[0]+1)//2], tf.reverse(self.kernel[:self.kernel_size[0]//2], axis=[0])], axis=0))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_rev = tf.reverse(inputs, axis=[1])\n",
    "        output = super().call(inputs)\n",
    "        output_rev = super().call(input_rev)\n",
    "        return 0.5*(output + tf.reverse(output_rev, axis=[1]))\n",
    "\n",
    "def CNN_KineticEnergyFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['model_kwargs']['l2_regularisation']) if params['model_kwargs'].get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params['model_kwargs'].get('bias_mean_initialisation', False) else None\n",
    "    conv1d_layer = SymmetricConv1D if params['model_kwargs'].get('symmetric', False) else tf.keras.layers.Conv1D\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['dataset']['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    for layer in range(len(params['model_kwargs']['filters'])):\n",
    "        value = conv1d_layer(filters=params['model_kwargs']['filters'][layer], \n",
    "                             kernel_size=params['model_kwargs']['kernel_size'][layer], \n",
    "                             activation=params['model_kwargs']['activation'] if not params['model_kwargs'].get('batch_normalization', False) else None, \n",
    "                             padding=params['model_kwargs']['padding'],\n",
    "                             kernel_regularizer=kernel_regularizer)(value)\n",
    "        if params['model_kwargs'].get('batch_normalization', False):\n",
    "            value = tf.keras.layers.BatchNormalization()(value)\n",
    "            value = tf.keras.layers.Activation(params['model_kwargs']['activation'])(value)\n",
    "\n",
    "    value = tf.keras.layers.Flatten()(value)\n",
    "    value = tf.keras.layers.Dense(1, kernel_regularizer=kernel_regularizer, bias_initializer=bias_initializer)(value)\n",
    "    kinetic_energy = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy')(value)\n",
    "\n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy})\n",
    "\n",
    "\n",
    "def CNN_KineticEnergyDensityFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['model_kwargs']['l2_regularisation']) if params['model_kwargs'].get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params['model_kwargs'].get('bias_mean_initialisation', False) else None\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['dataset']['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    for layer in range(len(params['model_kwargs']['filters'])):\n",
    "        value = tf.keras.layers.Conv1D(filters=params['model_kwargs']['filters'][layer], \n",
    "                                       kernel_size=params['model_kwargs']['kernel_size'][layer], \n",
    "                                       activation=params['model_kwargs']['activation'] if not params['model_kwargs'].get('batch_normalization', False) and layer < len(params['model_kwargs']['filters'])-1 else None, \n",
    "                                       padding=params['model_kwargs']['padding'],\n",
    "                                       kernel_regularizer=kernel_regularizer)(value)\n",
    "\n",
    "        if params['model_kwargs'].get('batch_normalization', False):\n",
    "            value = tf.keras.layers.BatchNormalization()(value)\n",
    "\n",
    "            if layer < len(params['model_kwargs']['filters'])-1:\n",
    "                value = tf.keras.layers.Activation(params['model_kwargs']['activation'])(value)\n",
    "\n",
    "    kinetic_energy_density = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy_density')(value)\n",
    "    kinetic_energy = IntegrateLayer(params['dataset']['h'], name='kinetic_energy')(kinetic_energy_density)\n",
    "    \n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy, 'kinetic_energy_density': kinetic_energy_density})\n",
    "\n",
    "\n",
    "def ResNet_KineticEnergyDensityFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['model_kwargs']['l2_regularisation']) if params['model_kwargs'].get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params['model_kwargs'].get('bias_mean_initialisation', False) else None\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['dataset']['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    def resnet_block(input, filters, kernel_size, padding=None, activation=None, add_input=True):\n",
    "        value = input\n",
    "        for layer in range(len(filters)):\n",
    "            value = tf.keras.layers.Conv1D(filters=filters[layer], \n",
    "                                        kernel_size=kernel_size[layer], \n",
    "                                        padding=padding,\n",
    "                                        use_bias=True,\n",
    "                                        activation= activation if layer < len(filters)-1 else None,\n",
    "                                        kernel_regularizer=kernel_regularizer)(value)\n",
    "\n",
    "        if add_input:\n",
    "            value = tf.keras.layers.Add()([value, input])\n",
    "        \n",
    "        if activation is not None:\n",
    "            return tf.keras.layers.Activation(activation=activation)(value)\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    for layer in range(len(params['model_kwargs']['blocks'])):\n",
    "        value = resnet_block(value, **params['model_kwargs']['blocks'][layer])\n",
    "\n",
    "    kinetic_energy_density = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy_density')(value)\n",
    "    kinetic_energy = IntegrateLayer(params['dataset']['h'], name='kinetic_energy')(kinetic_energy_density)\n",
    "    \n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy, 'kinetic_energy_density': kinetic_energy_density})\n",
    "\n",
    "\n",
    "class KineticEnergyFunctionalDerivativeModel(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = params['model_kwargs']['base_model'](params)\n",
    "        self.h = tf.constant(params['dataset']['h'], dtype=params['dtype'])\n",
    "\n",
    "        self.output_names = sorted(['derivative'] + self.model.output_names)\n",
    "        self.input_names = self.model.input_names\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, density):\n",
    "        density = tf.nest.flatten(density)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(density)\n",
    "            predictions = self.model(density)\n",
    "            kinetic_energy = predictions['kinetic_energy']\n",
    "\n",
    "        predictions['derivative'] = tf.identity(1/self.h*tape.gradient(kinetic_energy, density), name='derivative')\n",
    "        return predictions\n",
    "\n",
    "    #def fit(self, y=None, validation_data=None, **kwargs):\n",
    "    #    if isinstance(y, dict):\n",
    "    #        y = tf.nest.flatten(y)\n",
    "    #    \n",
    "    #    if isinstance(validation_data, (tuple, list)) and isinstance(validation_data[1], dict):\n",
    "    #        validation_data = (validation_data[0], tf.nest.flatten(validation_data[1]))#\n",
    "    #\n",
    "    #    super().fit(y=y, validation_data=validation_data, **kwargs)\n",
    "\n",
    "    def _set_output_attrs(self, outputs):\n",
    "        super()._set_output_attrs(outputs)\n",
    "        self.output_names = sorted(['derivative'] + self.model.output_names)\n",
    "\n",
    "    def summary(self, *args, **kwargs):\n",
    "        return self.model.summary(*args, **kwargs)\n",
    "\n",
    "    def save(self, *args, **kwargs):\n",
    "        return self.model.save(*args, **kwargs)\n",
    "\n",
    "    def save_weights(self, *args, **kwargs):\n",
    "        self.model.optimizer = self.optimizer\n",
    "        returns = self.model.save_weights(*args, **kwargs)\n",
    "        self.model.optimizer = None\n",
    "        return returns\n",
    "\n",
    "    def load_weights(self, *args, **kwargs):\n",
    "        self.model.optimizer = self.optimizer\n",
    "        returns = self.model.load_weights(*args, **kwargs)\n",
    "        self.optimizer = self.model.optimizer\n",
    "        self.model.optimizer = None\n",
    "        return returns\n",
    "\n",
    "\n",
    "#from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "\n",
    "#def Ranger(*args, **kwargs):\n",
    "#    return Lookahead(RectifiedAdam(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run $file_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_hyperparams\n",
    "globals: \n",
    "    - CNN_KineticEnergyFunctional\n",
    "    - CNN_KineticEnergyDensityFunctional\n",
    "    - KineticEnergyFunctionalDerivativeModel\n",
    "    - ResNet_KineticEnergyDensityFunctional\n",
    "    - WarmupExponentialDecay\n",
    "    - CustomTensorBoard\n",
    "    \n",
    "resnet_1000: &RESNET\n",
    "    load_checkpoint: null\n",
    "\n",
    "    N: 1\n",
    "    seed: 0\n",
    "    dtype: float32\n",
    "    dataset_train: datasets/dataset_large.hdf5\n",
    "    dataset_validate: datasets/dataset_validate.hdf5\n",
    "    dataset_info:\n",
    "        shapes: True\n",
    "        h: True\n",
    "\n",
    "    model: KineticEnergyFunctionalDerivativeModel\n",
    "    model_kwargs: &RESNET_MODEL_KWARGS\n",
    "        base_model: ResNet_KineticEnergyDensityFunctional\n",
    "        l2_regularisation: 0.000025\n",
    "        bias_mean_initialisation: False\n",
    "        batch_normalization: False\n",
    "\n",
    "        resnet_block_kwargs: &RESNET_RESNET_BLOCK\n",
    "            filters: [32, 32]\n",
    "            kernel_size: [100, 100]\n",
    "            padding: same\n",
    "            activation: softplus\n",
    "            add_input: True\n",
    "\n",
    "        final_block_kwargs: &RESNET_FINAL_BLOCK\n",
    "            filters: [1]\n",
    "            kernel_size: [100]\n",
    "            padding: same\n",
    "            activation: null\n",
    "            add_input: False\n",
    "\n",
    "        blocks: [<<: *RESNET_RESNET_BLOCK,\n",
    "                 <<: *RESNET_RESNET_BLOCK,\n",
    "                 <<: *RESNET_RESNET_BLOCK,\n",
    "                 <<: *RESNET_FINAL_BLOCK]\n",
    "                    \n",
    "\n",
    "    features: [density]\n",
    "    targets: [kinetic_energy, kinetic_energy_density, derivative]\n",
    "\n",
    "    metrics: \n",
    "        kinetic_energy: mean_absolute_error\n",
    "        kinetic_energy_density: mean_absolute_error\n",
    "        derivative: mean_absolute_error\n",
    "\n",
    "    loss: \n",
    "        kinetic_energy: mean_squared_error\n",
    "        kinetic_energy_density: mean_squared_error\n",
    "        derivative: mean_squared_error\n",
    "\n",
    "    loss_weights:\n",
    "        kinetic_energy: 1.0\n",
    "        kinetic_energy_density: 1.0\n",
    "        derivative: 1.0\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0000000001\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 43700 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 20000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 1000 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 20000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "    tensorboard: CustomTensorBoard\n",
    "    tensorboard_kwargs:\n",
    "        histogram_freq: 10000 # epochs\n",
    "        metrics_freq: 100 # epochs\n",
    "        write_graph: True\n",
    "        write_images: True\n",
    "        update_freq: epoch\n",
    "        profile_batch: 0\n",
    "\n",
    "    save_model: False\n",
    "    export: True\n",
    "    export_model: model\n",
    "\n",
    "\n",
    "resnet_1000_steep: \n",
    "    <<: *RESNET\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.875\n",
    "            decay_steps: 1800 # batches\n",
    "\n",
    "            cold_steps: 43700 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "\n",
    "resnet_1000_longer: \n",
    "    <<: *RESNET\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 25000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 1000 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "\n",
    "resnet_1000_longerer: \n",
    "    <<: *RESNET\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 30000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 1000 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "\n",
    "resnet_vW_N2_1000_longerer: \n",
    "    <<: *RESNET\n",
    "    N: 2\n",
    "    subtract_von_weizsaecker: True\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 30000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 1000 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 30000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "\n",
    "resnet_vW_N2_1000_longerer_nf: \n",
    "    <<: *RESNET\n",
    "    N: 2\n",
    "    normalize_features: True\n",
    "    subtract_von_weizsaecker: True\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 30000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 1000 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 30000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "\n",
    "resnet_vW_N2_1000_longerer_nf_2: \n",
    "    <<: *RESNET\n",
    "    N: 2\n",
    "    normalize_features: True\n",
    "    subtract_von_weizsaecker: True\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 30000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 1000 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 30000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "\n",
    "resnet_vW_N2_10000: \n",
    "    <<: *RESNET\n",
    "    dataset_train: datasets/dataset_larger.hdf5\n",
    "    N: 2\n",
    "    subtract_von_weizsaecker: True\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 3000\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 100 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 30000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "    tensorboard: CustomTensorBoard\n",
    "    tensorboard_kwargs:\n",
    "        histogram_freq: 1000 # epochs\n",
    "        metrics_freq: 10 # epochs\n",
    "        write_graph: True\n",
    "        write_images: True\n",
    "        update_freq: epoch\n",
    "        profile_batch: 0\n",
    "\n",
    "\n",
    "\n",
    "resnet_vW_N2_50000: &RESNET_50000\n",
    "    <<: *RESNET\n",
    "    dataset_train: datasets/dataset_huge.hdf5\n",
    "    N: 2\n",
    "    subtract_von_weizsaecker: True\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 600\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 20 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 30000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "    tensorboard: CustomTensorBoard\n",
    "    tensorboard_kwargs:\n",
    "        histogram_freq: 200 # epochs\n",
    "        metrics_freq: 2 # epochs\n",
    "        write_graph: True\n",
    "        write_images: True\n",
    "        update_freq: epoch\n",
    "        profile_batch: 0\n",
    "\n",
    "\n",
    "resnet_vW_N2_50000_l2: \n",
    "    <<: *RESNET_50000\n",
    "    model_kwargs:\n",
    "        <<: *RESNET_MODEL_KWARGS\n",
    "        l2_regularisation: 0.00000025\n",
    "\n",
    "\n",
    "resnet_vW_N2_100000:\n",
    "    <<: *RESNET\n",
    "    dataset_train: datasets/dataset_huger.hdf5\n",
    "    N: 2\n",
    "    subtract_von_weizsaecker: True\n",
    "\n",
    "    model_kwargs:\n",
    "        <<: *RESNET_MODEL_KWARGS\n",
    "        l2_regularisation: 0.0000001\n",
    "\n",
    "    optimizer: Adam\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: WarmupExponentialDecay\n",
    "        beta_1: 0.9\n",
    "        beta_2: 0.999\n",
    "        epsilon: 1e-07\n",
    "        amsgrad: False\n",
    "\n",
    "        clipnorm: 100.0\n",
    "\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.0001\n",
    "            final_learning_rate: 0.0\n",
    "            decay_rate: 0.9\n",
    "            decay_steps: 2000 # batches\n",
    "\n",
    "            cold_steps: 40000 # batches\n",
    "            warmup_steps: 0 # batches\n",
    "            cold_factor: 1.0\n",
    "\n",
    "    fit_kwargs:\n",
    "        batch_size: 100\n",
    "        epochs: 300\n",
    "        verbose: 0\n",
    "        #validation_split: 0.1\n",
    "        validation_freq: 20 # epochs\n",
    "        shuffle: True\n",
    "        initial_epoch: 0\n",
    "\n",
    "    checkpoint: True\n",
    "    checkpoint_kwargs:\n",
    "        filename: weights.{epoch:03d}.tf\n",
    "        save_freq: 30000 # samples\n",
    "        save_weights_only: True\n",
    "\n",
    "    tensorboard: CustomTensorBoard\n",
    "    tensorboard_kwargs:\n",
    "        histogram_freq: 100 # epochs\n",
    "        metrics_freq: 1 # epochs\n",
    "        write_graph: True\n",
    "        write_images: True\n",
    "        update_freq: epoch\n",
    "        profile_batch: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SampleCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, sample_freq=1, merge_layers=None):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.predictions = []\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.epochs = []\n",
    "        self.metrics = []\n",
    "        self.additional = []\n",
    "        \n",
    "        self.merge_layers = merge_layers or {}\n",
    "\n",
    "        self.sample_freq = sample_freq\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % self.sample_freq == 0:\n",
    "            self.epochs.append(epoch)\n",
    "            self.predictions.append(self.model(self.dataset.features))\n",
    "            self.weights.append({weight.name: weight.numpy() for weight in self.model.trainable_variables})\n",
    "    \n",
    "            layers_dict = OrderedDict()\n",
    "\n",
    "            def add_layer(layer, value, name):\n",
    "                if layer.name in self.merge_layers:\n",
    "                    value = layer([value, layers_dict[self.merge_layers[layer.name]]])\n",
    "                else:\n",
    "                    value = layer(value)\n",
    "                layers_dict[name] = value.numpy()\n",
    "                return value\n",
    "\n",
    "            value = self.dataset.density #tf.nest.flatten(self.dataset.features)\n",
    "            for layer in self.model.layers:\n",
    "                if hasattr(layer, 'layers'): # sub-model\n",
    "                    for sub_layer in layer.layers:\n",
    "                        value = add_layer(sub_layer, value, layer.name + '/' + sub_layer.name)\n",
    "                else:\n",
    "                    value = add_layer(layer, value, layer.name)\n",
    "\n",
    "            self.layers.append(layers_dict)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.sample_freq == 0:\n",
    "            self.metrics.append(logs)\n",
    "            self.additional.append({'learning_rate': self.model.optimizer._decayed_lr(tf.float32).numpy(),\n",
    "                                    'adam_iterations': self.model.optimizer.iterations.numpy(),\n",
    "                                    'adam_m_' + self.model.trainable_variables[0].name: self.model.optimizer.get_slot(self.model.trainable_variables[0], 'm').numpy(),\n",
    "                                    'adam_v_' + self.model.trainable_variables[0].name: self.model.optimizer.get_slot(self.model.trainable_variables[0], 'v').numpy(),\n",
    "                                    'adam_beta_1': self.model.optimizer._get_hyper('beta_1', tf.float32).numpy()})\n",
    "    def get_metric(self, key):\n",
    "        return tf.stack([metric[key] for metric in self.metrics])\n",
    "\n",
    "    def get_prediction(self, key):\n",
    "        return tf.stack([prediction[key] for prediction in self.predictions])\n",
    "\n",
    "    def get_weight(self, key):\n",
    "        return tf.stack([weight[key] for weight in self.weights])\n",
    "\n",
    "    def get_layer(self, key):\n",
    "        return tf.stack([layer[key] for layer in self.layers])\n",
    "    \n",
    "    def get_additional(self, key):\n",
    "        return tf.stack([add[key] for add in self.additional])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantumflow.utils import anim_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'resnet_vW_N2_100000'\n",
    "load_checkpoint = 0\n",
    "run_epochs = 300\n",
    "sample_freq = 2\n",
    "model_dir = os.path.join(data_dir, experiment, run_name)\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "model = build_model(params)\n",
    "display(model.summary())\n",
    "\n",
    "params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "params['fit_kwargs']['initial_epoch'] = load_checkpoint\n",
    "params['load_checkpoint'] = ('{}/{}/' + params['checkpoint_kwargs']['filename']).format(experiment, run_name, epoch=load_checkpoint) if load_checkpoint else None\n",
    "params['fit_kwargs']['epochs'] = params['fit_kwargs']['initial_epoch'] + run_epochs\n",
    "\n",
    "merge_layers = {'add': 'model/lambda',\n",
    "                'add_1': 'model/activation',\n",
    "                'add_2': 'model/activation_1',\n",
    "                'add_3': 'model/activation_2',\n",
    "                'add_4': 'model/activation_3'}\n",
    "\n",
    "#model_dir = '../data/pop_test/' + run_name\n",
    "\n",
    "#params['dataset'] = {'h': 1/499}\n",
    "#params['loss'] = {'kinetic_energy': params['loss']['kinetic_energy'], 'derivative': params['loss']['derivative'](params)}\n",
    "\n",
    "sample_callback = SampleCallback(QFDataset(os.path.join(data_dir, 'recreate/dataset_sample.hdf5'), params), sample_freq=sample_freq, merge_layers=merge_layers)\n",
    "model, params = train(params, callbacks=[sample_callback], model_dir=globals().get('model_dir', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_callback.weights[0].keys())\n",
    "print(sample_callback.metrics[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(sample_callback.epochs, sample_callback.get_prediction('kinetic_energy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kedensity = sample_callback.get_prediction('kinetic_energy_density')\n",
    "kedensity = np.stack([kedensity, np.repeat(np.expand_dims(sample_callback.dataset.kinetic_energy_density, axis=0), len(kedensity), axis=0)], axis=2)\n",
    "print(kedensity.shape)\n",
    "anim_plot(np.moveaxis(kedensity[:, 0], 2, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative = sample_callback.get_prediction('derivative')\n",
    "derivative = np.stack([derivative[:, 0], np.repeat(np.expand_dims(sample_callback.dataset.derivative, axis=0), len(derivative), axis=0)], axis=2)\n",
    "print(derivative.shape)\n",
    "anim_plot(np.moveaxis(derivative[:, 0], 2, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(sample_callback.epochs, sample_callback.get_metric('loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = sample_callback.layers[0].keys()\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    print(layer_name)\n",
    "    value = sample_callback.get_layer(layer_name)\n",
    "    #frame = 0\n",
    "    #plt.figure(figsize=(10, 1))\n",
    "    #plt.plot(value[frame][0])\n",
    "    #plt.title(layer_name + ' ' + str(value[frame].shape))\n",
    "    #plt.show()\n",
    "\n",
    "    if len(value.shape) < 3:\n",
    "        continue\n",
    "    if len(value.shape) == 3:\n",
    "        value = tf.expand_dims(value, axis=-1)\n",
    "\n",
    "    anim_plot(value[:, 0], figsize=(10, 1), bar='Rendering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "for key in sample_callback.metrics[0].keys():\n",
    "    if 'loss' not in key:\n",
    "        continue\n",
    "    metric = sample_callback.get_metric(key)\n",
    "    while len(metric.shape) > 1:\n",
    "        metric = tf.reduce_mean(metric, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, metric, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "for key in sample_callback.metrics[0].keys():\n",
    "    if 'mean' not in key:\n",
    "        continue\n",
    "    metric = sample_callback.get_metric(key)\n",
    "    while len(metric.shape) > 1:\n",
    "        metric = tf.reduce_mean(metric, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, metric, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for key in sample_callback.weights[0].keys():\n",
    "    if not 'conv' in key:\n",
    "        continue\n",
    "    weight = sample_callback.get_weight(key)\n",
    "    while len(weight.shape) > 1:\n",
    "        weight = tf.reduce_mean(weight, axis=-1)\n",
    "\n",
    "    plt.plot(sample_callback.epochs, weight, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sample_callback.get_weight('conv1d_4/kernel:0')\n",
    "anim_plot(kernel[:, :, 0], bar='Rendering')\n",
    "gradient = kernel[1:] - kernel[:-1]\n",
    "anim_plot(gradient[:, 0], bar='Rendering')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(np.log(tf.reduce_sum(tf.square(tf.reshape(gradient, [run_epochs//sample_freq-1, -1])), axis=-1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUkCtGbRUd1oOoIQtXITGl",
   "collapsed_sections": [],
   "name": "3c_resnet_train_large.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
