{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = \"Projects/QuantumFlow/notebooks\"\n",
    "try:\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/\" + notebook_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "!pip install -q ruamel.yaml\n",
    "!pip install -q tensorflow-addons\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from quantumflow.colab_utils import load_hyperparameters, test_colab_devices, get_resolver, QFDataset\n",
    "\n",
    "has_gpu, has_tpu = test_colab_devices()\n",
    "if has_gpu: print(\"Found GPU\")\n",
    "if has_tpu: print(\"Found TPU\")\n",
    "\n",
    "data_dir = \"../data\"\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "experiment = 'ke_cnn'\n",
    "\n",
    "base_dir = os.path.join(data_dir, experiment)\n",
    "log_dir = \"gs://quantumflow/\" + experiment if has_tpu else '/home/' + experiment\n",
    "if not os.path.exists(base_dir): os.makedirs(base_dir)\n",
    "file_hyperparams = os.path.join(base_dir, \"hyperparams.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_hyperparams\n",
    "globals: [ConvNNKineticEnergyFunctional, DerivConvNNKineticEnergyFunctional, has_tpu]\n",
    "\n",
    "default: &DEFAULT\n",
    "    dataset_train: recreate/dataset_paper\n",
    "    dataset_validate: recreate/dataset_validate\n",
    "    N: 1\n",
    "    seed: 0\n",
    "    dtype: float32\n",
    "\n",
    "    model_fn: DerivConvNNKineticEnergyFunctional\n",
    "    base_model_fn: ConvNNKineticEnergyFunctional\n",
    "\n",
    "    model: &DEFAULT_MODEL\n",
    "        filters: [32, 32, 32, 32, 32]\n",
    "        kernel_size: [100, 100, 100, 100, 100]\n",
    "        padding: valid\n",
    "        activation: softplus\n",
    "        l2_regularization: True\n",
    "        bias_mean_initialization: True\n",
    "\n",
    "    features: ['density']\n",
    "    targets: ['kinetic_energy', 'derivative']\n",
    "\n",
    "    eval_metrics:\n",
    "        kinetic_energy: MeanAbsoluteError\n",
    "        derivative: MeanAbsoluteError\n",
    "\n",
    "    loss: &DEFAULT_LOSS\n",
    "        kinetic_energy: MeanSquaredError\n",
    "        derivative: MeanSquaredError\n",
    "\n",
    "    loss_weights:\n",
    "        regularization: 0.00002\n",
    "        kinetic_energy: 1.0\n",
    "        derivative: 0.2\n",
    "\n",
    "    shuffle: True\n",
    "    shuffle_buffer_size: 100\n",
    "    drop_remainder: False\n",
    "    \n",
    "    eval_batch_size: 250\n",
    "    train_batch_size: 10\n",
    "\n",
    "    train_epochs: 10000\n",
    "    train_epochs_per_eval: 1000\n",
    "\n",
    "    save_summary_epochs: 100\n",
    "    save_model_epochs: 2000\n",
    "\n",
    "    summaries: [losses, metrics, learning_rate, examples_per_sec, global_norm]\n",
    "\n",
    "    optimizer: RectifiedAdam\n",
    "    optimizer_lookahead: True\n",
    "    \n",
    "    gradient_clip_norm: 100.0\n",
    "\n",
    "    optimizer_kwargs:\n",
    "        learning_rate: ExponentialDecay\n",
    "        learning_rate_kwargs:\n",
    "            initial_learning_rate: 0.002\n",
    "            decay_steps: 10000\n",
    "            decay_rate: 0.5\n",
    "            staircase: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNNKineticEnergyFunctional(params):\n",
    "    density = tf.keras.layers.Input(shape=params['features_shape']['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "\n",
    "    kernel_regularizer = None\n",
    "    bias_initializer = None\n",
    "\n",
    "    if params['model'].get('l2_regularization', False):\n",
    "        kernel_regularizer = tf.keras.regularizers.l2(params['loss_weights']['regularization'])\n",
    "\n",
    "    if params['model'].get('bias_mean_initialization', False):\n",
    "        bias_initializer = tf.constant_initializer(value=params['targets_mean']['kinetic_energy'])\n",
    "\n",
    "    for layer in range(len(params['model']['filters'])):\n",
    "        value = tf.keras.layers.Conv1D(filters=params['model']['filters'][layer], \n",
    "                                       kernel_size=params['model']['kernel_size'][layer], \n",
    "                                       activation=params['model']['activation'], \n",
    "                                       padding=params['model']['padding'],\n",
    "                                       kernel_regularizer=kernel_regularizer)(value)\n",
    "\n",
    "    value = tf.keras.layers.Flatten()(value)\n",
    "    value = tf.keras.layers.Dense(1, kernel_regularizer=kernel_regularizer, bias_initializer=bias_initializer)(value)\n",
    "    kinetic_energy = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy')(value)\n",
    "\n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy})\n",
    "\n",
    "class DerivConvNNKineticEnergyFunctional(tf.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = params['base_model_fn'](params)\n",
    "        self.h = params['h']\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, inputs, training=False):\n",
    "        if isinstance(inputs, dict):\n",
    "            density = inputs['density']\n",
    "        else:\n",
    "            density = inputs\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inputs)\n",
    "            predictions = self.model(inputs)\n",
    "            kinetic_energy = predictions['kinetic_energy']\n",
    "\n",
    "        predictions['derivative'] = 1/self.h*tape.gradient(kinetic_energy, density)\n",
    "        return predictions\n",
    "\n",
    "    @property\n",
    "    def losses(self):\n",
    "        return self.model.losses\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return self.model.trainable_weights\n",
    "\n",
    "    def signatures(self, dataset_train):\n",
    "        return {'serving_default': self.__call__.get_concrete_function(tf.TensorSpec([None, dataset_train.discretisation_points], dataset_train.dtype, name='density')),\n",
    "                'serving_dict': self.__call__.get_concrete_function({'density': tf.TensorSpec([None, dataset_train.discretisation_points], dataset_train.dtype, name='density')})\n",
    "                }\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = widgets.IntProgress(value=0, max=0, description='...', \n",
    "                               bar_style='info', layout=widgets.Layout(width='92%'))\n",
    "display(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_names = ['default']\n",
    "\n",
    "remote_file_hyperparams = os.path.join(log_dir, \"hyperparams.config\")\n",
    "if log_dir.startswith('gs://'):\n",
    "    !gsutil -q cp $file_hyperparams $remote_file_hyperparams\n",
    "else:\n",
    "    if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "    !cp $file_hyperparams $remote_file_hyperparams\n",
    "\n",
    "for run_name in training_run_names:\n",
    "    params = load_hyperparameters(file_hyperparams, run_name=run_name, globals=globals())\n",
    "    params['model_dir'] = remote_model_dir = os.path.join(log_dir, run_name)\n",
    "    local_model_dir = os.path.join(base_dir, run_name)\n",
    "\n",
    "    display(params)\n",
    "    if os.path.exists(remote_model_dir):\n",
    "        print('removing logdir', remote_model_dir)\n",
    "        import shutil\n",
    "        shutil.rmtree(remote_model_dir)\n",
    "\n",
    "    if os.path.exists(local_model_dir):\n",
    "        print('WARNING: model directory', local_model_dir, 'will be overwritten after training completes.')\n",
    "\n",
    "    dataset_train = QFDataset(os.path.join(data_dir, params['dataset_train'] + '.pkl'), params, set_h=True, set_shapes=True, set_mean=True)\n",
    "    dataset_eval = QFDataset(os.path.join(data_dir, params['dataset_validate'] + '.pkl'), params)\n",
    "\n",
    "    writer = tf.summary.create_file_writer(params['model_dir'])\n",
    "    eval_writer = tf.summary.create_file_writer(os.path.join(params['model_dir'], 'eval'))\n",
    "\n",
    "    eval_metrics = {key:getattr(tf.keras.metrics, params['eval_metrics'][key])() for key in params['eval_metrics']}\n",
    "    eval_losses = {key:tf.keras.metrics.Mean() for key in list(params['loss'].keys()) + ['loss', 'regularization']}\n",
    "\n",
    "    optimizer_kwargs = params['optimizer_kwargs'].copy()\n",
    "    if isinstance(params['optimizer_kwargs']['learning_rate'], str):\n",
    "        optimizer_kwargs['learning_rate'] = learning_rate = getattr(tf.keras.optimizers.schedules, params['optimizer_kwargs']['learning_rate'])(**params['optimizer_kwargs']['learning_rate_kwargs'])\n",
    "        del optimizer_kwargs['learning_rate_kwargs']\n",
    "\n",
    "    try:\n",
    "        optimizer = getattr(tf.keras.optimizers, params['optimizer'])(**optimizer_kwargs)\n",
    "    except AttributeError:\n",
    "        optimizer = getattr(tfa.optimizers, params['optimizer'])(**optimizer_kwargs)\n",
    "\n",
    "    if params['optimizer_lookahead']:\n",
    "        optimizer = tfa.optimizers.Lookahead(optimizer, **params.get('lookahead_kwargs', {}))\n",
    "\n",
    "    loss_fn = {key:getattr(tf.keras.losses, params['loss'][key])() for key in params['loss']}\n",
    "    weights = {key:params.get('loss_weights', {key: 1.0}).get(key, 1.0) for key in params['loss']}\n",
    "\n",
    "    model = params['model_fn'](params)\n",
    "\n",
    "    def save_model(epoch=None):\n",
    "        signatures = None\n",
    "        if hasattr(model, 'signatures'):\n",
    "            signatures = model.signatures(dataset_train)\n",
    "\n",
    "        if epoch is not None:\n",
    "            save_dir = os.path.join(params['model_dir'], (\"{:0\" + str(1+int(np.log10(params['train_epochs']))) +  \"d}\").format(epoch))\n",
    "        else:\n",
    "            save_dir = os.path.join(params['model_dir'], 'saved_model')\n",
    "\n",
    "        tf.saved_model.save(model, save_dir, signatures=signatures)\n",
    "\n",
    "    def calc_loss(targets, predictions):\n",
    "        losses = {}\n",
    "\n",
    "        for key in targets.keys():\n",
    "            losses[key] = weights[key]*loss_fn[key](targets[key], predictions[key])\n",
    "\n",
    "        for loss_tensor in model.losses:\n",
    "            losses[loss_tensor.name] = loss_tensor\n",
    "        \n",
    "        loss = tf.add_n(list(losses.values()))\n",
    "        \n",
    "        losses['loss'] = loss\n",
    "        losses['regularization'] = tf.add_n(model.losses)\n",
    "\n",
    "        return loss, losses\n",
    "\n",
    "    @tf.function\n",
    "    def eval_step(batch):\n",
    "        features, targets = batch\n",
    "        predictions = model(features)\n",
    "        loss, losses = calc_loss(predictions, targets)\n",
    "        \n",
    "        for key, metric in eval_metrics.items():\n",
    "            metric.update_state(targets[key], predictions[key])\n",
    "\n",
    "        for key, loss_metric in eval_losses.items():\n",
    "            loss_metric.update_state(losses[key])\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(batch, step, params):\n",
    "        features, targets = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(features)\n",
    "            loss, losses = calc_loss(targets, predictions)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "        clip_norm = params.get('gradient_clip_norm', 0.0)\n",
    "        if clip_norm > 0.0:\n",
    "            grads, global_norm = tf.clip_by_global_norm(grads, clip_norm)\n",
    "        else:\n",
    "            global_norm = None\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return losses, global_norm\n",
    "\n",
    "    global_step = tf.constant(0, dtype=tf.int64)\n",
    "\n",
    "    if progress is not None:\n",
    "        progress.max = params['train_epochs']\n",
    "        progress.description = 'Training'\n",
    "\n",
    "    for epoch in range(params['train_epochs']):\n",
    "        if progress is not None and epoch % (params['train_epochs']//1000) == 0:\n",
    "            progress.value = epoch + 1\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        dataset = dataset_train.dataset\n",
    "        if params['shuffle']:\n",
    "            dataset = dataset.shuffle(params['shuffle_buffer_size'])\n",
    "\n",
    "        for batch in dataset.batch(params['train_batch_size']):\n",
    "            global_step += 1\n",
    "            losses, global_norm = train_step(batch, global_step, params)\n",
    "\n",
    "            current_time = time.time()\n",
    "            global_steps_per_sec = 1/(current_time - batch_start_time)\n",
    "            batch_start_time = current_time\n",
    "            examples_per_sec = global_steps_per_sec*params['train_batch_size']\n",
    "\n",
    "        if global_step % params['save_summary_epochs'] == 0:\n",
    "            with writer.as_default():\n",
    "                if 'learning_rate' in params['summaries'] and 'learning_rate' in globals():\n",
    "                    tf.summary.scalar('learning_rate', learning_rate(global_step), step=epoch)\n",
    "                    \n",
    "                if 'losses' in params['summaries']:\n",
    "                    tf.summary.scalar('loss', losses['loss'], step=epoch)\n",
    "                    tf.summary.scalar('loss/regularization', losses['regularization'], step=epoch)\n",
    "\n",
    "                    for key in list(params['loss'].keys()):\n",
    "                        tf.summary.scalar('loss/' + key, losses[key], step=epoch)\n",
    "\n",
    "                if 'global_norm' in params['summaries'] and params.get('gradient_clip_norm', 0.0) > 0.0:\n",
    "                    tf.summary.scalar('global_norm', global_norm, step=epoch)\n",
    "\n",
    "                tf.summary.scalar('examples/sec', examples_per_sec, step=epoch)\n",
    "                writer.flush()\n",
    "\n",
    "        if (epoch + 1) % params['train_epochs_per_eval'] == 0:\n",
    "            with eval_writer.as_default():\n",
    "\n",
    "                # reset all metric states\n",
    "                for metric in eval_metrics.values():\n",
    "                    metric.reset_states()\n",
    "\n",
    "                for loss_metric in eval_losses.values():\n",
    "                    loss_metric.reset_states()\n",
    "\n",
    "                for batch in dataset_eval.dataset.batch(params['eval_batch_size']):\n",
    "                    eval_step(batch)\n",
    "\n",
    "                for key, metric in eval_metrics.items():\n",
    "                    tf.summary.scalar('metrics/' + key, metric.result(), step=epoch)\n",
    "\n",
    "                for key, loss_metric in eval_losses.items():\n",
    "                    tf.summary.scalar('loss/'*(key != 'loss') + key, loss_metric.result(), step=epoch)\n",
    "\n",
    "                eval_writer.flush()\n",
    "\n",
    "        if (epoch + 1) % params['save_model_epochs'] == 0:\n",
    "            save_model(epoch=epoch+1)\n",
    "\n",
    "    save_model()\n",
    "\n",
    "    if os.path.exists(local_model_dir):\n",
    "        import shutil\n",
    "        shutil.rmtree(local_model_dir)\n",
    "\n",
    "    if log_dir.startswith('gs://'):\n",
    "        !gsutil -m cp -r $remote_model_dir $base_dir\n",
    "    else:\n",
    "        !cp -r $remote_model_dir $base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3a_keras_train_kenergy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
