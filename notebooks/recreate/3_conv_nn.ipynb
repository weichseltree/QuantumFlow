{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/My Drive/Projects/QuantumFlow/notebooks/recreate\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
    "    print('Found GPU')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, HTML, display\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from quantumflow.calculus_utils import integrate, integrate_simpson, laplace\n",
    "from quantumflow.numerov_solver import unpack_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../data\"\n",
    "dataset = \"recreate\"\n",
    "\n",
    "N = 1\n",
    "experiment = \"KE_learning\"\n",
    "shuffle_buffer_size = 1000\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "with open(os.path.join(datadir, dataset, 'dataset_training.pkl'), 'rb') as f:\n",
    "    np_x, np_potentials, np_wavefunctions, np_energies, np_densities, np_kenergies, M, G, h = unpack_dataset(N, pickle.load(f))\n",
    "    \n",
    "with open(os.path.join(datadir, dataset, 'dataset_validation.pkl'), 'rb') as f:\n",
    "    _, np_val_potentials, _, _, np_val_densities, np_val_kenergies, M_val, _, _ = unpack_dataset(N, pickle.load(f))\n",
    "\n",
    "print(\"learning_rate:\", learning_rate)\n",
    "print(\"batch_size:\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "for i, np_data in enumerate(np_densities[:5]):\n",
    "    plt.plot(np_x, np_data, 'C' + str(i%10))\n",
    "plt.title('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_nn(input, return_layers=False, bias=0.0, filters=(4, 16, 32, 64), kernel_size=(9, 27, 81, 243), padding='same', activation=tf.nn.leaky_relu, **kwargs):\n",
    "    layers_list = []\n",
    "    value = tf.expand_dims(input, axis=-1)\n",
    "    value = tf.expand_dims(value, axis=2)\n",
    "    layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "    \n",
    "    assert len(filters) == len(kernel_size)\n",
    "    layers = len(filters)\n",
    "    \n",
    "    for l in range(layers -1):\n",
    "        value = tf.layers.conv2d(value, filters=filters[l], kernel_size=(kernel_size[l], 1), activation=activation, padding=padding, **kwargs)\n",
    "        layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "        \n",
    "    value = tf.layers.conv2d(value, filters=filters[-1], kernel_size=(kernel_size[-1], 1), padding=padding, **kwargs)\n",
    "    value = tf.reduce_sum(value, axis=2)\n",
    "    layers_list.append(value)\n",
    "    \n",
    "    value = tf.reduce_sum(value, axis=2)\n",
    "    value = tf.reduce_sum(value, axis=1)\n",
    "    layers_list.append(value)\n",
    "    \n",
    "    if return_layers:\n",
    "        return value, layers_list\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def conv_deconv_nn(input, return_layers=False, bias=0.0, filters=(4, 16, 32, 64), kernel_size=(9, 27, 81, 243), padding='same', activation=tf.nn.leaky_relu, **kwargs):\n",
    "    layers_list = []\n",
    "    value = tf.expand_dims(input, axis=-1)\n",
    "    value = tf.expand_dims(value, axis=2)\n",
    "    layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "    \n",
    "    assert len(filters) == len(kernel_size)\n",
    "    layers = len(filters)\n",
    "    \n",
    "    for l in range(layers):\n",
    "        value = tf.layers.conv2d(value, filters=filters[l], kernel_size=(kernel_size[l], 1), activation=activation, padding=padding, **kwargs)\n",
    "        layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "        \n",
    "    for l in reversed(range(1, layers)):\n",
    "        value = tf.layers.conv2d_transpose(value, filters=filters[l], kernel_size=(kernel_size[l], 1), activation=activation, padding=padding, **kwargs)\n",
    "        layers_list.append(tf.reduce_sum(value, axis=2))\n",
    "    \n",
    "    value = tf.layers.conv2d_transpose(value, filters=1, kernel_size=(kernel_size[0], 1), padding=padding)\n",
    "    value = tf.reduce_sum(value, axis=2)\n",
    "    layers_list.append(value)\n",
    "    \n",
    "    value = tf.reduce_sum(value, axis=-1)\n",
    "    \n",
    "    if return_layers:\n",
    "        return value, layers_list\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "print('train dataset size:', M)\n",
    "print('discretization points:', G)\n",
    "print('discretization width:', h, 'Bohr')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np_densities, np_kenergies))\n",
    "\n",
    "dataset_length = len(np_densities)\n",
    "print('dataset_length:', dataset_length)\n",
    "\n",
    "EPOCHS = tf.placeholder(tf.int64)\n",
    "iter = dataset.shuffle(shuffle_buffer_size).repeat(EPOCHS).batch(batch_size).make_initializable_iterator()\n",
    "train_density, train_kenergy = iter.get_next()\n",
    "\n",
    "train_density = tf.identity(train_density, name='density')\n",
    "train_kenergy = tf.identity(train_kenergy, name='kinetic_energy')\n",
    "\n",
    "print(train_density)\n",
    "print(train_kenergy)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((np_val_densities, np_val_kenergies))\n",
    "\n",
    "val_dataset_length = len(np_val_densities)\n",
    "print('val_dataset_length:', val_dataset_length)\n",
    "\n",
    "val_iter = val_dataset.batch(batch_size).make_initializable_iterator()\n",
    "val_density, val_kenergy = val_iter.get_next()\n",
    "\n",
    "kwargs = {'filters': (16, 8, 4, 4), 'kernel_size': (121, 121, 121, 121), 'bias': 0.0, 'padding': 'valid', 'activation': tf.nn.softplus}\n",
    "\n",
    "with tf.variable_scope(\"model\", reuse=False):\n",
    "    pred_kenergy, layers = conv_nn(train_density, return_layers=True, **kwargs)\n",
    "    loss = tf.losses.mean_squared_error(pred_kenergy, train_kenergy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.create_global_step())\n",
    "\n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "    DENSITY = tf.placeholder(dtype=tf.float64, shape=train_density.get_shape())\n",
    "    test_kenergy, layers = conv_nn(DENSITY, return_layers=True, **kwargs)\n",
    "    test_gradient = tf.gradients(test_kenergy, DENSITY)\n",
    "\n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "    pred_val_kenergy = conv_nn(val_density, **kwargs)\n",
    "    val_loss, val_update = tf.metrics.mean_squared_error(pred_val_kenergy, val_kenergy)    \n",
    "\n",
    "for variable in tf.trainable_variables():\n",
    "    print(variable.name, variable.get_shape())\n",
    "print(\"Free Parameters:\", np.sum([np.prod(variable.get_shape()) for variable in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwargs = {'filters': (4, 16), 'kernel_size': (9, 81), 'bias': 0.0, 'padding': 'valid', 'activation': tf.nn.softplus}\n",
    "#\n",
    "#with tf.variable_scope(\"model\", reuse=False):\n",
    "#    pred_k_density = conv_deconv_nn(train_density, **kwargs)\n",
    "#    loss = tf.losses.mean_squared_error(pred_k_density, train_k_density)\n",
    "#    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#    train_op = optimizer.minimize(loss, global_step=tf.train.create_global_step())\n",
    "#\n",
    "#with tf.variable_scope(\"model\", reuse=True):\n",
    "#    DENSITY = tf.placeholder(dtype=tf.float32, shape=train_density.get_shape())\n",
    "#    test_k_density, layers = conv_deconv_nn(DENSITY, return_layers=True, **kwargs)\n",
    "#    test_k_energy = tf.reduce_mean(test_k_density, axis=1) # not trapezoidal rule but yeah...\n",
    "#    test_gradient = tf.gradients(test_k_energy, DENSITY)\n",
    "#\n",
    "#with tf.variable_scope(\"model\", reuse=True):\n",
    "#    pred_val_k_density = conv_deconv_nn(val_density, **kwargs)\n",
    "#    val_loss, val_update = tf.metrics.mean_squared_error(pred_val_k_density, val_k_density)    \n",
    "#\n",
    "#for variable in tf.trainable_variables():\n",
    "#    print(variable.name, variable.get_shape())\n",
    "#print(\"Free Parameters:\", np.sum([np.prod(variable.get_shape()) for variable in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, HTML, display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = []\n",
    "loss_step_ = []\n",
    "np_test_k_density_ = []\n",
    "np_layers_ = []\n",
    "\n",
    "val_loss_ = []\n",
    "val_loss_step_ = []\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, tf.train.latest_checkpoint(experiment))\n",
    "with open('logging.pkl', 'rb') as f:\n",
    "    loss_, loss_step_, val_loss_, val_loss_step_, np_test_k_density_, np_layers_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "    data = np.array(data)\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "#plt.figure(figsize=(20, 8))\n",
    "#plt.plot(loss_step_, np.log(loss_))\n",
    "#plt.plot(val_loss_step_, np.log(val_loss_))\n",
    "#plt.plot(numpy_ewma_vectorized_v2(loss_, 1))\n",
    "#plt.ylim([0, 0.01])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    save_path = saver.save(sess, os.path.join(datadir, experiment, \"model.ckpt\"), global_step=sess.run(tf.train.get_global_step()))\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "    \n",
    "    with open(os.path.join(datadir, experiment, 'logging.pkl'), 'wb') as f:\n",
    "        pickle.dump([loss_, loss_step_, val_loss_, val_loss_step_, np_test_k_density_, np_layers_], f)\n",
    "\n",
    "def train(epochs, val_interval, save_interval, progress=None, val_progress=None):\n",
    "    try:\n",
    "        sess.run(iter.initializer, feed_dict={EPOCHS: epochs})\n",
    "        if progress is not None:\n",
    "            progress.max = sess.run(tf.train.get_global_step()) + (dataset_length*epochs)//batch_size\n",
    "        while True:\n",
    "            loss_.append(sess.run([loss, train_op])[0])\n",
    "            global_step = sess.run(tf.train.get_global_step())\n",
    "            loss_step_.append(global_step)\n",
    "            \n",
    "            if progress is not None:\n",
    "                progress.value = global_step\n",
    "                progress.description = str(progress.value) + '/' + str(progress.max)\n",
    "\n",
    "            np_test_k_density, np_layers = sess.run([test_kenergy, layers], \n",
    "                                                    feed_dict={DENSITY: np.expand_dims(np_densities[61], axis=0)})\n",
    "\n",
    "            np_test_k_density_.append(np_test_k_density)\n",
    "            np_layers_.append(np_layers)\n",
    "\n",
    "            if global_step % val_interval == 0:\n",
    "                sess.run(val_iter.initializer)\n",
    "                sess.run(tf.local_variables_initializer())\n",
    "                if val_progress is not None:\n",
    "                    val_progress.max = val_dataset_length\n",
    "                    val_progress.value = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        sess.run(val_update)\n",
    "                        if val_progress is not None:\n",
    "                            val_progress.value += batch_size\n",
    "                            val_progress.description = str(val_progress.value) + '/' + str(val_progress.max)\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    np_val_loss = sess.run(val_loss)\n",
    "                    val_loss_.append(np_val_loss)\n",
    "                    val_loss_step_.append(global_step)\n",
    "                    \n",
    "                    #plt.figure(figsize=(20, 8))\n",
    "                    #plt.plot(loss_step_, np.log(loss_))\n",
    "                    #plt.plot(val_loss_step_, np.log(val_loss_))\n",
    "                    #clear_output(wait=True)\n",
    "                    #display(plt.gcf())\n",
    "\n",
    "            if global_step % save_interval == 0:\n",
    "                save_model()\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "        save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = widgets.IntProgress(value=0, max=0, description='Training:', \n",
    "                               bar_style='info', layout=widgets.Layout(width='92%'))\n",
    "display(progress)\n",
    "\n",
    "val_progress = widgets.IntProgress(value=0, max=0, description='Validation:', \n",
    "                                   bar_style='success', layout=widgets.Layout(width='92%'))\n",
    "display(val_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "val_interval = 100\n",
    "save_interval = 1000\n",
    "train(epochs, val_interval, save_interval, progress=progress, val_progress=val_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(loss_step_, np.log(loss_))\n",
    "plt.plot(val_loss_step_, np.log(val_loss_))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(loss_step_[500:-499], running_mean(np.log(loss_), 1000))\n",
    "plt.plot(val_loss_step_[5:-4], running_mean(np.log(val_loss_), 10))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "def anim_plot(array, x=None, interval=100, bar=\"\", figsize=(10, 4), **kwargs):\n",
    "    frames = len(array)\n",
    "    \n",
    "    if not bar == \"\":\n",
    "        import ipywidgets as widgets\n",
    "        widget = widgets.IntProgress(min=0, max=frames, description=bar, bar_style='success',\n",
    "                                     layout=widgets.Layout(width='92%'))\n",
    "        display(widget)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if x is None:\n",
    "        plt_h = ax.plot(array[0], **kwargs)\n",
    "    else:\n",
    "        plt_h = ax.plot(x, array[0], **kwargs) \n",
    "        \n",
    "    ax.set_ylim([np.min(array), np.max(array)])\n",
    "\n",
    "    def init():\n",
    "        return plt_h\n",
    "\n",
    "    def animate(f):\n",
    "        if not bar == \"\":\n",
    "            widget.value = f\n",
    "\n",
    "        for i, h in enumerate(plt_h):\n",
    "            if x is None:\n",
    "                h.set_data(np.arange(len(array[f][:, i])), array[f][:, i], **kwargs)\n",
    "            else:\n",
    "                h.set_data(x, array[f][:, i], **kwargs)\n",
    "        return plt_h\n",
    "\n",
    "    # call the animator. blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=frames, interval=interval,\n",
    "                                   blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "    rc('animation', html='html5')\n",
    "    display(HTML(anim.to_html5_video(embed_limit=1024)))\n",
    "\n",
    "    if not bar == \"\":\n",
    "        widget.close()\n",
    "\n",
    "\n",
    "\n",
    "np_test_K, np_test_gradient = sess.run([test_kenergy, test_gradient[0]], \n",
    "                                                feed_dict={DENSITY: np.expand_dims(np_densities[61], axis=0)})\n",
    "        \n",
    "    \n",
    "print('Energy prediction:')\n",
    "print(np_kenergies[61], np_test_K[0])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(np_x, np_test_gradient[0]/h)\n",
    "plt.plot(np_x, -np_test_gradient[0]/h, 'C0', linestyle='--')\n",
    "#plt.plot(np_x, np_density[61])\n",
    "plt.plot(np_x, np_potentials[61])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in reversed(range(1, len(np_layers_[0])-1)):\n",
    "    anim_plot([np_layers_[i][l][0, :, :] for i in range(0, len(np_layers_), 100)], bar=\"Animation:\", interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_conv_nn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
